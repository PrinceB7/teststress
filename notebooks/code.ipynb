{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wq7IWZ_7BIOM"
   },
   "source": [
    "# 1. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgpFjplrBgsH"
   },
   "source": [
    "## 1.1. Initialize paths\n",
    "    > Mount Google Drive\n",
    "    > Set file paths (e.g. sensing data, EMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4tuknI77_1eA",
    "outputId": "7861d49e-0962-407b-f4ca-59feaa6ea949"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths initialized -- filesystem ready --\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "from os.path import join as j\n",
    "from os.path import sep\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class Dirs:\n",
    "    ROOT = 'root dir'\n",
    "\n",
    "    # raw datasets\n",
    "    RAW_LAB = 'initial lab'\n",
    "    RAW_FIELD = 'initial field'\n",
    "    RAW_FIELD_LEFTOVERS = 'initial field leftovers'\n",
    "\n",
    "    # initial datasets (per participant)\n",
    "    INIT_PARTICIPANTS = 'imported participants data'\n",
    "\n",
    "    # splitting datasets\n",
    "    SPLIT_TREADMILL_DATASET = 'unprocessed treadmill data'\n",
    "    SPLIT_LAB_STRESS_DATASET = 'unprocessed lab stress data'\n",
    "    SPLIT_FIELD_DATASET = 'unprocessed field stress data'\n",
    "\n",
    "    # processed splitted datasets\n",
    "    PROCESSED_TREADMILL_DATASET = 'processed treadmill data'\n",
    "    PROCESSED_LAB_STRESS_DATASET = 'processed lab stress data'\n",
    "    PROCESSED_FIELD_DATASET = 'processed field stress data'\n",
    "\n",
    "    # filtered splitted datasets\n",
    "    FILTERED_LAB_STRESS_DATASET = 'filtered lab stress data'\n",
    "    FILTERED_FIELD_DATASET = 'filtered field stress data'\n",
    "\n",
    "    # normalized splitted dataset\n",
    "    NORMALIZED_LAB_STRESS_DATASET = 'normalized lab stress data'\n",
    "    NORMALIZED_FIELD_DATASET = 'normalized field stress data'\n",
    "\n",
    "    # dataset statistics\n",
    "    STATS_ROOT = 'stats root'\n",
    "    STATS_EMA_GRANULAR = 'granular ema stats'\n",
    "    STATS_LAB_EMA = 'lab stress ema scores'\n",
    "    STATS_LOCATION_ELBOW_CURVES = 'location k-means elbow curves'\n",
    "\n",
    "    # personalized model's features\n",
    "    FEATURES_LAB_STRESS = 'lab stress features'\n",
    "    FEATURES_FIELD = 'field stress features'\n",
    "    FEATURES_FIELD_WITH_CONTEXT = 'field (watch + context) features'\n",
    "    FEATURES_FIELD_WITH_CONTEXT_MOTION_FILTER = 'field (watch + context) features with motion filter'\n",
    "    FEATURES_FIELD_WITH_CONTEXT_LOOSENESS_FILTER = 'field (watch + stress) features with looseness filter'\n",
    "    FEATURES_FIELD_WITH_CONTEXT_COMBINED_FILTER = 'field (watch + stress) features with combined filter'\n",
    "\n",
    "    # results / accuracy\n",
    "    RESULTS_ROOT = 'results'\n",
    "\n",
    "    # thresholds\n",
    "    THRESHOLDS_TREADMILL_OVERVIEW = 'treadmill values overview for thresholds'\n",
    "    THRESHOLDS_MOTION = 'motion threshold results'\n",
    "    THRESHOLDS_LOOSENESS = 'looseness threshold results'\n",
    "\n",
    "    # ppg accuracy i.e. ppg vs. ecg\n",
    "    PPG_ACCURACY_SUMMARY = 'ppg accuracy (ppg vs. ecg)'\n",
    "    PPG_ACCURACY_DETAILED = 'ppg accuracy (ppg vs. ecg) detailed'\n",
    "\n",
    "    # all keys\n",
    "    keys = {\n",
    "        ROOT,\n",
    "        RAW_LAB,\n",
    "        RAW_FIELD,\n",
    "        RAW_FIELD_LEFTOVERS,\n",
    "        INIT_PARTICIPANTS,\n",
    "        SPLIT_TREADMILL_DATASET,\n",
    "        SPLIT_LAB_STRESS_DATASET,\n",
    "        SPLIT_FIELD_DATASET,\n",
    "        PROCESSED_TREADMILL_DATASET,\n",
    "        PROCESSED_LAB_STRESS_DATASET,\n",
    "        PROCESSED_FIELD_DATASET,\n",
    "        FILTERED_LAB_STRESS_DATASET,\n",
    "        FILTERED_FIELD_DATASET,\n",
    "        NORMALIZED_LAB_STRESS_DATASET,\n",
    "        NORMALIZED_FIELD_DATASET,\n",
    "        STATS_ROOT,\n",
    "        STATS_EMA_GRANULAR,\n",
    "        STATS_LAB_EMA,\n",
    "        STATS_LOCATION_ELBOW_CURVES,\n",
    "        FEATURES_LAB_STRESS,\n",
    "        FEATURES_FIELD,\n",
    "        FEATURES_FIELD_WITH_CONTEXT,\n",
    "        FEATURES_FIELD_WITH_CONTEXT_MOTION_FILTER,\n",
    "        FEATURES_FIELD_WITH_CONTEXT_LOOSENESS_FILTER,\n",
    "        FEATURES_FIELD_WITH_CONTEXT_COMBINED_FILTER,\n",
    "        RESULTS_ROOT,\n",
    "        THRESHOLDS_TREADMILL_OVERVIEW,\n",
    "        THRESHOLDS_MOTION,\n",
    "        THRESHOLDS_LOOSENESS,\n",
    "        PPG_ACCURACY_SUMMARY,\n",
    "        PPG_ACCURACY_DETAILED\n",
    "    }\n",
    "\n",
    "\n",
    "class Files:\n",
    "    # configuration files\n",
    "    CONF_PARTICIPANT_IDS = 'participant ids'\n",
    "    CONF_LAB_PROTOCOL = 'lab stress protocol'\n",
    "    CONF_TREADMILL_PROTOCOL = 'treadmill protocol'\n",
    "\n",
    "    # data files\n",
    "    DATA_EMA = 'ema responses'\n",
    "\n",
    "    # personalized model's results\n",
    "    RESULTS_LAB_STRESS = 'lab stress results'\n",
    "    RESULTS_FIELD_STRESS = 'field stress results'\n",
    "    RESULTS_FIELD_WITH_CONTEXT = 'field (watch+context) stress results'\n",
    "    RESULTS_FIELD_WITH_CONTEXT_MOTION_FILTER = 'field (watch+context) stress results with motion filter'\n",
    "    RESULTS_FIELD_WITH_CONTEXT_LOOSENESS_FILTER = 'field (watch+context) stress results with looseness filter'\n",
    "    RESULTS_FIELD_WITH_CONTEXT_COMBINED_FILTER = 'field (watch+context) stress results with combined filter'\n",
    "\n",
    "    # generic model's results\n",
    "    RESULTS_G_LAB_STRESS = 'generalized lab stress results'\n",
    "    RESULTS_G_FIELD_STRESS = 'generalized field stress results'\n",
    "    RESULTS_G_FIELD_WITH_CONTEXT = 'generalized field (watch+context) stress results'\n",
    "    RESULTS_G_FIELD_WITH_CONTEXT_MOTION_FILTER = 'generalized field (watch+context) stress results with motion filter'\n",
    "    RESULTS_G_FIELD_WITH_CONTEXT_LOOSENESS_FILTER = 'generalized field (watch+context) stress results with looseness filter'\n",
    "    RESULTS_G_FIELD_WITH_CONTEXT_COMBINED_FILTER = 'generalized field (watch+context) stress results with combined filter'\n",
    "\n",
    "    # all keys\n",
    "    keys = {\n",
    "        CONF_PARTICIPANT_IDS,\n",
    "        CONF_LAB_PROTOCOL,\n",
    "        CONF_TREADMILL_PROTOCOL,\n",
    "        DATA_EMA,\n",
    "        RESULTS_LAB_STRESS,\n",
    "        RESULTS_FIELD_STRESS,\n",
    "        RESULTS_FIELD_WITH_CONTEXT,\n",
    "        RESULTS_FIELD_WITH_CONTEXT_MOTION_FILTER,\n",
    "        RESULTS_FIELD_WITH_CONTEXT_LOOSENESS_FILTER,\n",
    "        RESULTS_FIELD_WITH_CONTEXT_COMBINED_FILTER,\n",
    "        RESULTS_G_LAB_STRESS,\n",
    "        RESULTS_G_FIELD_STRESS,\n",
    "        RESULTS_G_FIELD_WITH_CONTEXT,\n",
    "        RESULTS_G_FIELD_WITH_CONTEXT_MOTION_FILTER,\n",
    "        RESULTS_G_FIELD_WITH_CONTEXT_LOOSENESS_FILTER,\n",
    "        RESULTS_G_FIELD_WITH_CONTEXT_COMBINED_FILTER,\n",
    "    }\n",
    "\n",
    "\n",
    "class Paths:\n",
    "    _root = \"/Users/kobiljon/Desktop/SOSW\"\n",
    "    _dirs = {\n",
    "        Dirs.ROOT: _root,\n",
    "\n",
    "        # initial dataset\n",
    "        Dirs.RAW_LAB: j(_root, '0. raw dataset', 'lab_study_data'),\n",
    "        Dirs.RAW_FIELD: j(_root, '0. raw dataset', 'field_sensing'),\n",
    "        Dirs.RAW_FIELD_LEFTOVERS: j(_root, '0. raw dataset', 'field_sensing_leftovers'),\n",
    "\n",
    "        # merging datasets\n",
    "        Dirs.INIT_PARTICIPANTS: j(_root, '1. participant datasets'),\n",
    "\n",
    "        # splitting datasets\n",
    "        Dirs.SPLIT_TREADMILL_DATASET: j(_root, '2. three studies datasets', '1. ppg accuracy'),\n",
    "        Dirs.SPLIT_LAB_STRESS_DATASET: j(_root, '2. three studies datasets', '2. lab stress'),\n",
    "        Dirs.SPLIT_FIELD_DATASET: j(_root, '2. three studies datasets', '3. field stress'),\n",
    "\n",
    "        # processed splitted datasets\n",
    "        Dirs.PROCESSED_TREADMILL_DATASET: j(_root, '2.1. three studies datasets (processed)', '1. ppg accuracy'),\n",
    "        Dirs.PROCESSED_LAB_STRESS_DATASET: j(_root, '2.1. three studies datasets (processed)', '2. lab stress'),\n",
    "        Dirs.PROCESSED_FIELD_DATASET: j(_root, '2.1. three studies datasets (processed)', '3. field stress'),\n",
    "\n",
    "        # filtered splitted datasets\n",
    "        Dirs.FILTERED_LAB_STRESS_DATASET: j(_root, '2.2. three studies datasets (filtered)', '2. lab stress'),\n",
    "        Dirs.FILTERED_FIELD_DATASET: j(_root, '2.2. three studies datasets (filtered)', '3. field stress'),\n",
    "\n",
    "        # normalized splitted datasets\n",
    "        Dirs.NORMALIZED_LAB_STRESS_DATASET: j(_root, '2.3. three studies datasets (normalized)', '2. lab stress'),\n",
    "        Dirs.NORMALIZED_FIELD_DATASET: j(_root, '2.3. three studies datasets (normalized)', '3. field stress'),\n",
    "\n",
    "        # dataset statistics\n",
    "        Dirs.STATS_ROOT: j(_root, '3. dataset stats'),\n",
    "        Dirs.STATS_EMA_GRANULAR: j(_root, '3. dataset stats', 'granular_ema_stats'),\n",
    "        Dirs.STATS_LAB_EMA: j(_root, '3. dataset stats', 'lab_stress_ema_scores'),\n",
    "        Dirs.STATS_LOCATION_ELBOW_CURVES: j(_root, '3. dataset stats', 'location_elbow_curves'),\n",
    "\n",
    "        # features\n",
    "        Dirs.FEATURES_LAB_STRESS: j(_root, '4. stress features', '1. lab stress'),\n",
    "        Dirs.FEATURES_FIELD: j(_root, '4. stress features', '2. field stress'),\n",
    "        Dirs.FEATURES_FIELD_WITH_CONTEXT: j(_root, '4. stress features', '3. field stress + context'),\n",
    "        Dirs.FEATURES_FIELD_WITH_CONTEXT_MOTION_FILTER: j(_root, '4. stress features', '4. field stress + context + motion filter'),\n",
    "        Dirs.FEATURES_FIELD_WITH_CONTEXT_LOOSENESS_FILTER: j(_root, '4. stress features', '5. field stress + context + looseness filter'),\n",
    "        Dirs.FEATURES_FIELD_WITH_CONTEXT_COMBINED_FILTER: j(_root, '4. stress features', '6. field stress + context + combined filter'),\n",
    "\n",
    "        # results / accuracy\n",
    "        Dirs.RESULTS_ROOT: j(_root, '5. result'),\n",
    "\n",
    "        # thresholds\n",
    "        Dirs.THRESHOLDS_TREADMILL_OVERVIEW: j(_root, '6. threshold gridsearch results', '1. treadmill values overview'),\n",
    "        Dirs.THRESHOLDS_MOTION: j(_root, '6. threshold gridsearch results', '2. motion threshold results'),\n",
    "        Dirs.THRESHOLDS_LOOSENESS: j(_root, '6. threshold gridsearch results', '3. looseness threshold results'),\n",
    "\n",
    "        # ppg accuracy i.e. ppg vs. ecg\n",
    "        Dirs.PPG_ACCURACY_SUMMARY: j(_root, '7. ppg accuracy', '1. summary of differences'),\n",
    "        Dirs.PPG_ACCURACY_DETAILED: j(_root, '7. ppg accuracy', '2. detailed differences'),\n",
    "    }\n",
    "    _files = {\n",
    "        # configuration files\n",
    "        Files.CONF_PARTICIPANT_IDS: j(_root, '0. raw dataset', 'participant_map.csv'),\n",
    "        Files.CONF_LAB_PROTOCOL: j(_root, '0. raw dataset', 'lab_stress_protocol.csv'),\n",
    "        Files.CONF_TREADMILL_PROTOCOL: j(_root, '0. raw dataset', 'ppg_accuracy_protocol.csv'),\n",
    "\n",
    "        # data files\n",
    "        Files.DATA_EMA: j(_root, '0. raw dataset', 'ema_responses.csv'),\n",
    "\n",
    "        # personalized model's results / accuracy\n",
    "        Files.RESULTS_LAB_STRESS: j(_root, '5. result', 'lab.csv'),\n",
    "        Files.RESULTS_FIELD_STRESS: j(_root, '5. result', 'field.csv'),\n",
    "        Files.RESULTS_FIELD_WITH_CONTEXT: j(_root, '5. result', 'field_context.csv'),\n",
    "        Files.RESULTS_FIELD_WITH_CONTEXT_MOTION_FILTER: j(_root, '5. result', 'field_context_motion_filter.csv'),\n",
    "        Files.RESULTS_FIELD_WITH_CONTEXT_LOOSENESS_FILTER: j(_root, '5. result', 'field_context_looseness_filter.csv'),\n",
    "        Files.RESULTS_FIELD_WITH_CONTEXT_COMBINED_FILTER: j(_root, '5. result', 'field_context_combined_filter.csv'),\n",
    "\n",
    "        Files.RESULTS_G_LAB_STRESS: j(_root, '5. result', 'lab_generalized.csv'),\n",
    "        Files.RESULTS_G_FIELD_STRESS: j(_root, '5. result', 'field_generalized.csv'),\n",
    "        Files.RESULTS_G_FIELD_WITH_CONTEXT: j(_root, '5. result', 'field_generalized_context.csv'),\n",
    "        Files.RESULTS_G_FIELD_WITH_CONTEXT_MOTION_FILTER: j(_root, '5. result', 'field_generalized_context_motion_filter.csv'),\n",
    "        Files.RESULTS_G_FIELD_WITH_CONTEXT_LOOSENESS_FILTER: j(_root, '5. result', 'field_generalized_context_looseness_filter.csv'),\n",
    "        Files.RESULTS_G_FIELD_WITH_CONTEXT_COMBINED_FILTER: j(_root, '5. result', 'field_generalized_context_combined_filter.csv'),\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def get(key, *attach: str):\n",
    "        if key in Dirs.keys:\n",
    "            if attach:\n",
    "                return j(Paths._dirs[key], f'{sep}'.join(attach))\n",
    "            else:\n",
    "                return Paths._dirs[key]\n",
    "        elif key in Files.keys:\n",
    "            return Paths._files[key]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "print('Paths initialized -- filesystem ready --')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Utility functions\n",
    "    > Loading data file\n",
    "    > Selecting portion of data\n",
    "    > Binary search for filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, cast_type=float, single_column=True, sort=True):\n",
    "    _res = []\n",
    "    with open(path) as _r:\n",
    "        if single_column:\n",
    "            for _line in _r:\n",
    "                _ts, _val = _line[:-1].split(',')\n",
    "                try:\n",
    "                    _ts, _val = int(_ts), cast_type(_val)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                _res += [(_ts, _val)]\n",
    "        else:\n",
    "            for _line in _r:\n",
    "                _cells = _line[:-1].split(',')\n",
    "                try:\n",
    "                    _ts = int(_cells[0])\n",
    "                    _values = tuple(cast_type(_x) for _x in _cells[1:])\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                _res += [(_ts,) + _values]\n",
    "    if sort:\n",
    "        _res.sort(key=lambda e: e[0])\n",
    "    return _res\n",
    "\n",
    "\n",
    "def bin_find(dataset, low, high, ts, go_up):\n",
    "    if high > low:\n",
    "        mid = (high + low) // 2\n",
    "        if dataset[mid][0] == ts:\n",
    "            return mid\n",
    "        elif dataset[mid][0] > ts:\n",
    "            return bin_find(dataset, low, mid, ts, go_up)\n",
    "        else:\n",
    "            return bin_find(dataset, mid + 1, high, ts, go_up)\n",
    "    else:\n",
    "        return max(high, low) if go_up else min(high, low)\n",
    "\n",
    "\n",
    "def select_data(dataset, from_ts, till_ts, with_timestamp=False, threshold=None):\n",
    "    _res = []\n",
    "    _idx0 = bin_find(dataset, 0, len(dataset) - 1, from_ts, go_up=False)\n",
    "    _idx1 = len(dataset)\n",
    "    while _idx0 < _idx1:\n",
    "        _ts = dataset[_idx0][0]\n",
    "        if from_ts <= _ts < till_ts:\n",
    "            if with_timestamp:\n",
    "                _res += [dataset[_idx0]]\n",
    "            else:\n",
    "                _cells = dataset[_idx0][1:]\n",
    "                _res += [_cells if len(_cells) > 1 else _cells[0]]\n",
    "        elif _ts >= till_ts:\n",
    "            break\n",
    "        _idx0 += 1\n",
    "    return _res if not threshold or len(_res) >= threshold else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtE4fudnCFxh"
   },
   "source": [
    "## 1.3. Initialize study configs\n",
    "    > Feature selection\n",
    "    > Sliding window size\n",
    "    > Sliding window overlap\n",
    "    > M_ROUNDS for building personalized models\n",
    "    > EMA span for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ypse15ywCK3Q",
    "outputId": "91eeaf0a-0b0d-41f2-96c7-c2e3a45cdab7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hrv-analysis in /Users/kobiljon/miniforge3/envs/science/lib/python3.9/site-packages/hrv_analysis-1.0.4-py3.9.egg (1.0.4)\r\n",
      "Requirement already satisfied: numpy>=1.15.1 in /Users/kobiljon/miniforge3/envs/science/lib/python3.9/site-packages (from hrv-analysis) (1.21.4)\r\n",
      "Requirement already satisfied: astropy>=3.0.4 in /Users/kobiljon/miniforge3/envs/science/lib/python3.9/site-packages (from hrv-analysis) (4.3.1)\r\n",
      "Requirement already satisfied: nolds>=0.4.1 in /Users/kobiljon/miniforge3/envs/science/lib/python3.9/site-packages/nolds-0.5.2-py3.9.egg (from hrv-analysis) (0.5.2)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/kobiljon/miniforge3/envs/science/lib/python3.9/site-packages (from hrv-analysis) (1.7.1)\r\n",
      "Requirement already satisfied: pandas>=0.23.4 in /Users/kobiljon/miniforge3/envs/science/lib/python3.9/site-packages (from hrv-analysis) (1.3.3)\r\n",
      "Requirement already satisfied: matplotlib>=2.2.2 in /Users/kobiljon/miniforge3/envs/science/lib/python3.9/site-packages (from hrv-analysis) (3.4.3)\r\n",
      "Requirement already satisfied: pyerfa>=1.7.3 in /Users/kobiljon/miniforge3/envs/science/lib/python3.9/site-packages (from astropy>=3.0.4->hrv-analysis) (2.0.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/kobiljon/miniforge3/envs/science/lib/python3.9/site-packages (from matplotlib>=2.2.2->hrv-analysis) (1.3.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/kobiljon/miniforge3/envs/science/lib/python3.9/site-packages (from matplotlib>=2.2.2->hrv-analysis) (3.0.6)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/kobiljon/miniforge3/envs/science/lib/python3.9/site-packages (from matplotlib>=2.2.2->hrv-analysis) (2.8.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/kobiljon/miniforge3/envs/science/lib/python3.9/site-packages (from matplotlib>=2.2.2->hrv-analysis) (0.11.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/kobiljon/miniforge3/envs/science/lib/python3.9/site-packages (from matplotlib>=2.2.2->hrv-analysis) (8.4.0)\r\n",
      "Requirement already satisfied: future in /Users/kobiljon/miniforge3/envs/science/lib/python3.9/site-packages (from nolds>=0.4.1->hrv-analysis) (0.18.2)\r\n",
      "Requirement already satisfied: setuptools in /Users/kobiljon/miniforge3/envs/science/lib/python3.9/site-packages (from nolds>=0.4.1->hrv-analysis) (59.2.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/kobiljon/miniforge3/envs/science/lib/python3.9/site-packages (from pandas>=0.23.4->hrv-analysis) (2021.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/kobiljon/miniforge3/envs/science/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=2.2.2->hrv-analysis) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install hrv-analysis\n",
    "#!pip install shap==0.36.0\n",
    "\n",
    "\n",
    "class Labels:\n",
    "    LABEL_STRESSED = 'stressed'\n",
    "    LABEL_NOT_STRESSED = 'not-stressed'\n",
    "    STR_LABELS = [LABEL_STRESSED, LABEL_NOT_STRESSED]\n",
    "\n",
    "    LABEL_STRESSED_INT = 1\n",
    "    LABEL_NOT_STRESSED_INT = 0\n",
    "    INT_LABELS = [LABEL_STRESSED_INT, LABEL_NOT_STRESSED_INT]\n",
    "\n",
    "\n",
    "class Features:\n",
    "    CONTEXT_ONLY = ['activity', 'location', 'is_weekday']\n",
    "    CATEGORICAL = ['activity', 'location']\n",
    "\n",
    "    WATCH_BASIC = ['mean_nni', 'sdnn', 'rmssd', 'nni_50', 'mean_hr', 'std_hr', 'lf', 'hf', 'lf_hf_ratio', 'sd1', 'sd2', 'sampen']\n",
    "    WATCH_ALL = ['mean_nni', 'sdnn', 'rmssd', 'nni_50', 'mean_hr', 'std_hr', 'sdsd', 'median_nni', 'pnni_50', 'nni_20', 'pnni_20', 'range_nni', 'cvsd', 'cvnni', 'max_hr', 'min_hr', 'lf', 'hf', 'total_power', 'vlf', 'csi', 'cvi', 'modified_csi', 'sd1', 'sd2', 'ratio_sd2_sd1', 'sampen']\n",
    "    # WATCH_ALL = ['hr_mean', 'hr_median', 'hr_max', 'hr_min', 'hr_std', 'hr_kurtosis', 'hr_skew', 'hr_80', 'hr_20', 'rr_mean', 'rr_median', 'rr_max', 'rr_min', 'rr_std', 'rr_kurtosis', 'rr_skew', 'rr_80', 'rr_20', 'rr_rmssd']\n",
    "    WATCH_AND_CONTEXT = ['mean_nni', 'sdnn', 'rmssd', 'nni_50', 'mean_hr', 'std_hr', 'lf', 'hf', 'lf_hf_ratio', 'sd1', 'sd2', 'sampen', 'activity', 'location', 'hour', 'is_weekday']\n",
    "    WATCH_AND_CONTEXT_NO_LOCATION = ['mean_nni', 'sdnn', 'rmssd', 'nni_50', 'mean_hr', 'std_hr', 'lf', 'hf', 'lf_hf_ratio', 'sd1', 'sd2', 'sampen', 'activity', 'hour', 'is_weekday']\n",
    "\n",
    "    ALL = ['mean_nni', 'sdnn', 'rmssd', 'nni_50', 'mean_hr', 'std_hr', 'sdsd', 'median_nni', 'pnni_50', 'nni_20', 'pnni_20', 'range_nni', 'cvsd', 'cvnni', 'max_hr', 'min_hr', 'lf', 'hf', 'lf_hf_ratio', 'total_power', 'vlf', 'lfnu', 'hfnu', 'csi', 'cvi', 'modified_csi', 'triangular_index', 'sd1', 'sd2', 'ratio_sd2_sd1', 'sampen', 'activity', 'location', 'hour', 'is_weekday']\n",
    "\n",
    "\n",
    "class Configs:\n",
    "    # region previous thresholds\n",
    "    # MOTION_THRESHOLDS_RANGE = np.array(range(20, 110, 5)) / 10\n",
    "    # LOOSENESS_THRESHOLDS_RANGE = np.array(range(187, 348, 10)) * 1000\n",
    "    # MOTION_THRESHOLDS = {\n",
    "    #     '12170603@inha.edu': 10.5,\n",
    "    #     '12181157@inha.edu': 7,\n",
    "    #     '12201686@inha.edu': 9.5,\n",
    "    #     '12212982so@inha.edu': 10,\n",
    "    #     'as7177as3912@gmail.com': 8.5,\n",
    "    #     'bagle1029@gmail.com': 9.5,\n",
    "    #     'bw04029@gmail.com': 6,\n",
    "    #     'chaewoni65@gmail.com': 2,\n",
    "    #     'csissis1997@gmail.com': 3,\n",
    "    #     'david000914@gmail.com': 5,\n",
    "    #     'duecjf011521212106@gmail.com': 2.5,\n",
    "    #     'eowjdtjdwls@gmail.com': 3.5,\n",
    "    #     'fpdlsqhdn962@inha.edu': 2,\n",
    "    #     'gh011127@gmail.com': 9,\n",
    "    #     'gurwns7772@gmail.com': 2,\n",
    "    #     'hhcc05@inha.edu': 3,\n",
    "    #     'jinsoonshim@naver.com': 7,\n",
    "    #     'jwshoho4@gmail.com': 8.5,\n",
    "    #     'jyypaul@gmail.com': 5.5,\n",
    "    #     'km55181499@gmail.com': 7,\n",
    "    #     'kpm1323@gmail.com': 2.5,\n",
    "    #     'memm1439@gmail.com': 4.5,\n",
    "    #     'minjae20000207@gmail.com': 6.5,\n",
    "    #     'minrudcho01@gmail.com': 4,\n",
    "    #     'nigaram8@gmail.com': 7,\n",
    "    #     'nnozilaxonim@gmail.com': 10.5,\n",
    "    #     'powerampshere@gmail.com': 3,\n",
    "    #     'rlarkgus11170@inha.edu': 7.5,\n",
    "    #     'unicpn21@gmail.com': 3.5,\n",
    "    #     'vusgowlwk17@gmail.com': 7,\n",
    "    #     'wjdwogus0604@gmail.com': 5.5,\n",
    "    #     'wonjs0725@gmail.com': 5,\n",
    "    #     'zkapdh123@gmail.com': 2.5\n",
    "    # }\n",
    "    # LOOSENESS_THRESHOLDS = {\n",
    "    #     '12170603@inha.edu': 237000,\n",
    "    #     '12181157@inha.edu': 297000,\n",
    "    #     '12212982so@inha.edu': 327000,\n",
    "    #     'as7177as3912@gmail.com': 277000,\n",
    "    #     'bagle1029@gmail.com': 267000,\n",
    "    #     'bw04029@gmail.com': 187000,\n",
    "    #     'chaewoni65@gmail.com': 327000,\n",
    "    #     'csissis1997@gmail.com': 247000,\n",
    "    #     'david000914@gmail.com': 317000,\n",
    "    #     'duecjf011521212106@gmail.com': 247000,\n",
    "    #     'eowjdtjdwls@gmail.com': 237000,\n",
    "    #     'fpdlsqhdn962@inha.edu': 187000,\n",
    "    #     'gh011127@gmail.com': 217000,\n",
    "    #     'gurwns7772@gmail.com': 277000,\n",
    "    #     'hhcc05@inha.edu': 237000,\n",
    "    #     'jinsoonshim@naver.com': 297000,\n",
    "    #     'jwshoho4@gmail.com': 347000,\n",
    "    #     'jyypaul@gmail.com': 197000,\n",
    "    #     'km55181499@gmail.com': 287000,\n",
    "    #     'kpm1323@gmail.com': 247000,\n",
    "    #     'memm1439@gmail.com': 317000,\n",
    "    #     'minjae20000207@gmail.com': 277000,\n",
    "    #     'minrudcho01@gmail.com': 247000,\n",
    "    #     'nigaram8@gmail.com': 347000,\n",
    "    #     'nnozilaxonim@gmail.com': 307000,\n",
    "    #     'powerampshere@gmail.com': 267000,\n",
    "    #     'rlarkgus11170@inha.edu': 237000,\n",
    "    #     'unicpn21@gmail.com': 267000,\n",
    "    #     'vusgowlwk17@gmail.com': 227000,\n",
    "    #     'wjdwogus0604@gmail.com': 227000,\n",
    "    #     'wonjs0725@gmail.com': 267000,\n",
    "    #     'zkapdh123@gmail.com': 287000\n",
    "    # }\n",
    "    # endregion\n",
    "\n",
    "    MOTION_THRESHOLDS_RANGE = np.array(range(20, 60, 2)) / 10\n",
    "    LOOSENESS_THRESHOLDS_RANGE = np.array(range(180, 300, 6)) * 1000\n",
    "    MOTION_THRESHOLDS = {\n",
    "        '12170603@inha.edu': 3.2,\n",
    "        '12181157@inha.edu': 4.6,\n",
    "        '12201686@inha.edu': 2.8,\n",
    "        '12212982so@inha.edu': 4.2,\n",
    "        'as7177as3912@gmail.com': 4.4,\n",
    "        'bagle1029@gmail.com': 4.8,\n",
    "        'bw04029@gmail.com': 3.6,\n",
    "        'chaewoni65@gmail.com': 2.2,\n",
    "        'csissis1997@gmail.com': 2.6,\n",
    "        'david000914@gmail.com': 3.2,\n",
    "        'duecjf011521212106@gmail.com': 5.2,\n",
    "        'eowjdtjdwls@gmail.com': 3.6,\n",
    "        'fpdlsqhdn962@inha.edu': 5.8,\n",
    "        'gh011127@gmail.com': 5.8,\n",
    "        'gurwns7772@gmail.com': 5.0,\n",
    "        'hhcc05@inha.edu': 2.0,\n",
    "        'jinsoonshim@naver.com': 3.6,\n",
    "        'jwshoho4@gmail.com': 2.0,\n",
    "        'jyypaul@gmail.com': 4.8,\n",
    "        'km55181499@gmail.com': 3.0,\n",
    "        'kpm1323@gmail.com': 2.2,\n",
    "        'memm1439@gmail.com': 2.8,\n",
    "        'minjae20000207@gmail.com': 2.8,\n",
    "        'minrudcho01@gmail.com': 4.8,\n",
    "        'nigaram8@gmail.com': 5.2,\n",
    "        'nnozilaxonim@gmail.com': 2.0,\n",
    "        'powerampshere@gmail.com': 4.0,\n",
    "        'rlarkgus11170@inha.edu': 5.8,\n",
    "        'unicpn21@gmail.com': 2.0,\n",
    "        'vusgowlwk17@gmail.com': 3.2,\n",
    "        'wjdwogus0604@gmail.com': 2.4,\n",
    "        'wonjs0725@gmail.com': 5.2,\n",
    "        'zkapdh123@gmail.com': 3.4\n",
    "    }\n",
    "    LOOSENESS_THRESHOLDS = {\n",
    "        '12170603@inha.edu': 204000,\n",
    "        '12181157@inha.edu': 258000,\n",
    "        '12212982so@inha.edu': 180000,\n",
    "        'as7177as3912@gmail.com': 288000,\n",
    "        'bagle1029@gmail.com': 198000,\n",
    "        'bw04029@gmail.com': 216000,\n",
    "        'chaewoni65@gmail.com': 204000,\n",
    "        'csissis1997@gmail.com': 276000,\n",
    "        'david000914@gmail.com': 180000,\n",
    "        'duecjf011521212106@gmail.com': 198000,\n",
    "        'eowjdtjdwls@gmail.com': 288000,\n",
    "        'fpdlsqhdn962@inha.edu': 234000,\n",
    "        'gh011127@gmail.com': 246000,\n",
    "        'gurwns7772@gmail.com': 210000,\n",
    "        'hhcc05@inha.edu': 222000,\n",
    "        'jinsoonshim@naver.com': 180000,\n",
    "        'jwshoho4@gmail.com': 222000,\n",
    "        'jyypaul@gmail.com': 294000,\n",
    "        'km55181499@gmail.com': 276000,\n",
    "        'kpm1323@gmail.com': 210000,\n",
    "        'memm1439@gmail.com': 264000,\n",
    "        'minjae20000207@gmail.com': 276000,\n",
    "        'minrudcho01@gmail.com': 282000,\n",
    "        'nigaram8@gmail.com': 210000,\n",
    "        'nnozilaxonim@gmail.com': 222000,\n",
    "        'powerampshere@gmail.com': 282000,\n",
    "        'rlarkgus11170@inha.edu': 288000,\n",
    "        'unicpn21@gmail.com': 210000,\n",
    "        'vusgowlwk17@gmail.com': 216000,\n",
    "        'wjdwogus0604@gmail.com': 252000,\n",
    "        'wonjs0725@gmail.com': 222000,\n",
    "        'zkapdh123@gmail.com': 228000\n",
    "    }\n",
    "\n",
    "    M_ROUNDS = 30\n",
    "\n",
    "    # time unit: milliseconds\n",
    "    WINDOW_SIZE = 60 * 1000\n",
    "    WINDOW_OVERLAP = 0.75\n",
    "    MIN_WINDOW_SAMPLES = 60\n",
    "    EMA_SPAN = 30 * 60 * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caT-7lUPC1av"
   },
   "source": [
    "# 2. Data processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GAinuuHDDh4"
   },
   "source": [
    "## 2.1. Clean HR signals\n",
    "    > Motion and looseness filters\n",
    "    > Obvious errors by PPG sensor\n",
    "    > Outlier HR readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otIGghPu4-uw"
   },
   "source": [
    "### 2.1.1 Copying untouched data (done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "collapsed": true,
    "id": "pj1T00ak5C9m",
    "outputId": "69174869-53ae-4ec1-9ac8-aad7fa9cde81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab stress\n",
      "\t1. baseline rest\n",
      "\t2. speech instructions\n",
      "\t3. speech preparation\n",
      "\t4. speech stressor\n",
      "\t5. rest\n",
      "\t6. arithmetic instructions\n",
      "\t7. arithmetic stressor p1\n",
      "\t8. arithmetic rest\n",
      "\t9. arithmetic stressor p2\n",
      "\t10. rest\n",
      "\t11. cold instructions\n",
      "\t12. cold stressor\n",
      "\t13. recovery\n",
      "ppg accuracy\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "field stress\n",
      "\t 1/36 jinsoonshim@naver.com\n",
      "\t 2/36 kpm1323@gmail.com\n",
      "\t 3/36 minrudcho01@gmail.com\n",
      "\t 4/36 fpdlsqhdn962@inha.edu\n",
      "\t 5/36 csissis1997@gmail.com\n",
      "\t 6/36 .DS_Store\n",
      "\t 7/36 chaewoni65@gmail.com\n",
      "\t 8/36 eowjdtjdwls@gmail.com\n",
      "\t 9/36 rlarkgus11170@inha.edu\n",
      "\t 10/36 12201686@inha.edu\n",
      "\t 11/36 km55181499@gmail.com\n",
      "\t 12/36 bw04029@gmail.com\n",
      "\t 13/36 vusgowlwk17@gmail.com\n",
      "\t 14/36 rbfl675@inha.edu\n",
      "\t 15/36 minjae20000207@gmail.com\n",
      "\t 16/36 jwshoho4@gmail.com\n",
      "\t 17/36 duecjf011521212106@gmail.com\n",
      "\t 18/36 12212982so@inha.edu\n",
      "\t 19/36 nigaram8@gmail.com\n",
      "\t 20/36 12181157@inha.edu\n",
      "\t 21/36 as7177as3912@gmail.com\n",
      "\t 22/36 nnozilaxonim@gmail.com\n",
      "\t 23/36 powerampshere@gmail.com\n",
      "\t 24/36 wjdwogus0604@gmail.com\n",
      "\t 25/36 gurwns7772@gmail.com\n",
      "\t 26/36 zkapdh123@gmail.com\n",
      "\t 27/36 ysl@inha.edu\n",
      "\t 28/36 jyypaul@gmail.com\n",
      "\t 29/36 wonjs0725@gmail.com\n",
      "\t 30/36 hhcc05@inha.edu\n",
      "\t 31/36 unicpn21@gmail.com\n",
      "\t 32/36 gh011127@gmail.com\n",
      "\t 33/36 bagle1029@gmail.com\n",
      "\t 34/36 memm1439@gmail.com\n",
      "\t 35/36 12170603@inha.edu\n",
      "\t 36/36 david000914@gmail.com\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile as cp\n",
    "from os import listdir as ld\n",
    "import os\n",
    "os.system(f'find {Paths.get(Dirs.ROOT)} -name \".DS_Store\" -delete')\n",
    "\n",
    "\n",
    "# normal copy with exists() check\n",
    "def copy_file(src_path, dst_path):\n",
    "  if os.path.exists(src_path):\n",
    "    if os.path.exists(dst_path):\n",
    "      os.remove(dst_path)\n",
    "    cp(src_path, dst_path)\n",
    "\n",
    "\n",
    "'''\n",
    "(STEP 1)\n",
    "  Copy Lab-Stress and PPG-Accuracy datasets\n",
    "  parent_dir / scenario / participant / data\n",
    "'''\n",
    "datasets = [\n",
    "  ('lab stress', Dirs.SPLIT_LAB_STRESS_DATASET, Dirs.PROCESSED_LAB_STRESS_DATASET),\n",
    "  ('ppg accuracy', Dirs.SPLIT_TREADMILL_DATASET, Dirs.PROCESSED_TREADMILL_DATASET)\n",
    "]\n",
    "for title, src_dataset_key, dst_dataset_key in datasets:\n",
    "  print(title)\n",
    "  scenarios = ld(Paths.get(src_dataset_key))\n",
    "  scenarios.remove('.DS_Store')\n",
    "  scenarios.sort(key=lambda x: int(x[:x.index('.')]))\n",
    "  for scenario in scenarios:\n",
    "    print(f'\\t{scenario}')\n",
    "    # path check\n",
    "    if not os.path.exists(Paths.get(dst_dataset_key, scenario)):\n",
    "      os.mkdir(Paths.get(dst_dataset_key, scenario))\n",
    "    \n",
    "    for participant in ld(Paths.get(src_dataset_key, scenario)):\n",
    "      # path check\n",
    "      if not os.path.exists(Paths.get(dst_dataset_key, scenario, participant)):\n",
    "          os.mkdir(Paths.get(dst_dataset_key, scenario, participant))\n",
    "\n",
    "      # copy other files (i.e. ecg_hr, ecg_rr, ppg_li)\n",
    "      for filename in ['acc', 'ppg_hr', 'ppg_rr', 'ecg_hr', 'ecg_rr', 'ppg_li']:\n",
    "        src_path = Paths.get(src_dataset_key, scenario, participant, f'{filename}.csv')\n",
    "        dst_path = Paths.get(dst_dataset_key, scenario, participant, f'{filename}.csv')\n",
    "        copy_file(src_path, dst_path)\n",
    "\n",
    "      # remove this after ecg check\n",
    "      for filename in ['ecg_hr', 'ecg_rr']:\n",
    "        src_path = Paths.get(src_dataset_key, scenario, participant, f'{filename}.csv')\n",
    "        dst_path = Paths.get(dst_dataset_key, scenario, participant, f'{filename.replace(\"ecg\", \"ppg\")}.csv')\n",
    "        copy_file(src_path, dst_path)\n",
    "\n",
    "\n",
    "'''\n",
    "(STEP 2)\n",
    "  Copy field stress dataset\n",
    "  parent_dir / participant / data\n",
    "'''\n",
    "print('field stress')\n",
    "participants = ld(Paths.get(Dirs.SPLIT_FIELD_DATASET))\n",
    "for idx, participant in enumerate(participants):\n",
    "  print(f'\\t {idx+1}/{len(participants)} {participant}')\n",
    "\n",
    "  # path check\n",
    "  if not os.path.exists(Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant)):\n",
    "      os.mkdir(Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant))\n",
    "\n",
    "  # copy other files (i.e. ecg_hr, ecg_rr, ppg_li)\n",
    "  for filename in ['ppg_hr', 'ppg_rr', 'acc', 'ppg_li']:\n",
    "    src_path = Paths.get(Dirs.SPLIT_FIELD_DATASET, participant, f'{filename}.csv')\n",
    "    dst_path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, f'{filename}.csv')\n",
    "    copy_file(src_path, dst_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnV2IJnsDTXJ"
   },
   "source": [
    "### 2.1.1. Human HR range (counter done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Lnb4xKTCGPfK",
    "outputId": "c05e6131-178e-485c-aaba-1f7570a91388"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " lab stress\n",
      "\t1. baseline rest\n",
      "\t2. speech instructions\n",
      "\t3. speech preparation\n",
      "\t4. speech stressor\n",
      "\t5. rest\n",
      "\t6. arithmetic instructions\n",
      "\t7. arithmetic stressor p1\n",
      "\t8. arithmetic rest\n",
      "\t9. arithmetic stressor p2\n",
      "\t10. rest\n",
      "\t11. cold instructions\n",
      "\t12. cold stressor\n",
      "\t13. recovery\n",
      "# of dropped lines (ppg_hr, ecg_hr, ppg_rr, ecg_rr) 0 0 20 20\n",
      "\n",
      "field stress\n",
      "\t 1/36 jinsoonshim@naver.com (0 hrs removed, 72 rrs removed)\n",
      "\t 2/36 kpm1323@gmail.com (109741 hrs removed, 26885 rrs removed)\n",
      "\t 3/36 minrudcho01@gmail.com (406871 hrs removed, 64804 rrs removed)\n",
      "\t 4/36 fpdlsqhdn962@inha.edu (137273 hrs removed, 30072 rrs removed)\n",
      "\t 5/36 csissis1997@gmail.com (311840 hrs removed, 266828 rrs removed)\n",
      "\t 6/36 .DS_Store (0 hrs removed, 0 rrs removed)\n",
      "\t 7/36 chaewoni65@gmail.com (347712 hrs removed, 22463 rrs removed)\n",
      "\t 8/36 eowjdtjdwls@gmail.com (735038 hrs removed, 217283 rrs removed)\n",
      "\t 9/36 rlarkgus11170@inha.edu (206753 hrs removed, 34759 rrs removed)\n",
      "\t 10/36 12201686@inha.edu (0 hrs removed, 4 rrs removed)\n",
      "\t 11/36 km55181499@gmail.com (130254 hrs removed, 51528 rrs removed)\n",
      "\t 12/36 bw04029@gmail.com (159746 hrs removed, 18825 rrs removed)\n",
      "\t 13/36 vusgowlwk17@gmail.com (0 hrs removed, 2165 rrs removed)\n",
      "\t 14/36 rbfl675@inha.edu (0 hrs removed, 0 rrs removed)\n",
      "\t 15/36 minjae20000207@gmail.com (53632 hrs removed, 10988 rrs removed)\n",
      "\t 16/36 jwshoho4@gmail.com (723115 hrs removed, 53413 rrs removed)\n",
      "\t 17/36 duecjf011521212106@gmail.com (220226 hrs removed, 16784 rrs removed)\n",
      "\t 18/36 12212982so@inha.edu (181812 hrs removed, 62464 rrs removed)\n",
      "\t 19/36 nigaram8@gmail.com (0 hrs removed, 400 rrs removed)\n",
      "\t 20/36 12181157@inha.edu (0 hrs removed, 7833 rrs removed)\n",
      "\t 21/36 as7177as3912@gmail.com (64354 hrs removed, 35357 rrs removed)\n",
      "\t 22/36 nnozilaxonim@gmail.com (68744 hrs removed, 29102 rrs removed)\n",
      "\t 23/36 powerampshere@gmail.com (471358 hrs removed, 34139 rrs removed)\n",
      "\t 24/36 wjdwogus0604@gmail.com (163137 hrs removed, 10238 rrs removed)\n",
      "\t 25/36 gurwns7772@gmail.com (926604 hrs removed, 47051 rrs removed)\n",
      "\t 26/36 zkapdh123@gmail.com (0 hrs removed, 4265 rrs removed)\n",
      "\t 27/36 ysl@inha.edu (149404 hrs removed, 135702 rrs removed)\n",
      "\t 28/36 jyypaul@gmail.com (203756 hrs removed, 11281 rrs removed)\n",
      "\t 29/36 wonjs0725@gmail.com (560608 hrs removed, 506628 rrs removed)\n",
      "\t 30/36 hhcc05@inha.edu (986504 hrs removed, 767381 rrs removed)\n",
      "\t 31/36 unicpn21@gmail.com (183113 hrs removed, 20655 rrs removed)\n",
      "\t 32/36 gh011127@gmail.com (164781 hrs removed, 15508 rrs removed)\n",
      "\t 33/36 bagle1029@gmail.com (1218574 hrs removed, 509779 rrs removed)\n",
      "\t 34/36 memm1439@gmail.com (1719789 hrs removed, 1345876 rrs removed)\n",
      "\t 35/36 12170603@inha.edu (117952 hrs removed, 21553 rrs removed)\n",
      "\t 36/36 david000914@gmail.com (0 hrs removed, 862 rrs removed)\n",
      "# of dropped lines (ppg_hr, ppg_rr): 10722691 4382947\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile as cp\n",
    "from os import listdir as ld\n",
    "import os\n",
    "os.system(f'find {Paths.get(Dirs.ROOT)} -name \".DS_Store\" -delete')\n",
    "\n",
    "\n",
    "# checks HR via range\n",
    "def process_hr_file(path):\n",
    "  if os.path.exists(path):\n",
    "    with open(path, 'r') as r:\n",
    "      lines = r.readlines()\n",
    "      l_ = len(lines)\n",
    "    with open(path, 'w+') as w:\n",
    "      for line in lines:\n",
    "        ts, hr = line[:-1].split(',')\n",
    "        try:\n",
    "          if 30 <= int(hr) <= 220:\n",
    "            w.write(line)\n",
    "            l_-=1\n",
    "        except ValueError:\n",
    "          pass\n",
    "    return l_ #the number of dropped hr lines\n",
    "  else:\n",
    "    return 0                       \n",
    "\n",
    "\n",
    "# checks HR-interval via range\n",
    "def process_rr_file(path):\n",
    "  if os.path.exists(path):\n",
    "    with open(path, 'r') as r:\n",
    "      lines = r.readlines()\n",
    "      l_ = len(lines)\n",
    "    with open(path, 'w+') as w:\n",
    "      for line in lines:\n",
    "        ts, rri = line[:-1].split(',')\n",
    "        try:\n",
    "          if 272 <= int(rri) <= 2000:\n",
    "            w.write(line)\n",
    "            l_-=1\n",
    "        except ValueError:\n",
    "          pass\n",
    "    return l_ #the number of dropped rr lines\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "\n",
    "'''\n",
    "(STEP 1)\n",
    "  Cleaning Lab-Stress and PPG-Accuracy datasets\n",
    "  parent_dir / scenario / participant / data\n",
    "'''\n",
    "datasets = [\n",
    "  ('lab stress', Dirs.PROCESSED_LAB_STRESS_DATASET),\n",
    "  #('ppg accuracy', Dirs.PROCESSED_TREADMILL_DATASET)\n",
    "]\n",
    "for title, dataset_key in datasets:\n",
    "  t_n_ppg_hr = t_n_ecg_hr = t_n_ppg_rr = t_n_ecg_rr = 0\n",
    "  print(\"\\n\",title)\n",
    "  scenarios = ld(Paths.get(dataset_key))\n",
    "  scenarios.remove('.DS_Store')\n",
    "  scenarios.sort(key=lambda x: int(x[:x.index('.')]))\n",
    "  for scenario in scenarios:\n",
    "    print(f'\\t{scenario}')\n",
    "    for participant in ld(Paths.get(dataset_key, scenario)):\n",
    "      # (a) fix heart rate (bpm)\n",
    "      path = Paths.get(dataset_key, scenario, participant, 'ppg_hr.csv')\n",
    "      n_ppg_hr = process_hr_file(path)\n",
    "      t_n_ppg_hr += n_ppg_hr\n",
    "      path = Paths.get(dataset_key, scenario, participant, 'ecg_hr.csv')\n",
    "      n_ecg_hr = process_hr_file(path)\n",
    "      t_n_ecg_hr += n_ecg_hr\n",
    "\n",
    "      # (b) fix interbeat-intervals (ms)\n",
    "      path = Paths.get(dataset_key, scenario, participant, 'ppg_rr.csv')\n",
    "      n_ppg_rr = process_rr_file(path)\n",
    "      t_n_ppg_rr += n_ppg_rr\n",
    "      path = Paths.get(dataset_key, scenario, participant, 'ecg_rr.csv')\n",
    "      n_ecg_rr = process_rr_file(path)\n",
    "      t_n_ecg_rr += n_ecg_rr\n",
    "  print(\"# of dropped lines (ppg_hr, ecg_hr, ppg_rr, ecg_rr)\", t_n_ppg_hr, t_n_ecg_hr, t_n_ppg_rr, t_n_ecg_rr)\n",
    "\n",
    "'''\n",
    "(STEP 2)\n",
    "  Cleaning field stress dataset\n",
    "  parent_dir / participant / data\n",
    "'''\n",
    "print('\\nfield stress')\n",
    "t_n_ppg_hr = t_n_ppg_rr = 0\n",
    "participants = ld(Paths.get(Dirs.PROCESSED_FIELD_DATASET))\n",
    "for idx, participant in enumerate(participants):\n",
    "  print(f'\\t {idx+1}/{len(participants)} {participant}', end = ' ')\n",
    "\n",
    "  # (a) fix heart rate (bpm)\n",
    "  path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, 'ppg_hr.csv')\n",
    "  n_ppg_hr = process_hr_file(path)\n",
    "  print(f'({n_ppg_hr} hrs removed, ', end='')\n",
    "  t_n_ppg_hr += n_ppg_hr\n",
    "\n",
    "  # (b) fix interbeat-intervals (ms)\n",
    "  path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, 'ppg_rr.csv')\n",
    "  n_ppg_rr = process_rr_file(path)\n",
    "  print(f'{n_ppg_rr} rrs removed)')\n",
    "  t_n_ppg_rr += n_ppg_rr\n",
    "print(\"# of dropped lines (ppg_hr, ppg_rr):\", t_n_ppg_hr, t_n_ppg_rr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXjzFgNsDWLH"
   },
   "source": [
    "### 2.1.2. Outlier trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "C6ajK6c7lRzz",
    "outputId": "7960f3c2-b09b-4d8f-c058-96f4878fb96d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " lab stress\n",
      "\t1. baseline rest\n",
      "\t2. speech instructions\n",
      "\t3. speech preparation\n",
      "\t4. speech stressor\n",
      "\t5. rest\n",
      "\t6. arithmetic instructions\n",
      "\t7. arithmetic stressor p1\n",
      "\t8. arithmetic rest\n",
      "\t9. arithmetic stressor p2\n",
      "\t10. rest\n",
      "\t11. cold instructions\n",
      "\t12. cold stressor\n",
      "\t13. recovery\n",
      "# of dropped lines (ppg_hr, ecg_hr, ppg_rr, ecg_rr) 0 0 0 0\n",
      "\n",
      "field stress\n",
      "\t 1/36 jinsoonshim@naver.com\n",
      "\t 2/36 kpm1323@gmail.com\n",
      "\t 3/36 minrudcho01@gmail.com\n",
      "\t 4/36 fpdlsqhdn962@inha.edu\n",
      "\t 5/36 csissis1997@gmail.com\n",
      "\t 6/36 .DS_Store\n",
      "\t 7/36 chaewoni65@gmail.com\n",
      "\t 8/36 eowjdtjdwls@gmail.com\n",
      "\t 9/36 rlarkgus11170@inha.edu\n",
      "\t 10/36 12201686@inha.edu\n",
      "\t 11/36 km55181499@gmail.com\n",
      "\t 12/36 bw04029@gmail.com\n",
      "\t 13/36 vusgowlwk17@gmail.com\n",
      "\t 14/36 rbfl675@inha.edu\n",
      "\t 15/36 minjae20000207@gmail.com\n",
      "\t 16/36 jwshoho4@gmail.com\n",
      "\t 17/36 duecjf011521212106@gmail.com\n",
      "\t 18/36 12212982so@inha.edu\n",
      "\t 19/36 nigaram8@gmail.com\n",
      "\t 20/36 12181157@inha.edu\n",
      "\t 21/36 as7177as3912@gmail.com\n",
      "\t 22/36 nnozilaxonim@gmail.com\n",
      "\t 23/36 powerampshere@gmail.com\n",
      "\t 24/36 wjdwogus0604@gmail.com\n",
      "\t 25/36 gurwns7772@gmail.com\n",
      "\t 26/36 zkapdh123@gmail.com\n",
      "\t 27/36 ysl@inha.edu\n",
      "\t 28/36 jyypaul@gmail.com\n",
      "\t 29/36 wonjs0725@gmail.com\n",
      "\t 30/36 hhcc05@inha.edu\n",
      "\t 31/36 unicpn21@gmail.com\n",
      "\t 32/36 gh011127@gmail.com\n",
      "\t 33/36 bagle1029@gmail.com\n",
      "\t 34/36 memm1439@gmail.com\n",
      "\t 35/36 12170603@inha.edu\n",
      "\t 36/36 david000914@gmail.com\n",
      "# of dropped lines (ppg_hr, ppg_rr): 0 0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import median_absolute_deviation\n",
    "from shutil import copyfile as cp\n",
    "from os import listdir as ld\n",
    "import numpy as np\n",
    "import os\n",
    "os.system(f'find {Paths.get(Dirs.ROOT)} -name \".DS_Store\" -delete')\n",
    "\n",
    "\n",
    "# checks via 3*MAD range\n",
    "def process_hr_file(path):\n",
    "  if os.path.exists(path):\n",
    "    with open(path, 'r') as r:\n",
    "      hrs = []\n",
    "      lines = []\n",
    "      for line in r:\n",
    "          cells = line[:-1].split(' ')\n",
    "          if len(cells) == 2:\n",
    "            hr = cells[1]\n",
    "            if hr.isdigit():\n",
    "                hrs += [int(hr)]\n",
    "                lines += [line]\n",
    "      l_ = len(lines)\n",
    "    \n",
    "    if len(hrs) == 0:\n",
    "      return 0\n",
    "\n",
    "    trim_min = np.median(hrs) - 3 * median_absolute_deviation(hrs)\n",
    "    trim_max = np.median(hrs) + 3 * median_absolute_deviation(hrs)\n",
    "    \n",
    "    with open(path, 'w+') as w:\n",
    "      for line in lines:\n",
    "        ts, hr = line[:-1].split(',')\n",
    "        try:\n",
    "          if trim_min <= int(hr) <= trim_max:\n",
    "            w.write(line)\n",
    "            l_-=1\n",
    "        except ValueError:\n",
    "          pass\n",
    "    return l_ #the number of dropped hr lines\n",
    "  else:\n",
    "    return 0                       \n",
    "\n",
    "\n",
    "# checks via 3*MAD range\n",
    "def process_rr_file(path):\n",
    "  if os.path.exists(path):\n",
    "    with open(path, 'r') as r:\n",
    "      rr_intervals = []\n",
    "      lines = []\n",
    "      for line in r:\n",
    "          cells = line[:-1].split(' ')\n",
    "          if len(cells) == 2:\n",
    "            rr = cells[1]\n",
    "            if rr.isdigit():\n",
    "                rr_intervals += [int(rr)]\n",
    "                lines += [line]\n",
    "      l_ = len(lines)\n",
    "    \n",
    "    if len(rr_intervals) == 0:\n",
    "      return 0\n",
    "\n",
    "    trim_min = np.median(rr_intervals) - 3 * median_absolute_deviation(rr_intervals)\n",
    "    trim_max = np.median(rr_intervals) + 3 * median_absolute_deviation(rr_intervals)\n",
    "\n",
    "    with open(path, 'w+') as w:\n",
    "      for line in lines:\n",
    "        ts, rri = line[:-1].split(',')\n",
    "        try:\n",
    "          if trim_min <= int(rri) <= trim_max:\n",
    "            w.write(line)\n",
    "            l_-=1\n",
    "        except ValueError:\n",
    "          pass\n",
    "    return l_ #the number of dropped rr lines\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "\n",
    "'''\n",
    "(STEP 1)\n",
    "  Cleaning Lab-Stress and PPG-Accuracy datasets\n",
    "  parent_dir / scenario / participant / data\n",
    "'''\n",
    "datasets = [\n",
    "  ('lab stress', Dirs.PROCESSED_LAB_STRESS_DATASET),\n",
    "  #('ppg accuracy', Dirs.PROCESSED_TREADMILL_DATASET)\n",
    "]\n",
    "t_n_ppg_hr = t_n_ppg_rr = 0\n",
    "for title, dataset_key in datasets:\n",
    "  t_n_ppg_hr = t_n_ecg_hr = t_n_ppg_rr = t_n_ecg_rr = 0\n",
    "  print(\"\\n\",title)\n",
    "  scenarios = ld(Paths.get(dataset_key))\n",
    "  scenarios.remove('.DS_Store')\n",
    "  scenarios.sort(key=lambda x: int(x[:x.index('.')]))\n",
    "  for scenario in scenarios:\n",
    "    print(f'\\t{scenario}')\n",
    "    for participant in ld(Paths.get(dataset_key, scenario)):\n",
    "      # (a) fix heart rate (bpm)\n",
    "      path = Paths.get(dataset_key, scenario, participant, 'ppg_hr.csv')\n",
    "      n_ppg_hr = process_hr_file(path)\n",
    "      t_n_ppg_hr += n_ppg_hr\n",
    "      path = Paths.get(dataset_key, scenario, participant, 'ecg_hr.csv')\n",
    "      n_ecg_hr = process_hr_file(path)\n",
    "      t_n_ecg_hr += n_ecg_hr\n",
    "\n",
    "      # (b) fix interbeat-intervals (ms)\n",
    "      path = Paths.get(dataset_key, scenario, participant, 'ppg_rr.csv')\n",
    "      n_ppg_rr = process_rr_file(path)\n",
    "      t_n_ppg_rr += n_ppg_rr\n",
    "      path = Paths.get(dataset_key, scenario, participant, 'ecg_rr.csv')\n",
    "      n_ecg_rr = process_rr_file(path)\n",
    "      t_n_ecg_rr += n_ecg_rr\n",
    "  print(\"# of dropped lines (ppg_hr, ecg_hr, ppg_rr, ecg_rr)\", t_n_ppg_hr, t_n_ecg_hr, t_n_ppg_rr, t_n_ecg_rr)\n",
    "\n",
    "'''\n",
    "(STEP 2)\n",
    "  Cleaning field stress dataset\n",
    "  parent_dir / participant / data\n",
    "'''\n",
    "print('\\nfield stress')\n",
    "t_n_ppg_hr = t_n_ppg_rr = 0\n",
    "participants = ld(Paths.get(Dirs.PROCESSED_FIELD_DATASET))\n",
    "for idx, participant in enumerate(participants):\n",
    "  print(f'\\t {idx+1}/{len(participants)} {participant}')\n",
    "\n",
    "  # (a) fix heart rate (bpm)\n",
    "  path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, 'ppg_hr.csv')\n",
    "  n_ppg_hr = process_hr_file(path)\n",
    "  t_n_ppg_hr += n_ppg_hr\n",
    "\n",
    "  # (b) fix interbeat-intervals (ms)\n",
    "  path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, 'ppg_rr.csv')\n",
    "  n_ppg_rr = process_rr_file(path)\n",
    "  t_n_ppg_rr += n_ppg_rr\n",
    "print(\"# of dropped lines (ppg_hr, ppg_rr):\", t_n_ppg_hr, t_n_ppg_rr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbHZCikvDZrb"
   },
   "source": [
    "### 2.1.3. Motion filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ql5ugV85fb4s"
   },
   "source": [
    "#### (a) Initial cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "mGutEEFvuLxP"
   },
   "outputs": [],
   "source": [
    "# initial cleanup\n",
    "from os import listdir as ld\n",
    "import os\n",
    "os.system(f'find {Paths.get(Dirs.ROOT)} -name \".DS_Store\" -delete')\n",
    "\n",
    "# treadmill and lab stress datasets\n",
    "for key in [Dirs.SPLIT_TREADMILL_DATASET, Dirs.SPLIT_LAB_STRESS_DATASET, Dirs.PROCESSED_TREADMILL_DATASET, Dirs.PROCESSED_LAB_STRESS_DATASET]:\n",
    "  print(key)\n",
    "  scenarios = ld(Paths.get(key))\n",
    "  for scenario in scenarios:\n",
    "    for participant in ld(Paths.get(key, scenario)):\n",
    "      path = Paths.get(key, scenario, participant, 'acc.csv')\n",
    "      if os.path.exists(path):\n",
    "        with open(path) as r:\n",
    "          lines = r.readlines()\n",
    "        with open(path, 'w') as w:\n",
    "          for line in lines:\n",
    "            w.write(line.replace('\"\\n', '\\n'))\n",
    "\n",
    "# field stress dataset\n",
    "for key in [Dirs.SPLIT_FIELD_DATASET, Dirs.PROCESSED_FIELD_DATASET]:\n",
    "  for participant in ld(Paths.get(key)):\n",
    "    path = Paths.get(key, participant, 'acc.csv')\n",
    "    if os.path.exists(path):\n",
    "      with open(path) as r:\n",
    "        lines = r.readlines()\n",
    "      with open(path, 'w') as w:\n",
    "        for line in lines:\n",
    "          w.write(line.replace('\"\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14aMd1EifgRb"
   },
   "source": [
    "#### (b) filter out motion artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "AweAHkj8yXDo",
    "outputId": "e72f2f9b-93c6-492b-93ce-949a355040ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "field stress\n",
      "1/35 participant: jinsoonshim@naver.com\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import median_absolute_deviation\n",
    "from shutil import copyfile as cp\n",
    "from os import listdir as ld\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import os\n",
    "import math\n",
    "os.system(f'find {Paths.get(Dirs.ROOT)} -name \".DS_Store\" -delete')\n",
    "\n",
    "\n",
    "WINDOW_SIZE = 30000 # ms\n",
    "SUB_WINDOWS_COUNT = 5\n",
    "OVERLAP = 0.5 # half of window size\n",
    "MILD_THRESHOLD = 0.35\n",
    "\n",
    "def clean_motion_artifacts(acc_df, hrv_df, threshold=0.21384):\n",
    "  if acc_df.empty:\n",
    "    return hrv_df.iloc[0:0], 0, hrv_df.shape[0] # drop all\n",
    "  if hrv_df.empty:\n",
    "    return hrv_df, 0, 0 # empty set\n",
    "  drop_count = 0\n",
    "  drop_missing_data_count = 0\n",
    "  hrv_df.sort_values(by=['ts'])\n",
    "  \n",
    "  from_ts = hrv_df.iloc[0]['ts']\n",
    "  till_ts = hrv_df.iloc[-1]['ts']\n",
    "\n",
    "  while from_ts < till_ts:\n",
    "    sub_acc_df = acc_df.query(f'ts >= {from_ts} and ts < {from_ts + WINDOW_SIZE}')\n",
    "    \n",
    "    if sub_acc_df.empty:\n",
    "      drop_indices = hrv_df.query(f'ts >= {from_ts} and ts < {from_ts + WINDOW_SIZE}').index\n",
    "      drop_missing_data_count += len(drop_indices)\n",
    "      hrv_df.drop(drop_indices, inplace=True)\n",
    "      from_ts += WINDOW_SIZE\n",
    "      continue\n",
    "    \n",
    "    activeness_scores = []\n",
    "    _ts = from_ts\n",
    "    while _ts < from_ts + WINDOW_SIZE:\n",
    "      _df = sub_acc_df.query(f'ts >= {_ts} and ts < {_ts + WINDOW_SIZE / SUB_WINDOWS_COUNT}')\n",
    "      magnitudes = []\n",
    "      for _, _r in _df.iterrows():\n",
    "        magnitudes += [math.sqrt(_r['x'] ** 2 + _r['y'] ** 2 + _r['z'] ** 2)]\n",
    "      if len(magnitudes) > 0:\n",
    "        activeness_scores += [np.mean(magnitudes)]\n",
    "      _ts += WINDOW_SIZE  / SUB_WINDOWS_COUNT\n",
    "    \n",
    "    active_count = sum((1 for score in activeness_scores if score > threshold))\n",
    "    if active_count > SUB_WINDOWS_COUNT / 2:\n",
    "      drop_indices = hrv_df.query(f'ts >= {from_ts} and ts < {from_ts + WINDOW_SIZE}').index\n",
    "      drop_count += len(drop_indices)\n",
    "      hrv_df.drop(drop_indices, inplace=True)\n",
    "      from_ts += WINDOW_SIZE\n",
    "      continue\n",
    "    from_ts += WINDOW_SIZE * OVERLAP\n",
    "  return hrv_df, drop_count, drop_missing_data_count\n",
    "\n",
    "\n",
    "'''\n",
    "(STEP 1)\n",
    "  Cleaning Lab-Stress dataset\n",
    "  parent_dir / scenario / participant / data\n",
    "'''\n",
    "print('lab stress')\n",
    "participants = os.listdir(Paths.get(Dirs.PROCESSED_FIELD_DATASET))\n",
    "scenarios = ld(Paths.get(Dirs.PROCESSED_LAB_STRESS_DATASET))\n",
    "if '.DS_Store':\n",
    "  scenarios.remove('.DS_Store')\n",
    "scenarios.sort(key=lambda x: int(x[:x.index('.')]))\n",
    "for index, participant in enumerate(participants):\n",
    "  rr_total_count, rr_drop_count = 0, 0\n",
    "  hr_total_count, hr_drop_count = 0, 0\n",
    "  print(f'{index+1}/{len(participants)} participant: {participant}')\n",
    "  \n",
    "  for scenario in scenarios:  \n",
    "    # fix paths\n",
    "    if not os.path.exists(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario)):\n",
    "      os.mkdir(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario))\n",
    "      os.mkdir(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario, participant))\n",
    "    elif not os.path.exists(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario, participant)):\n",
    "      os.mkdir(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario, participant))\n",
    "    \n",
    "    # print(f'\\t{scenario}')\n",
    "    acc_path = Paths.get(Dirs.PROCESSED_LAB_STRESS_DATASET, scenario, participant, 'acc.csv')\n",
    "    if os.path.exists(acc_path):\n",
    "      acc_df = pd.read_csv(acc_path, names=['ts', 'x', 'y', 'z'], header=None)\n",
    "      rr_path = Paths.get(Dirs.PROCESSED_LAB_STRESS_DATASET, scenario, participant, 'ppg_rr.csv')\n",
    "      if os.path.exists(rr_path):\n",
    "        rr_df = pd.read_csv(rr_path, names=['ts', 'rr'], header=None)\n",
    "        rr_count = rr_df.shape[0]\n",
    "        rr_total_count += rr_count\n",
    "        rr_df, drop_count, drop_missing_data_count = clean_motion_artifacts(acc_df=acc_df, hrv_df=rr_df, threshold=MILD_THRESHOLD)\n",
    "        rr_drop_count += drop_count\n",
    "        #print(f'(rr) drop/max: {rr_drop_count}/{rr_count}, missing_data_drops={drop_missing_data_count}')\n",
    "        rr_df.to_csv(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario, participant, 'ppg_rr.csv'), header=None)\n",
    "      hr_path = Paths.get(Dirs.PROCESSED_LAB_STRESS_DATASET, scenario, participant, 'ppg_hr.csv')\n",
    "      if os.path.exists(hr_path):\n",
    "        hr_df = pd.read_csv(hr_path, names=['ts', 'hr'], header=None)\n",
    "        hr_count = hr_df.shape[0]\n",
    "        hr_total_count += hr_count\n",
    "        hr_df, drop_count, drop_missing_data_count = clean_motion_artifacts(acc_df=acc_df, hrv_df=hr_df, threshold=MILD_THRESHOLD)\n",
    "        hr_drop_count += drop_count\n",
    "        #print(f'(hr) drop/max: {hr_drop_count}/{hr_count}, missing_data_drops={drop_missing_data_count}')\n",
    "        hr_df.to_csv(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario, participant, 'ppg_hr.csv'), header=None)\n",
    "  print(f'(rr_drop_count/rr_total, hr_drop_count/hr_total) = ({rr_drop_count}/{rr_total_count}, {hr_drop_count}/{hr_total_count})')\n",
    "\n",
    "'''\n",
    "(STEP 2)\n",
    "  Cleaning field stress dataset\n",
    "  parent_dir / participant / data\n",
    "'''\n",
    "print('field stress')\n",
    "participants = os.listdir(Paths.get(Dirs.PROCESSED_FIELD_DATASET))\n",
    "for index, participant in enumerate(participants):\n",
    "  rr_total_count, rr_drop_count = 0, 0\n",
    "  hr_total_count, hr_drop_count = 0, 0\n",
    "  print(f'{index+1}/{len(participants)} participant: {participant}')\n",
    "  \n",
    "  # fix paths\n",
    "  if not os.path.exists(Paths.get(Dirs.FILTERED_FIELD_DATASET, participant)):\n",
    "    os.mkdir(Paths.get(Dirs.FILTERED_FIELD_DATASET, participant))\n",
    "  \n",
    "  acc_path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, 'acc.csv')\n",
    "  if os.path.exists(acc_path):\n",
    "    acc_df = pd.read_csv(acc_path, names=['ts', 'x', 'y', 'z'], header=None)\n",
    "    rr_path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, 'ppg_rr.csv')\n",
    "    if os.path.exists(rr_path):\n",
    "      rr_df = pd.read_csv(rr_path, names=['ts', 'rr'], header=None)\n",
    "      rr_count = rr_df.shape[0]\n",
    "      rr_total_count += rr_count\n",
    "      rr_df, drop_count, drop_missing_data_count = clean_motion_artifacts(acc_df=acc_df, hrv_df=rr_df)\n",
    "      rr_drop_count += drop_count\n",
    "      #print(f'(rr) drop/max: {drop_count}/{rr_count}, missing_data_drops={drop_missing_data_count}')\n",
    "      rr_df.to_csv(Paths.get(Dirs.FILTERED_FIELD_DATASET, participant, 'ppg_rr.csv'), header=None)\n",
    "    hr_path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, 'ppg_hr.csv')\n",
    "    if os.path.exists(hr_path):\n",
    "      hr_df = pd.read_csv(hr_path, names=['ts', 'hr'], header=None)\n",
    "      hr_count = hr_df.shape[0]\n",
    "      hr_total_count += hr_count\n",
    "      hr_df, drop_count, drop_missing_data_count = clean_motion_artifacts(acc_df=acc_df, hrv_df=hr_df)\n",
    "      hr_drop_count += drop_count\n",
    "      #print(f'(hr) drop/max: {drop_count}/{hr_count}, missing_data_drops={drop_missing_data_count}')\n",
    "      hr_df.to_csv(Paths.get(Dirs.FILTERED_FIELD_DATASET, participant, 'ppg_hr.csv'), header=None)\n",
    "  print(f'(rr_drop_count/rr_total, hr_drop_count/hr_total) = ({rr_drop_count}/{rr_total_count}, {hr_drop_count}/{hr_total_count})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vqLP9uG2oZ2"
   },
   "source": [
    "### Backup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "collapsed": true,
    "id": "B9juJqFyhpvk",
    "outputId": "342b5910-784e-4915-d819-177c24370311"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ysl@inha.edu\n",
      "-1.551918029785156 3.815654993057251 -1.580695867538452\n",
      "0.0225224494934082 0.1271672248840332 0.08612251281738281\n",
      "wonjs0725@gmail.com\n",
      "zkapdh123@gmail.com\n",
      "gh011127@gmail.com\n",
      "hhcc05@inha.edu\n",
      "bagle1029@gmail.com\n",
      "12170603@inha.edu\n",
      "memm1439@gmail.com\n",
      "david000914@gmail.com\n",
      "jinsoonshim@naver.com\n",
      "\t static\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0cff9cbc241b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticipant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t static'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{participant}: ${acc_values[participant]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'jinsoonshim@naver.com'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy.stats import median_absolute_deviation\n",
    "from collections import OrderedDict\n",
    "from shutil import copyfile as cp\n",
    "from os import listdir as ld\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "os.system(f'find {Paths.get(Dirs.ROOT)} -name \".DS_Store\" -delete')\n",
    "\n",
    "\n",
    "# get list of participants\n",
    "participants = os.listdir(Paths.get(Dirs.PROCESSED_FIELD_DATASET))\n",
    "\n",
    "# identify thresholds using preliminary dataset\n",
    "scenarios = ld(Paths.get(Dirs.PROCESSED_TREADMILL_DATASET))\n",
    "scenarios.sort(key=lambda x: int(x[:x.index('.')]))\n",
    "\n",
    "acc_values = OrderedDict()\n",
    "li_values = OrderedDict()\n",
    "for participant in participants:\n",
    "  print(participant)\n",
    "\n",
    "  static_magnitudes = []\n",
    "  walking_magnitudes = []\n",
    "  running_magnitudes = []\n",
    "\n",
    "  tight_intensities = []\n",
    "  medium_intensities = []\n",
    "  loose_intensities = []\n",
    "  \n",
    "  for scenario in scenarios:\n",
    "    path = Paths.get(Dirs.PROCESSED_TREADMILL_DATASET, scenario, participant, 'acc.csv')\n",
    "    if 'tight' in scenario:\n",
    "      intensities = tight_intensities\n",
    "    elif 'medium' in scenario:\n",
    "      intensities = medium_intensities\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "      df = pd.read_csv(path, names=['ts', 'li'], header=None)\n",
    "      for _, r in df.iterrows():\n",
    "        intensities += [r['li']]\n",
    "    acc_values[participant] = {\n",
    "      'static': (np.mean(static_magnitudes), np.var(static_magnitudes)),\n",
    "      'walking': (np.mean(walking_magnitudes), np.var(walking_magnitudes)),\n",
    "      'running': (np.mean(running_magnitudes), np.var(running_magnitudes)),\n",
    "    }\n",
    "\n",
    "\n",
    "for participant in participants:\n",
    "  print(participant)\n",
    "  print('\\t static', )\n",
    "  print(f'{participant}: {acc_values[participant]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0eYLQzmDfdK"
   },
   "source": [
    "### 2.1.4. Looseness filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "CSuLVSv-RYvG",
    "outputId": "e02f1bab-2d62-49c7-d618-347bc258971a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/35 participant: jinsoonshim@naver.com\n",
      "2/35 participant: kpm1323@gmail.com\n",
      "==> acc = 81.25\n",
      "3/35 participant: minrudcho01@gmail.com\n",
      "==> acc = 66.66666666666666\n",
      "4/35 participant: fpdlsqhdn962@inha.edu\n",
      "5/35 participant: csissis1997@gmail.com\n",
      "6/35 participant: chaewoni65@gmail.com\n",
      "==> acc = 57.89473684210527\n",
      "7/35 participant: eowjdtjdwls@gmail.com\n",
      "==> acc = 91.66666666666666\n",
      "8/35 participant: rlarkgus11170@inha.edu\n",
      "9/35 participant: 12201686@inha.edu\n",
      "10/35 participant: km55181499@gmail.com\n",
      "==> acc = 78.26086956521739\n",
      "11/35 participant: bw04029@gmail.com\n",
      "==> acc = 63.1578947368421\n",
      "12/35 participant: vusgowlwk17@gmail.com\n",
      "13/35 participant: rbfl675@inha.edu\n",
      "14/35 participant: minjae20000207@gmail.com\n",
      "==> acc = 82.35294117647058\n",
      "15/35 participant: jwshoho4@gmail.com\n",
      "==> acc = 56.25\n",
      "16/35 participant: duecjf011521212106@gmail.com\n",
      "==> acc = 100.0\n",
      "17/35 participant: 12212982so@inha.edu\n",
      "==> acc = 60.0\n",
      "18/35 participant: nigaram8@gmail.com\n",
      "19/35 participant: 12181157@inha.edu\n",
      "20/35 participant: as7177as3912@gmail.com\n",
      "21/35 participant: nnozilaxonim@gmail.com\n",
      "22/35 participant: powerampshere@gmail.com\n",
      "23/35 participant: wjdwogus0604@gmail.com\n",
      "==> acc = 73.91304347826086\n",
      "24/35 participant: gurwns7772@gmail.com\n",
      "==> acc = 80.95238095238095\n",
      "25/35 participant: zkapdh123@gmail.com\n",
      "==> acc = 95.83333333333334\n",
      "26/35 participant: ysl@inha.edu\n",
      "==> acc = 76.19047619047619\n",
      "27/35 participant: jyypaul@gmail.com\n",
      "==> acc = 100.0\n",
      "28/35 participant: wonjs0725@gmail.com\n",
      "==> acc = 100.0\n",
      "29/35 participant: hhcc05@inha.edu\n",
      "==> acc = 95.65217391304348\n",
      "30/35 participant: unicpn21@gmail.com\n",
      "==> acc = 73.68421052631578\n",
      "31/35 participant: gh011127@gmail.com\n",
      "==> acc = 100.0\n",
      "32/35 participant: bagle1029@gmail.com\n",
      "==> acc = 88.23529411764706\n",
      "33/35 participant: memm1439@gmail.com\n",
      "==> acc = 86.66666666666667\n",
      "34/35 participant: 12170603@inha.edu\n",
      "==> acc = 85.18518518518519\n",
      "35/35 participant: david000914@gmail.com\n",
      "==> acc = 78.57142857142857\n",
      "\n",
      "lab stress\n",
      "1/35 participant: jinsoonshim@naver.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (0/0, 0/0)\n",
      "2/35 participant: kpm1323@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (3658/8412, 3660/8414)\n",
      "3/35 participant: minrudcho01@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (0/0, 0/0)\n",
      "4/35 participant: fpdlsqhdn962@inha.edu\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (0/0, 0/0)\n",
      "5/35 participant: csissis1997@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (11660/80710, 11660/80796)\n",
      "6/35 participant: chaewoni65@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (480/7906, 480/7910)\n",
      "7/35 participant: eowjdtjdwls@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (300/6004, 300/6004)\n",
      "8/35 participant: rlarkgus11170@inha.edu\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (0/17628, 0/17640)\n",
      "9/35 participant: 12201686@inha.edu\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (0/0, 0/0)\n",
      "10/35 participant: km55181499@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (849/4003, 870/4031)\n",
      "11/35 participant: bw04029@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (870/6162, 870/6162)\n",
      "12/35 participant: vusgowlwk17@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (0/7610, 0/7610)\n",
      "13/35 participant: rbfl675@inha.edu\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (0/0, 0/0)\n",
      "14/35 participant: minjae20000207@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (0/7702, 0/8336)\n",
      "15/35 participant: jwshoho4@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (452/778, 454/780)\n",
      "16/35 participant: duecjf011521212106@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (0/8414, 0/8422)\n",
      "17/35 participant: 12212982so@inha.edu\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (0/0, 0/0)\n",
      "18/35 participant: nigaram8@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (0/0, 0/0)\n",
      "19/35 participant: 12181157@inha.edu\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (0/0, 0/0)\n",
      "20/35 participant: as7177as3912@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (3494/7196, 3494/7196)\n",
      "21/35 participant: nnozilaxonim@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (0/7328, 0/7328)\n",
      "22/35 participant: powerampshere@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (0/0, 0/0)\n",
      "23/35 participant: wjdwogus0604@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (0/0, 0/0)\n",
      "24/35 participant: gurwns7772@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (0/8144, 0/8144)\n",
      "25/35 participant: zkapdh123@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (0/0, 0/0)\n",
      "26/35 participant: ysl@inha.edu\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (330/3900, 330/3904)\n",
      "27/35 participant: jyypaul@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (706/14628, 706/14628)\n",
      "28/35 participant: wonjs0725@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (0/2861, 0/2925)\n",
      "29/35 participant: hhcc05@inha.edu\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (0/7212, 0/7212)\n",
      "30/35 participant: unicpn21@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (0/3895, 0/3895)\n",
      "31/35 participant: gh011127@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (1424/4065, 1470/4146)\n",
      "32/35 participant: bagle1029@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (840/8156, 840/8158)\n",
      "33/35 participant: memm1439@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (3780/8364, 3780/8366)\n",
      "34/35 participant: 12170603@inha.edu\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (329/2488, 330/2493)\n",
      "35/35 participant: david000914@gmail.com\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (60/8044, 60/8044)\n",
      "field stress\n",
      "1/35 participant: jinsoonshim@naver.com\n",
      "2/35 participant: kpm1323@gmail.com3/35 participant: minrudcho01@gmail.com\n",
      "\n",
      "4/35 participant: fpdlsqhdn962@inha.edu5/35 participant: csissis1997@gmail.com\n",
      "\n",
      "6/35 participant: chaewoni65@gmail.com\n",
      "7/35 participant: eowjdtjdwls@gmail.com\n",
      "8/35 participant: rlarkgus11170@inha.edu\n",
      "9/35 participant: 12201686@inha.edu\n",
      "10/35 participant: km55181499@gmail.com\n",
      "11/35 participant: bw04029@gmail.com\n",
      "12/35 participant: vusgowlwk17@gmail.com\n",
      "13/35 participant: rbfl675@inha.edu\n",
      "14/35 participant: minjae20000207@gmail.com\n",
      "15/35 participant: jwshoho4@gmail.com\n",
      "16/35 participant: duecjf011521212106@gmail.com\n",
      "17/35 participant: 12212982so@inha.edu\n",
      "18/35 participant: nigaram8@gmail.com\n",
      "19/35 participant: 12181157@inha.edu\n",
      "20/35 participant: as7177as3912@gmail.com\n",
      "21/35 participant: nnozilaxonim@gmail.com\n",
      "22/35 participant: powerampshere@gmail.com\n",
      "23/35 participant: wjdwogus0604@gmail.com\n",
      "24/35 participant: gurwns7772@gmail.com\n",
      "25/35 participant: zkapdh123@gmail.com\n",
      "26/35 participant: ysl@inha.edu\n",
      "27/35 participant: jyypaul@gmail.com\n",
      "28/35 participant: wonjs0725@gmail.com\n",
      "29/35 participant: hhcc05@inha.edu\n",
      "30/35 participant: unicpn21@gmail.com\n",
      "31/35 participant: gh011127@gmail.com\n",
      "32/35 participant: bagle1029@gmail.com\n",
      "33/35 participant: memm1439@gmail.com\n",
      "34/35 participant: 12170603@inha.edu\n",
      "35/35 participant: david000914@gmail.com\n",
      "1%, 2%, 4%, 6%, 7%, 8%, 1%, 9%, 10%, 11%, 2%, 1%, 2%, 3%, 12%, 4%, 5%, 13%, 6%, 14%, 15%7%, , 16%, 8%, 17%, 9%, 10%, 11%, 12%, 13%, 14%, 15%, 16%, 17%, 18%, 19%, 20%, 21%, 22%, 23%, 24%, 25%, 26%, 27%, 28%, 29%, 30%, 31%, 32%, 33%, 34%, 35%, 36%, 37%, 38%, 39%, 40%, 41%, 42%, 43%, 44%, 45%, 46%, 47%, 48%, 49%, 50%, 51%, 52%, 53%, 54%, 55%, 56%, 57%, 58%, 59%, 1%, 60%, 61%, 62%, 63%, 64%, 2%65%, , 66%, 67%, 68%, 69%, 70%, 71%, 72%, 3%, 73%, 74%, 18%, 3%, 75%, 76%, 77%, 78%, 79%, 80%19%, , 20%, 4%, 81%, 21%, 82%, 22%83%, , 84%23%, , 85%, 86%, 87%, 88%, 5%, 89%, 90%, 91%, 92%, 93%, 24%94%, , 25%, 95%, 96%6%, , 97%, 7%, 8%, 9%, 26%, 4%, 27%, 28%, 10%, 11%, 29%, 30%, 31%, 33%, 12%, 13%, 35%, 36%, 37%, 5%, 38%, 6%, 39%, 1%, 2%, 3%, 4%, 7%, 5%, 6%, 1%, 7%, 40%, 41%, 2%, 3%, 8%, 4%, 9%, 5%, 6%, 10%, 7%, 8%, 9%, 42%, 43%, 44%, 8%, 45%, 46%, 48%, 49%, 50%, 51%, 52%, 53%, 9%, 55%, 56%, "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy.stats import median_absolute_deviation\n",
    "from collections import OrderedDict\n",
    "from shutil import copyfile as cp\n",
    "from os import listdir as ld\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import threading\n",
    "import math\n",
    "import os\n",
    "os.system(f'find {Paths.get(Dirs.ROOT)} -name \".DS_Store\" -delete')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pandas.errors import EmptyDataError\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "\n",
    "WINDOW_SIZE = 30000 # ms\n",
    "OVERLAP = 0.5       #1-OVERLAP\n",
    "\n",
    "\n",
    "def li_to_var(li_df):\n",
    "  var_=[]\n",
    "  from_ts = li_df.iloc[0]['ts']\n",
    "  till_ts = li_df.iloc[-1]['ts']\n",
    "  while from_ts < till_ts-WINDOW_SIZE:\n",
    "    sub_df = li_df.query(f'ts >= {from_ts} and ts < {from_ts + WINDOW_SIZE}')\n",
    "    var_.append(sub_df['li'].var())\n",
    "    from_ts += WINDOW_SIZE * OVERLAP\n",
    "  return var_\n",
    "\n",
    "\n",
    "def create_dataset(df_loose, df1, df2):\n",
    "  var1 = li_to_var(df_loose)\n",
    "  if not df1.empty:\n",
    "    var2_1 = li_to_var(df1)\n",
    "  else:\n",
    "    var2_1 = None\n",
    "  if not df2.empty:\n",
    "    var2_2 = li_to_var(df2)\n",
    "  else:\n",
    "    var2_2 = None\n",
    "  df_var1 = pd.DataFrame(var1)\n",
    "  df_var1['class']=1\n",
    "  df_var2_1 = pd.DataFrame(var2_1)\n",
    "  df_var2_2 = pd.DataFrame(var2_2)\n",
    "  if None in [var2_1, var2_2]:\n",
    "    if var2_1 is None:\n",
    "      df_var2 = df_var2_2\n",
    "    else:\n",
    "      df_var2 = df_var2_1\n",
    "  else:\n",
    "    df_var2 = pd.concat([df_var2_1, df_var2_2], ignore_index=True)\n",
    "  df_var2['class']=0\n",
    "  df_var1.columns=df_var2.columns=['var','class']\n",
    "  X = pd.concat([df_var1, df_var2], ignore_index=True)\n",
    "  x = X.drop(['class'], axis = 1)\n",
    "  y = X.drop(['var'], axis = 1)\n",
    "  return x,y\n",
    "\n",
    "\n",
    "def xgboost_looseness(x_train, x_test, y_train, y_test):\n",
    "  d_train = xgb.DMatrix(data=x_train, label=y_train.to_numpy())\n",
    "  d_test = xgb.DMatrix(data=x_test, label=y_test.to_numpy())\n",
    "  \n",
    "  booster = xgb.train(\n",
    "      params=dict(eval_metric='auc', booster='gbtree', verbosity=0, objective='binary:logistic'),\n",
    "      dtrain=d_train,\n",
    "      num_boost_round=10000,  # the number of boosted trees\n",
    "      early_stopping_rounds=25,  # early stop generating trees when eval_metric is not improved\n",
    "      evals=[(d_test, 'test')],  # evaluation set to check early stopping\n",
    "      \n",
    "      verbose_eval=False\n",
    "  )\n",
    "  \n",
    "  y_pred = booster.predict(data=d_test, ntree_limit=booster.best_ntree_limit)\n",
    "  y_pred_class = np.where(y_pred > 0.5, 1, 0) \n",
    "  acc = accuracy_score(y_test, y_pred_class)\n",
    "  print(\"==> acc =\", acc*100)\n",
    "  return booster, y_pred_class\n",
    "\n",
    "\n",
    "def clean_looseness_artifacts(booster, li_dataset, hrv_dataset, verbose=False):\n",
    "  if len(li_dataset) == 0:\n",
    "    return [], 0, len(hrv_dataset) # drop all\n",
    "  if len(hrv_dataset) == 0:\n",
    "    return hrv_dataset, 0, 0 # empty set\n",
    "  drop_count = 0\n",
    "  drop_missing_data_count = 0\n",
    "\n",
    "  from_ts = li_dataset[0][0]\n",
    "  till_ts = li_dataset[-1][0]\n",
    "  froom_ts = from_ts\n",
    "  ts_range = till_ts - froom_ts\n",
    "  prev_pct = 0\n",
    "  while from_ts < till_ts-WINDOW_SIZE:\n",
    "    sub_li_data = select_data(li_dataset, from_ts, from_ts+WINDOW_SIZE, threshold=WINDOW_SIZE/1000, with_timestamp=False)\n",
    "    hrv_count = len(hrv_dataset)\n",
    "    if verbose:\n",
    "      pct = int(100*(from_ts - froom_ts)/ts_range)\n",
    "      if prev_pct != pct:\n",
    "          print(f'{pct}%', end=', ')\n",
    "          prev_pct = pct\n",
    "    if sub_li_data is None:\n",
    "      i0 = bin_find(hrv_dataset, 0, hrv_count-1, from_ts, go_up=False)\n",
    "      i1 = bin_find(hrv_dataset, 0, hrv_count-1, from_ts+WINDOW_SIZE, go_up=True)\n",
    "      hrv_dataset = hrv_dataset[:i0] + hrv_dataset[i1:]\n",
    "      drop_missing_data_count += i1-i0\n",
    "      from_ts += WINDOW_SIZE\n",
    "      continue\n",
    "    var_ = np.var(sub_li_data)\n",
    "    x_ = pd.DataFrame([var_],columns =['var'])\n",
    "    x_ = xgb.DMatrix(x_) \n",
    "    y_pred = booster.predict(data=x_, ntree_limit=booster.best_ntree_limit) #!  \n",
    "    y_pred = np.where(y_pred > 0.5, 1, 0) \n",
    "    \n",
    "    if y_pred == 1:\n",
    "      i0 = bin_find(hrv_dataset, 0, hrv_count-1, from_ts, go_up=False)\n",
    "      i1 = bin_find(hrv_dataset, 0, hrv_count-1, from_ts+WINDOW_SIZE, go_up=True)\n",
    "      hrv_dataset = hrv_dataset[:i0] + hrv_dataset[i1:]\n",
    "      drop_count += i1-i0\n",
    "      from_ts += WINDOW_SIZE\n",
    "      continue\n",
    "    from_ts += WINDOW_SIZE * OVERLAP\n",
    "  if verbose:\n",
    "    print()\n",
    "  return hrv_dataset, drop_count, drop_missing_data_count\n",
    "\n",
    "\n",
    "'''\n",
    "(STEP 1)\n",
    "  Train Looseness models\n",
    "  parent_dir / scenario / participant / data\n",
    "'''\n",
    "# todo Bunyodbek pls fill with model building code here\n",
    "booster = {}\n",
    "participants = os.listdir(Paths.get(Dirs.PROCESSED_FIELD_DATASET))\n",
    "scenarios = ld(Paths.get(Dirs.PROCESSED_TREADMILL_DATASET))\n",
    "scenarios.sort(key=lambda x: int(x[:x.index('.')]))\n",
    "for index, participant in enumerate(participants):\n",
    "  rr_total_count, rr_drop_count = 0, 0\n",
    "  hr_total_count, hr_drop_count = 0, 0\n",
    "  print(f'{index+1}/{len(participants)} participant: {participant}')\n",
    "  \n",
    "  for scenario in scenarios:\n",
    "    try:\n",
    "      if 'loose' in scenario:\n",
    "        df_loose = pd.read_csv(Paths.get(Dirs.PROCESSED_TREADMILL_DATASET, scenario, participant, 'ppg_li.csv'), names=['ts', 'li'], header = None)\n",
    "      elif 'medium' in scenario:\n",
    "        df_medium = pd.read_csv(Paths.get(Dirs.PROCESSED_TREADMILL_DATASET, scenario, participant, 'ppg_li.csv'), names=['ts', 'li'], header = None)\n",
    "      else:\n",
    "        df_tight = pd.read_csv(Paths.get(Dirs.PROCESSED_TREADMILL_DATASET, scenario, participant, 'ppg_li.csv'), names=['ts', 'li'], header = None)\n",
    "    except EmptyDataError:\n",
    "      print(f'participant {participant} skipped on looseness model building')\n",
    "      continue\n",
    "  if df_loose.empty or (df_medium.empty and df_tight.empty):\n",
    "    #print(f'participant {participant} has a missing label for building a looseness model')\n",
    "    pass\n",
    "  else:\n",
    "    x,y = create_dataset(df_loose, df_medium, df_tight)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    booster_, y_pred = xgboost_looseness(x_train, x_test, y_train, y_test)\n",
    "    booster[participant] = booster_\n",
    "\n",
    "\n",
    "'''\n",
    "(STEP 2)\n",
    "  Cleaning Lab-Stress dataset\n",
    "  parent_dir / scenario / participant / data\n",
    "'''\n",
    "print('\\nlab stress')\n",
    "participants = os.listdir(Paths.get(Dirs.PROCESSED_FIELD_DATASET))\n",
    "scenarios = ld(Paths.get(Dirs.PROCESSED_LAB_STRESS_DATASET))\n",
    "scenarios.sort(key=lambda x: int(x[:x.index('.')]))\n",
    "for index, participant in enumerate(participants):\n",
    "  rr_total_count, rr_drop_count = 0, 0\n",
    "  hr_total_count, hr_drop_count = 0, 0\n",
    "  print(f'{index+1}/{len(participants)} participant: {participant}')\n",
    "  \n",
    "  for scenario in scenarios:  \n",
    "    # fix paths\n",
    "    if not os.path.exists(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario)):\n",
    "      os.mkdir(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario))\n",
    "      os.mkdir(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario, participant))\n",
    "    elif not os.path.exists(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario, participant)):\n",
    "      os.mkdir(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario, participant))\n",
    "\n",
    "    li_path = Paths.get(Dirs.PROCESSED_LAB_STRESS_DATASET, scenario, participant, 'ppg_li.csv')\n",
    "    if os.path.exists(li_path):\n",
    "      li_dataset = load_dataset(li_path, cast_type=int)\n",
    "      rr_path = Paths.get(Dirs.PROCESSED_LAB_STRESS_DATASET, scenario, participant, 'ppg_rr.csv')\n",
    "      booster_ = booster[participant] if participant in booster else booster[list(booster.keys())[0]]\n",
    "      if os.path.exists(rr_path):\n",
    "        rr_dataset = load_dataset(rr_path, cast_type=int)\n",
    "        rr_count = len(rr_dataset)\n",
    "        rr_total_count += rr_count\n",
    "        rr_dataset, drop_count, drop_missing_data_count = clean_looseness_artifacts(booster_, li_dataset=li_dataset, hrv_dataset=rr_dataset)\n",
    "        rr_drop_count += drop_count\n",
    "        with open(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario, participant, 'ppg_rr.csv'), 'w+') as w:\n",
    "          for ts, rr in rr_dataset:\n",
    "            w.write(f'{ts},{rr}\\n')\n",
    "      hr_path = Paths.get(Dirs.PROCESSED_LAB_STRESS_DATASET, scenario, participant, 'ppg_hr.csv')\n",
    "      if os.path.exists(hr_path):\n",
    "        hr_dataset = load_dataset(hr_path, cast_type=int)\n",
    "        hr_count = len(hr_dataset)\n",
    "        hr_total_count += hr_count\n",
    "        hr_df, drop_count, drop_missing_data_count = clean_looseness_artifacts(booster_, li_dataset=li_dataset, hrv_dataset=hr_dataset)\n",
    "        hr_drop_count += drop_count\n",
    "        with open(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario, participant, 'ppg_hr.csv'), 'w+') as w:\n",
    "          for ts, hr in hr_dataset:\n",
    "            w.write(f'{ts},{hr}\\n')\n",
    "  print(f'(rr_drop_count/rr_total, hr_drop_count/hr_total) = ({rr_drop_count}/{rr_total_count}, {hr_drop_count}/{hr_total_count})')\n",
    "\n",
    "\n",
    "'''\n",
    "(STEP 3)\n",
    "  Cleaning field stress dataset\n",
    "  parent_dir / participant / data\n",
    "'''\n",
    "print('field stress')\n",
    "def filter_field_data(idx, participants):\n",
    "  rr_total_count, rr_drop_count = 0, 0\n",
    "  hr_total_count, hr_drop_count = 0, 0\n",
    "  participant = participants[idx]\n",
    "  print(f'{idx+1}/{len(participants)} participant: {participant}')\n",
    "  \n",
    "  # fix paths\n",
    "  if not os.path.exists(Paths.get(Dirs.FILTERED_FIELD_DATASET, participant)):\n",
    "    os.mkdir(Paths.get(Dirs.FILTERED_FIELD_DATASET, participant))\n",
    "  \n",
    "  li_path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, 'ppg_li.csv')\n",
    "  if os.path.exists(li_path):\n",
    "    li_dataset = load_dataset(li_path, cast_type=int)\n",
    "    rr_path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, 'ppg_rr.csv')\n",
    "    booster_ = booster[participant] if participant in booster else booster[list(booster.keys())[0]]\n",
    "    if os.path.exists(rr_path):\n",
    "      rr_dataset = load_dataset(rr_path, cast_type=int)\n",
    "      rr_count = len(rr_dataset)\n",
    "      rr_total_count += rr_count\n",
    "      rr_dataset, drop_count, drop_missing_data_count = clean_looseness_artifacts(booster_, li_dataset=li_dataset, hrv_dataset=rr_dataset, verbose=True)\n",
    "      rr_drop_count += drop_count\n",
    "      with open(Paths.get(Dirs.FILTERED_FIELD_DATASET, participant, 'ppg_rr.csv'), 'w+') as w:\n",
    "        for ts, rr in rr_dataset:\n",
    "          w.write(f'{ts},{rr}\\n')\n",
    "    hr_path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, 'ppg_hr.csv')\n",
    "    if os.path.exists(hr_path):\n",
    "      hr_dataset = load_dataset(hr_path, cast_type=int)\n",
    "      hr_count = len(hr_dataset)\n",
    "      hr_total_count += hr_count\n",
    "      hr_df, drop_count, drop_missing_data_count = clean_looseness_artifacts(booster_, li_dataset=li_dataset, hrv_dataset=hr_dataset, verbose=True)\n",
    "      hr_drop_count += drop_count\n",
    "      with open(Paths.get(Dirs.FILTERED_FIELD_DATASET, participant, 'ppg_hr.csv'), 'w+') as w:\n",
    "        for ts, rr in hr_dataset:\n",
    "          w.write(f'{ts},{hr}\\n')\n",
    "  print(f'(rr_drop_count/rr_total, hr_drop_count/hr_total) = ({rr_drop_count}/{rr_total_count}, {hr_drop_count}/{hr_total_count})')\n",
    "\n",
    "threads = []\n",
    "participants = os.listdir(Paths.get(Dirs.PROCESSED_FIELD_DATASET))\n",
    "for idx, participant in enumerate(participants):\n",
    "  t = threading.Thread(target=filter_field_data, args=(idx, participants))\n",
    "  t.start()\n",
    "  threads += [t]\n",
    "for t in threads:\n",
    "  t.join()\n",
    "print('done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPomWz3FDj5w"
   },
   "source": [
    "## 2.2 Normalize the HR data\n",
    "    Z-score normalization\n",
    "\n",
    "    Input: Preprocessed HR data\n",
    "    Output: HRV with person-specific attributes removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KfwL-e5BcSFo",
    "outputId": "d37ccdfb-71b5-4e06-dd65-723998a2c64a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab\n",
      "\t 10. rest\n",
      "\t 11. cold instructions\n",
      "\t 6. arithmetic instructions\n",
      "\t 4. speech stressor\n",
      "\t 12. cold stressor\n",
      "\t 1. baseline rest\n",
      "\t 7. arithmetic stressor p1\n",
      "\t 9. arithmetic stressor p2\n",
      "\t 5. rest\n",
      "\t 3. speech preparation\n",
      "\t 2. speech instructions\n",
      "\t 8. arithmetic rest\n",
      "\t 13. recovery\n",
      "field\n",
      "\t 1/35 jinsoonshim@naver.com\n",
      "\t 2/35 kpm1323@gmail.com\n",
      "\t 3/35 minrudcho01@gmail.com\n",
      "\t 4/35 fpdlsqhdn962@inha.edu\n",
      "\t 5/35 csissis1997@gmail.com\n",
      "\t 6/35 chaewoni65@gmail.com\n",
      "\t 7/35 eowjdtjdwls@gmail.com\n",
      "\t 8/35 rlarkgus11170@inha.edu\n",
      "\t 9/35 12201686@inha.edu\n",
      "\t 10/35 km55181499@gmail.com\n",
      "\t 11/35 bw04029@gmail.com\n",
      "\t 12/35 vusgowlwk17@gmail.com\n",
      "\t 13/35 rbfl675@inha.edu\n",
      "\t 14/35 minjae20000207@gmail.com\n",
      "\t 15/35 jwshoho4@gmail.com\n",
      "\t 16/35 duecjf011521212106@gmail.com\n",
      "\t 17/35 12212982so@inha.edu\n",
      "\t 18/35 nigaram8@gmail.com\n",
      "\t 19/35 12181157@inha.edu\n",
      "\t 20/35 as7177as3912@gmail.com\n",
      "\t 21/35 nnozilaxonim@gmail.com\n",
      "\t 22/35 powerampshere@gmail.com\n",
      "\t 23/35 wjdwogus0604@gmail.com\n",
      "\t 24/35 gurwns7772@gmail.com\n",
      "\t 25/35 zkapdh123@gmail.com\n",
      "\t 26/35 ysl@inha.edu\n",
      "\t 27/35 jyypaul@gmail.com\n",
      "\t 28/35 wonjs0725@gmail.com\n",
      "\t 29/35 hhcc05@inha.edu\n",
      "\t 30/35 unicpn21@gmail.com\n",
      "\t 31/35 gh011127@gmail.com\n",
      "\t 32/35 bagle1029@gmail.com\n",
      "\t 33/35 memm1439@gmail.com\n",
      "\t 34/35 12170603@inha.edu\n",
      "\t 35/35 david000914@gmail.com\n"
     ]
    }
   ],
   "source": [
    "from os import listdir as ld\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import os\n",
    "os.system(f'find {Paths.get(Dirs.ROOT)} -name \".DS_Store\" -delete')\n",
    "\n",
    "lab_src_key = Dirs.PROCESSED_LAB_STRESS_DATASET # Dirs.FILTERED_LAB_STRESS_DATASET\n",
    "field_src_key = Dirs.PROCESSED_FIELD_DATASET # Dirs.FILTERED_FIELD_DATASET\n",
    "\n",
    "# normalize the lab data\n",
    "print('lab')\n",
    "scenarios = ld(Paths.get(lab_src_key))\n",
    "for scenario in scenarios:\n",
    "  print(f'\\t {scenario}')\n",
    "  # fix paths\n",
    "  if not os.path.exists(Paths.get(Dirs.NORMALIZED_LAB_STRESS_DATASET, scenario)):\n",
    "    os.mkdir(Paths.get(Dirs.NORMALIZED_LAB_STRESS_DATASET, scenario))\n",
    "\n",
    "  for participant in ld(Paths.get(lab_src_key, scenario)):\n",
    "    # fix paths\n",
    "    if not os.path.exists(Paths.get(Dirs.NORMALIZED_LAB_STRESS_DATASET, scenario, participant)):\n",
    "      os.mkdir(Paths.get(Dirs.NORMALIZED_LAB_STRESS_DATASET, scenario, participant))\n",
    "\n",
    "    # normalize rr data\n",
    "    rr_path = Paths.get(lab_src_key, scenario, participant, 'ecg_rr.csv')\n",
    "    if os.path.exists(rr_path):\n",
    "      rr_df = pd.read_csv(rr_path, names=['ts', 'rr'], header=None)\n",
    "      if rr_df.empty:\n",
    "        continue\n",
    "      norm_rr = stats.zscore(rr_df['rr'])\n",
    "      norm_rr_df = pd.DataFrame({'ts': rr_df['ts'], 'rr': norm_rr})\n",
    "      norm_rr_df.to_csv(Paths.get(Dirs.NORMALIZED_LAB_STRESS_DATASET, scenario, participant, 'ppg_rr.csv'), index=False, header=False)\n",
    "    \n",
    "    # normalize hr data\n",
    "    hr_path = Paths.get(lab_src_key, scenario, participant, 'ecg_hr.csv')\n",
    "    if os.path.exists(hr_path):\n",
    "      hr_df = pd.read_csv(hr_path, names=['ts', 'hr'], header=None)\n",
    "      if hr_df.empty:\n",
    "        continue\n",
    "      norm_hr = stats.zscore(hr_df['hr'])\n",
    "      norm_hr_df = pd.DataFrame({'ts': hr_df['ts'], 'hr': norm_hr})\n",
    "      norm_hr_df.to_csv(Paths.get(Dirs.NORMALIZED_LAB_STRESS_DATASET, scenario, participant, 'ppg_hr.csv'), index=False, header=False)\n",
    "\n",
    "# normalize the field data\n",
    "print('field')\n",
    "participants = ld(Paths.get(field_src_key))\n",
    "for idx, participant in enumerate(participants):\n",
    "  print(f'\\t {idx+1}/{len(participants)} {participant}')\n",
    "  # fix paths\n",
    "  if not os.path.exists(Paths.get(Dirs.NORMALIZED_FIELD_DATASET, participant)):\n",
    "    os.mkdir(Paths.get(Dirs.NORMALIZED_FIELD_DATASET, participant))\n",
    "\n",
    "  # normalize rr data\n",
    "  rr_path = Paths.get(field_src_key, participant, 'ppg_rr.csv')\n",
    "  if os.path.exists(rr_path):\n",
    "    rr_df = pd.read_csv(rr_path, names=['ts', 'rr'], header=None)\n",
    "    if rr_df.empty:\n",
    "      continue\n",
    "    norm_rr = stats.zscore(rr_df['rr'])\n",
    "    norm_rr_df = pd.DataFrame({'ts': rr_df['ts'], 'rr': norm_rr})\n",
    "    norm_rr_df.to_csv(Paths.get(Dirs.NORMALIZED_FIELD_DATASET, participant, 'ppg_rr.csv'), index=False, header=False)\n",
    "  \n",
    "  # normalize hr data\n",
    "  hr_path = Paths.get(field_src_key, participant, 'ppg_hr.csv')\n",
    "  if os.path.exists(hr_path):\n",
    "    hr_df = pd.read_csv(hr_path, names=['ts', 'hr'], header=None)\n",
    "    if hr_df.empty:\n",
    "      continue\n",
    "    norm_hr = stats.zscore(hr_df['hr'])\n",
    "    norm_hr_df = pd.DataFrame({'ts': hr_df['ts'], 'hr': norm_hr})\n",
    "    norm_hr_df.to_csv(Paths.get(Dirs.NORMALIZED_FIELD_DATASET, participant, 'ppg_hr.csv'), index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEpSQCerZaJZ"
   },
   "source": [
    "## 2.3. Feature computation\n",
    "    > Breaking down the data into windows\n",
    "    > Time-domain features\n",
    "\n",
    "    Input: Preprocessed, normalized HR & rrInterval data\n",
    "    Output: Time-domain features from HR and rInterval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "xxVF1hmstytc",
    "outputId": "820ec7ce-6bcc-497a-e4fc-05d6bf3d0f02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1/35 jinsoonshim@naver.com2/35 kpm1323@gmail.com \n",
      "\n",
      "3/35 minrudcho01@gmail.com  4/35 fpdlsqhdn962@inha.edu\n",
      "5/35 csissis1997@gmail.com  \n",
      "6/35 chaewoni65@gmail.com \n",
      "7/35 eowjdtjdwls@gmail.com \n",
      "\n",
      "8/35 rlarkgus11170@inha.edu \n",
      "10/35 km55181499@gmail.com 9/35 12201686@inha.edu \n",
      "11/35 bw04029@gmail.com \n",
      "12/35 vusgowlwk17@gmail.com \n",
      "13/35 rbfl675@inha.edu \n",
      "\n",
      "14/35 minjae20000207@gmail.com 15/35 jwshoho4@gmail.com \n",
      "16/35 duecjf011521212106@gmail.com \n",
      "\n",
      "17/35 12212982so@inha.edu18/35 nigaram8@gmail.com  \n",
      "19/35 12181157@inha.edu \n",
      "20/35 as7177as3912@gmail.com\n",
      " 21/35 nnozilaxonim@gmail.com\n",
      " 22/35 powerampshere@gmail.com \n",
      "23/35 wjdwogus0604@gmail.com\n",
      " 24/35 gurwns7772@gmail.com \n",
      "25/35 zkapdh123@gmail.com \n",
      "26/35 ysl@inha.edu \n",
      "27/35 jyypaul@gmail.com \n",
      "\n",
      "28/35 wonjs0725@gmail.com \n",
      "30/35 unicpn21@gmail.com \n",
      "29/35 hhcc05@inha.edu \n",
      "\n",
      "31/35 gh011127@gmail.com32/35 bagle1029@gmail.com 33/35 memm1439@gmail.com  \n",
      "34/35 12170603@inha.edu \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 david000914@gmail.com 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 11, , 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 11, , 11, , 1, 1, 4, 2, 1, 1, 2, 5, 2, 6, 7, 2, 8, 2, 2, 2, 93, , 2, 2, 2, 3, 10, 2, 11, 3, 2, 2, 2, 2, 2, 12, 2, 13, 2, 2, 2, 4, 2, 14, 4, 2, 15, 2, 2, 2, 2, 3, 2, 2, 16, 2, 3, 2, 17, 2, 2, 4, 18, 3, 3, 5, 3, 19, 2, 3, 5, 20, 21, 3, 3, 22, 3, 5, 23, 33, , 3, 3, 6, 3, 4, 24, 3, 4, 25, 6, 26, 3, 3, 3, 3, 27, 2, 3, 3, 3, 4, 4, 28, 4, 3, 6, 3, 7, 29, 3, 43, , 30, 3, 31, 4, 7, 3, 4, 5, 32, 3, 5, 33, 4, 8, 34, 7, 4, 4, 4, 4, 35, 36, 3, 8, 4, 4, 37, 5, 5, 5, 438, , 4, 9, 4, 44, , 39, 4, 4, 4, 840, , 5, 6, 5, 41, 6, 4, 42, 9, 4, 10, 543, , 4, 4, 44, 5, 45, 5, 4, 9, 5, 46, 5, 6, 5, 6, 47, 5, 10, 48, 11, 6, 4, 5, 7, 49, 3, 6, 7, 50, 5, 65, , 5, 51, 5, 6, 10, 52, 5, 5, 5, 53, 5, 11, 12, 5, 54, 5, 55, 6, 7, 7, 6, 56, 5, 8, 6, 6, 57, 11, 6, 8, 4, 5, 13, 12, 58, 6, 5, 7, 7, 59, 76, , 7, 6, 60, 4, 61, 6, 6, 62, 6, 12, 14, 6, 636, , 13, 5, 6, 64, 9, 6, 8, 8, 7, 65, 6, 6, 9, 7, 7, 66, 7, 8, 7, 15, 8, 67, 7, 13, 68, 6, 14, 8, 69, 7, 7, 8, 6, 70, 1671, , 6, 5, 10, 72, 7, 7, 7, 7, 9, 73, 14, 9, 10, 915, , 74, 78, , 7, 7, 8, 75, 8, 17, 9, 8, 76, 8, 8, 77, 9, 5, 6, 78, 8, 16, 715, , 7, 9, 79, 11, 18, 8, 80, 81, 7, 107, , 8210, , 10, 11, 6, 8, 17, 8, 8, 9, 8, 19, 16, 7, 10, 9, 8, 9, 8, 9, 10, 9, 9, 9, 12, 8, 20, 18, 10, 11, 17, 9, 11, 8, 12, 11, 8, 7, 21, 11, 19, 8, 10, 10, 10, 9, 98, , 7, 10, 9, 9, 6, 13, 18, 11, 10, 22, 12, 99, , 10, 10, 8, 9, 13, 20, 11, 12, 10, 12, 23, 19, 9, 12, 14, 11, 11, 2111, 11, , 12, 24, 13, 9, 9, 10, 11, 7, 10, 8, 20, 10, 149, , 11, 11, 13, 8, 10, 25, 12, 10, 10, 22, 11, 13, 15, 13, 10, 9, 10, 21, 14, 26, 12, 12, 13, 12, 23, 12, 15, 1012, , 11, 14, 11, 12, 27, 22, 8, 11, 9, 13, 24, 16, 11, 14, 10, 12, 12, 10, 11, 14, 15, 28, 11, 10, 13, 11, 2314, , 1325, , 16, 13, 11, 13, 29, 11, 15, 13, 12, 17, 14, 15, 26, 12, 24, 16, 9, 301312, , , 10, 12, 9, 13, 15, 14, 13, 12, 15, 17, 12, 31, 27, 11, 11, 14, 12, 25, 14, 1118, , 14, 12, 14, 12, 17, 13, 32, 16, 15, 28, 16, 13, 13, 26, 10, 18, 14, 13, 33, 15, 16, 14, 29, 15, 19, 11, 13, 16, 18, 14, 15, 12, 34, 13, 13, 15, 15, 27, 13, 12, 17, 16, 12, 30, 14, 17, 19, 35, 13, 14, 17, 16, 15, 10, 20, 28, 14, 15, 1114, , 31, 19, 16, 36, 16, 16, 14, 14, 15, 13, 14, 16, 17, 20, 32, 29, 14, 37, 18, 12, 15, 17, 21, 20, 13, 17, 18, 14, 13, 16, 16, 18, 33, 38, 15, 30, 17, 12, 15, 21, 15, 1539, , 17, 34, 17, 17, 18, 22, 15, 16, 15, 18, 21, 15, 31, 16, 19, 13, 18, 40, 17, 35, 15, 14, 17, 11, 22, 14, 19, 14, 41, 32, 13, 23, 19, 18, 16, 16, 19, 22, 18, 36, 16, 19, 18, 18, 16, 17, 42, 20, 16, 16, 16, 18, 33, 17, 37, 23, 18, 14, 16, 43, 2024, , 19, 20, 23, 20, 12, 34, 19, 15, 38, 20, 15, 44, 19, 15, 17, 17, 17, 19, 21, 19, 18, 17, 24, 39, 45, 17, 17, 18, 2535, , 19, 21, 24, 17, 2119, , 17, 21, 46, 2140, , 20, 20, 14, 15, 36, 16, 13, 22, 25, 16, 18, 20, 47, 18, 20, 26, 20, 19, 41, 18, 18, 25, 16, 22, 19, 37, 22, 20, 18, 22, 48, 18, 20, 18, 42, 21, 26, 21, 27, 23, 49, 38, 18, 17, 26, 21, 14, 19, 43, 16, 20, 21, 19, 23, 21, 19, 17, 2350, , 23, 19, 20, 21, 39, 22, 17, 27, 19, 44, 22, 28, 51, 21, 24, 27, 15, 22, 19, 19, 22, 40, 19, 18, 45, 20, 2452, , 24, 21, 15, 24, 22, 28, 22, 22, 29, 21, 46, 53, 20, 23, 28, 18, 41, 17, 20, 20, 25, 22, 20, 23, 54, 18, 23, 47, 20, 23, 25, 25, 25, 42, 29, 20, 21, 30, 22, 23, 55, 29, 48, 16, 23, 20, 23, 2419, , 22, 26, 23, 43, 21, 24, 56, 18, 16, 21, 19, 49, 24, 21, 26, 31, 26, 21, 30, 30, 19, 57, 24, 44, 26, 24, 23, 21, 50, 24, 24, 25, 22, 58, 24, 25, 23, 21, 32, 45, 27, 51, 21, 22, 27, 20, 22, 31, 25, 59, 31, 27, 22, 27, 20, 20, 25, 25, 52, 19, 46, 60, 25, 24, 22, 25, 26, 17, 23, 26, 25, 32, 32, 28, 61, 53, 24, 22, 33, 26, 47, 22, 28, 17, 23, 28, 28, 26, 62, 54, 22, 23, 26, 21, 21, 33, 48, 21, 26, 2726, , 25, 27, 23, 33, 63, 24, 29, 26, 55, 34, 27, 29, 20, 25, 29, 29, 23, 64, 49, 18, 24, 34, 56, 27, 2723, , 22, 23, 65, 28, 28, 22, 23, 35, 30, 50, 27, 34, 26, 57, 24, 25, 18, 22, 27, 27, 66, 30, 30, 30, 35, 28, 58, 51, 25, 67, 24, 19, 28, 21, 26, 36, 29, 28, 24, 29, 31, 23, 28, 59, 31, 68, 3152, , 23, 24, 36, 27, 31, 24, 35, 28, 23, 28, 19, 26, 25, 60, 24, 69, 26, 37, 30, 29, 53, 29, 27, 20, 30, 22, 32, 25, 37, 29, 24, 32, 29, 32, 70, 61, 36, 32, 25, 24, 25, 54, 28, 29, 24, 38, 29, 71, 62, 25, 27, 31, 20, 33, 3830, , 31, 27, 28, 26, 33, 37, 21, 33, 55, 26, 72, 25, 63, 30, 30, 33, 39, 25, 30, 26, 23, 30, 73, 29, 56, 64, 39, 30, 28, 25, 26, 34, 25, 32, 31, 38, 26, 34, 34, 21, 74, 29, 32, 28, 65, 34, 22, 27, 57, 31, 40, 26, 75, 31, 40, 31, 26, 27, 66, 29, 39, 30, 31, 35, 58, 33, 35, 26, 76, 31, 27, 24, 26, 32, 27, 4135, , 35, 30, 22, 27, 67, 29, 41, 23, 77, 27, 28, 59, 32, 32, 40, 3233, , 27, 30, 32, 68, 36, 36, 78, 31, 34, 32, 60, 27, 28, 36, 33, 4227, , 6942, , 79, 31, 41, 28, 36, 28, 25, 29, 28, 61, 28, 33, 34, 80, 3770, , 31, 30, 37, 28, 33, 33, 33, 35, 33, 32, 37, 43, 23, 34, 81, 62, 43, 28, 42, 71, 32, 28, 37, 24, 82, 29, 29, 38, 29, 38, 72, 6344, , 32, 34, 36, 29, 34, 38, 29, 3430, , 31, 83, 4433, , 43, 26, 35, 35, 34, 34, 73, 64, 38, 29, 84, 33, 29, 45, 39, 39, 74, 29, 30, 37, 33, 30, 85, 35, 39, 65, 44, 30, 35, 45, 34, 30, 30, 31, 86, 35, 39, 24, 75, 36, 35, 46, 36, 27, 32, 66, 40, 40, 35, 30, 87, 25, 30, 45, 34, 38, 76, 34, 46, 36, 40, 31, 67, 36, 88, 31, 47, 40, 31, 35, 31, 77, 37, 32, 89, 41, 33, 41, 36, 68, 46, 31, 36, 36, 28, 47, 37, 35, 31, 3978, , 25, 90, 37, 48, 41, 30, 41, 32, 69, 35, 37, 32, 32, 79, 91, 42, 47, 38, 42, 34, 37, 36, 33, 70, 32, 32, 92, 49, 80, 26, 40, 31, 37, 42, 48, 42, 37, 36, 26, 32, 38, 93, 38, 31, 38, 48, 3333, , 71, 81, 29, 43, 36, 39, 33, 94, 35, 50, 38, 34, 82, 43, 72, 33, 43, 41, 43, 3795, , 3349, , 49, 44, 83, 33, 38, 27, 37, 38, 73, 3439, , 51, 96, 39, 39, 40, 3237, , 30, 34, 39, 36, 34, 4484, , 44, 97, 35, 42, 44, 27, 74, 50, 34, 50, 38, 45, 32, 85, 34, 98, 39, 39, 41, 34, 28, 38, 40, 75, 40, 38, 99, 45, 33, 3731, , 86, 40, 45, 40, 51, 35, 5135, , 43, 45, 36, 100, 76, 46, 35, 87, 35, 39, 35, 40, 40, 101, 41, 41, 39, 52, 46, 35, 77, 39, 29, 41, 52, 46, 88, 42, 38, 46, 34, 44, 102, 28, 36, 47, 41, 37, 32, 78, 36, 89, 103, 33, 42, 42, 5341, , 53, 36, 40, 47, 40, 47, 42, 47, 36, 40, 41, 36, 30, 39, 45, 79, 90, 104, 48, 29, 37, 35, 36, 43, 38, 54, 105, 33, 91, 43, 43, 80, 54, 42, 48, 48, 42, 106, 4148, , 37, 37, 41, 92, 43, 49, 40, 4681, , 42, 41, 107, 55, 37, 38, 30, 93, 44, 5544, , 37, 36, 39, 49, 108, 82, 43, 43, 34, 44, 49, 50, 37, 94, 49, 34, 109, 47, 38, 44, 31, 42, 56, 83, 38, 45, 56, 42, 45, 95, 42, 39, 110, 41, 31, 50, 43, 38, 40, 44, 50, 38, 45, 84, 44, 51, 111, 37, 96, 35, 50, 57, 38, 57, 46, 46, 48, 112, 39, 97, 32, 85, 43, 51, 45, 35, 40, 43, 32, 43, 39, 52, 51, 113, 98, 44, 45, 58, 42, 86, 45, 41, 3836, , 58, 46, 114, 47, 47, 39, 51, 39, 99, 39, 52, 44, 87, 49, 40, 115, 53, 41, 44, 59, 52, 100, 33, 46, 33, 46, 45, 59, 116, 36, 44, 40, 88, 48, 48, 47, 46, 37, 39, 101, 52, 53, 117, 43, 40, 40, 42, 45, 40, 54, 60, 89, 60, 102, 42, 118, 47, 45, 5350, , 47, 34, 41, 49, 49, 48, 119, 45, 90, 47, 103, 46, 54, 34, 61, 3853, , 40, 44, 55, 37, 41, 61, 41, 120, 41, 48, 104, 91, 51, 46, 46, 4154, , 43, 48, 50, 50, 43, 121, 35, 49, 105, 55, 62, 48, 62, 56, 122, 42, 47, 54, 35, 39, 45, 46, 42, 92, 41, 49, 52, 42106, , 38, 123, 51, 51, 49, 47, 55, 63, 44, 47, 124, 6336, , 42, 107, 49, 57, 5056, , 44, 42, 47, 55, 125, 50, 53, 43, 108, 48, 36, 52, 43, 42, 52, 46, 93, 64, 39, 126, 50, 64, 48, 56, 58, 109, 50, 43, 37, 45, 127, 43, 48, 57, 48, 45, 56, 51, 53, 110, 65, 53, 54, 4044, , 43, 128, 51, 65, 37, 47, 43, 51, 57, 59, 49, 111, 44, 49, 40, 129, 94, 58, 38, 49, 44, 66, 130, 51, 46, 54, 112, 54, 57, 66, 44, 49, 46, 55, 52, 44, 52, 60, 131, 113, 50, 58, 44, 52, 45, 67, 41, 59, 48, 132, 67, 55, 45, 50, 55, 114, 50, 45, 47, 39, 58, 53, 56, 50, 133, 47, 95, 61, 41, 45, 115, 52, 68, 45, 38, 59, 53, 134, 68, 60, 45, 56, 56, 5151, , 53, 116, 46, 46, 54, 135, 59, 49, 46, 57, 62, 42, 40, 69, 51, 117, 69, 46, 136, 51, 48, 60, 57, 5348, , 57, 39, 118, 52, 46, 137, 96, 52, 54, 55, 61, 60, 47, 46, 63, 58, 50, 119, 70, 138, 54, 47, 70, 52, 47, 58, 61, 42, 52, 43, 47, 58, 41, 139120, , 53, 49, 62, 56, 54, 49, 53, 40, 64, 47, 71, 140, 59, 61, 51, 55, 121, 48, 55, 97, 59, 62, 53, 141, 48, 47, 71, 59, 48, 122, 63, 54, 57, 53, 55, 65, 48, 72, 142, 42, 44, 41, 60, 50, 48, 123, 50, 62, 56, 60, 54, 143, 72, 63, 43, 56, 60, 54, 52, 124, 73, 58, 144, 55, 49, 66, 98, 64, 48, 49, 56, 61, 73, 145, 125, 61, 49, 42, 54, 64, 5763, , 49, 99, 45, 49, 74, 43, 61, 146, 51, 126, 55, 55, 67, 51, 56, 57, 65, 50, 74, 53, 59, 147, 62, 62, 127, 65, 75, 58, 57, 64, 55, 62, 148, 100, 49, 44, 50, 50, 50, 128, 50, 57, 7556, , 68, 149, 51, 46, 44, 52, 66, 56, 58, 63, 52, 76, 66, 129, 60, 150, 58, 5963, , 54, 65, 43, 5663, , 76, 50, 101, 151, 45, 69, 58, 130, 51, 51, 67, 77, 57, 52, 67, 51, 53, 152, 61, 51, 131, 47, 64, 45, 59, 60, 57, 77, 53, 64, 59, 66, 153, 57, 44, 70, 64, 59, 51, 78, 132, 68, 55, 154, 68, 46, 52, 102, 52, 53, 78, 133, 58, 65, 155, 54, 65, 61, 62, 52, 67, 54, 52, 79, 48, 65, 60, 60, 71, 134, 60, 58, 69, 156, 45, 58, 69, 4652, , 56, 54, 53, 53, 66, 135, 47, 79, 157, 66, 80, 63, 62, 103, 66, 6853, , 72, 55, 59, 70, 70, 61, 136, 55, 15853, , 61, 61, 53, 59, 67, 49, 81, 80, 59, 55, 54, 54, 47, 67, 64, 137, 5767, , 73, 48, 159, 71, 71, 69, 54, 62, 63, 60, 46, 104, 138, 68, 82, 56, 81, 62, 54160, , 56, 62, 6068, , 68, 65, 56, 60, 54, 55, 55, 72, 139, 72, 70, 50, 161, 49, 55, 83, 63, 74105, , 69, 48, 82, 58, 64, 140, 69, 55, 66, 69, 63, 57, 61, 73, 61, 73, 57, 55, 57, 71, 141, 106, 61, 47, 5656, , 63, 7064, , 83, 84, 75, 50, 142, 56, 67, 51, 70, 74, 49, 58, 56, 74, 59, 64, 65, 72, 58, 107, 62, 84, 143, 71, 62, 65, 64, 48, 57, 85, 62, 57, 56, 76, 58, 70, 68, 75, 57, 144, 71, 75, 73, 85, 65, 60, 66, 57, 72, 51, 65, 86, 6350, , 66, 52, 77, 59, 14559, , 108, 49, 63, 57, 76, 58, 58, 69, 59, 76, 63, 86, 72, 58, 74, 146, 73, 87, 67, 61, 78, 64, 66, 67, 71, 60, 6077, , 58, 8750, , 147, 70, 77, 58, 51, 52, 66, 59, 59, 64, 75, 73, 53, 109, 88, 59, 74, 64, 60, 68, 148, 62, 72, 79, 68, 88, 78, 65, 78, 61, 71, 67, 51, 89, 67, 76, 59, 53, 149, 60, 75, 74, 61, 60, 59, 52, 60, 65, 110, 73, 69, 80, 89, 54, 65, 150, 69, 79, 61, 90, 72, 62, 68, 63, 77, 66, 68, 79, 76, 151, 52, 75, 54, 60, 61, 62, 74, 90, 61, 81, 66, 70, 111, 70, 91, 80, 61, 152, 53, 73, 66, 60, 69, 6378, , 77, 80, 55, 91, 62153, , 64, 69, 76, 75, 53, 62, 55, 82, 63, 112, 67, 71, 67, 92, 81, 61, 71, 154, 74, 7970, , 62, 78, 64, 92, 54, 67, 62, 76, 77, 81, 155, 70, 56, 113, 83, 63, 63, 93, 54, 61, 56, 82, 68, 72, 68, 65, 62, 156, 72, 64, 80, 79, 75, 93, 71, 77, 65, 78, 114, 82, 157, 94, 84, 71, 68, 63, 83, 64, 57, 64, 55, 69, 63, 73, 69, 57, 55, 63, 81, 80, 158, 94, 76, 72, 62, 73, 115, 79, 83, 66, 95, 85, 78, 65, 84, 159, 6574, , 66, 70, 72, 116, 65, 58, 81, 58, 95, 82, 56, 64, 70, 160, 73, 69, 64, 56, 64, 80, 77, 96, 79, 63, 85, 74, 117, 86, 161, 84, 67, 75, 82, 73, 66, 66, 83, 96, 59, 74, 59, 11866, , 71, 65, 57, 97, 70, 162, 80, 81, 67, 86, 71, 78, 87, 64, 57, 75, 65, 65, 85, 163, 119, 76, 83, 84, 97, 67, 98, 75, 74, 81, 60, 72, 68, 87, 82, 66, 164, 58, 68, 120, 67, 79, 71, 88, 67, 60, 65, 86, 72, 76, 84, 77, 165, 85, 98, 58, 99, 121, 66, 82, 76, 88, 68, 73, 66, 61, 67, 83, 166, 80, 89, 59, 75, 69, 122, 85, 72, 66, 87, 78, 73, 69, 86, 68, 77, 100, 99, 167, 68, 83, 89, 67, 61, 74, 69, 59, 123, 90, 62, 84, 81, 168, 77, 76, 60, 67, 68, 86, 88, 79, 87, 101, 100, 70, 74, 84, 69, 124, 169, 90, 69, 78, 67, 73, 75, 68, 91, 70, 70, 85, 78, 170, 63, 125, 82, 89, 87, 60, 61, 88, 8062, , 102, 85, 101, 68, 77, 70, 91, 75, 70, 171, 69, 76, 79, 92, 126, 69, 74, 86, 71, 71, 71, 68, 79, 88, 64, 172, 102, 90, 10389, , 86, 81, 62, 83, 127, 61, 78, 92, 71, 93, 173, 77, 87, 63, 69, 71, 76, 72, 8072, , 89, 72, 103, 90, 104, 91, 87, 7069, , 70, 65, 174, 75, 82, 84, 63, 80, 93, 62, 94, 72, 78, 88, 79, 175, 104, 90, 77, 105, 88, 73, 92, 73, 70, 81, 85, 83, 9176, , 64, 66, 72, 73, 176, 95, 94, 64, 81, 71, 79, 105, 70, 73, 63, 7189, , 80, 91, 106, 177, 89, 93, 74, 78, 74, 86, 92, 84, 82, 71, 96, 65, 77, 67, 74, 106, 178, 95, 82, 73, 80, 90, 65, 74, 92, 64, 81, 94107, , 90, 72, 179, 72, 75, 75, 87, 71, 93, 85, 107, 79, 78, 96, 66, 83, 68, 72, 83, 81, 180, 9774, , 75, 93, 91, 108, 95, 91, 75, 82, 66, 65, 181, 88, 94, 108, 76, 76, 86, 72, 73, 97, 73, 84, 79, 82, 182, 94, 109, 6992, , 96, 92, 80, 73, 6784, , 76, 98, 83, 76, 109, 66, 67, 95, 89, 75, 183, 988777, , , 77, 73, 110, 95, 85, 80, 83, 93, 97, 93, 184, 110, 74, 74, 74, 84, 68, 77, 81, 90, 96, 99, 85, 67, 68, 77, 99, 185, 88, 76, 78, 111, 96, 98, 94, 78, 84, 74, 111, 81, 94, 86, 186, 70, 85, 91, 78, 75, 75, 69, 82, 100, 68, 89, 112, 187, 86, 69, 75100, , 97, 97, 99, 95, 79, 112, 78, 85, 95, 77, 82, 188, 75, 92, 87, 79, 71, 79, 101, 113, 83, 76, 86, 70, 90, 96, 98, 113, 69, 100, 98, 189, 70, 76, 80, 86, 87, 79, 96, 101, 78, 76, 83, 93, 190, 114, 102, 80, 88, 114, 87, 97, 101, 99, 99, 72, 7784, , 91, 76, 71, 80, 70, 191, 87, 77, 81, 97, 102, 71, 88, 80, 94, 115, 103, 79, 192, 84, 115, 98, 102, 100, 10081, , 77, 92, 89, 85, 88, 78, 73, 72, 88, 193, 71, 81103, , 98, 78, 77, 82, 116, 72, 95, 89, 116, 104, 81, 194, 99, 101, 103, 101, 85, 82, 78, 93, 80, 86, 104, 89, 99, 73, 79117, , 82, 72195, , 89, 74, 117, 90, 83, 96, 105, 73, 90, 100, 102, 79, 102, 104, 196, 82, 78, 86, 94, 105, 83, 118, 79, 90, 87, 118, 100, 197, 83, 106, 97, 90, 80, 73, 91, 84, 75, 101, 81, 103, 103, 74, 105, 74, 91, 198, 119, 83, 95, 106, 7987, , 119, 80, 91, 84, 80, 101, 88, 107, 199, 98, 91, 104, 106, 104, 81, 85, 76, 120, 92, 92, 120, 75, 107, 84, 200, 74, 82, 96, 88, 7580, , 84, 92, 102, 85, 81, 108, 89, 99, 201, 81, 107, 105, 105, 121, 121, 86, 82, 92, 108, 93, 77, 202, 97, 76, 75, 85, 93, 103, 10993, , 81, 85, 89, 82, 100, 122, 90, 122, 83108, , 106, 203, 106, 76, 86, 87, 109, 9483, , 82, 93, 98, 204, 78, 110, 104, 94, 76, 77, 123, 94, 123, 101, 86, 90, 109, 83, 82, 107, 107, 91, 205, 86, 110, 88, 9577, , 99, 83, 84, 124, 87, 84, 111, 105, 124, 95, 94, 206, 79, 102, 77, 78, 110, 108, 108, 91, 95, 87, 207, 83, 87, 92, 84, 125, 89, 111, 96, 125, 112, 100, 106, 96, 88, 85, 85, 111, 103, 208, 95, 84, 78, 109, 80, 109, 78, 79, 92, 96, 88, 126, 209, 93, 88, 113, 90, 97, 101, 107, 84, 97, 112, 112, 85, 110, 86, 210, 96, 104, 85, 127, 79, 110, 81, 89, 79, 80, 93, 86, 89, 114, 211, 94, 98, 91, 102, 97, 113, 108, 113, 98, 89, 128, 111, 212, 87, 97, 105, 85, 80, 82, 86, 115, 86, 81, 90, 94, 80, 111, 99, 213, 114, 92, 114, 103, 129, 90, 99, 95, 87, 112, 90, 98, 106, 98, 214, 88, 116, 81, 112, 83, 82, 95, 130, 115, 91, 87, 100, 86, 81, 104, 100, 215, 93, 87, 91, 113, 115, 88, 107, 91, 117, 96, 99, 216, 131, 89, 99, 116, 84, 96, 113, 92, 88, 83, 101, 105, 114, 82, 101, 116, 82, 92, 94, 217, 88, 89, 118, 132, 108, 87, 92, 117, 218, 97, 100, 90, 114, 100, 85, 97, 102, 93, 115, 89, 117, 106, 84, 102, 93, 95, 89, 219, 133, 119, 83, 90, 83, 109, 118, 220, 115, 93, 98, 116, 103, 118, 88, 91, 98, 90134, , 107, 94, 101, 101, 120, 85, 96, 103, 221, 90, 94, 119, 84, 110, 91, 116, 84, 117, 135, 222, 104, 99, 86, 108, 94, 91, 12199, , 119, 95, 97, 89, 86, 223, 120, 91, 111, 95, 102, 102, 92, 136, 104, 117, 85, 118, 92, 105, 224, 100, 122, 92, 8595, , 96, 100, 98, 121, 120, 112, 225, 137, 92, 87, 103, 90, 96, 119, 93, 106, 103, 86, 105, 226, 123, 118, 101, 87, 121, 122, 96, 138, 97, 93, 86, 113, 93, 99, 227, 93, 88, 101, 120, 104, 97, 107, 124, 94, 119, 228, 139, 106, 102, 123, 122, 91, 87, 104, 98, 97, 100, 114, 87, 229, 94, 121, 94, 102, 98, 89, 125, 108, 140, 88, 120, 105, 123, 124, 230, 107, 95, 99, 115, 103, 94, 88, 101, 122, 98, 88, 105, 231, 95, 141, 92, 126, 109, 95, 121, 90, 124, 125, 103, 106, 108, 99, 232, 116, 100, 123, 102, 142, 89, 104, 89, 127, 89, 96, 96, 99, 233, 122, 110, 125, 12693, , 107, 91, 106, 104, 100, 117, 96, 234, 95, 143, 109, 101, 124, 128103, , 90, 126, 90, 127, 97, 123, 111, 235, 100, 94, 107, 108, 105, 144, 97, 105, 92, 118, 101, 125, 90, 102, 129, 236, 110, 97, 104, 127, 124, 128, 91, 112, 98, 91, 145, 237, 95, 101, 109, 119, 126, 96, 106, 93, 130, 102, 106, 91, 103, 108, 98, 111, 238, 125, 129, 128, 105, 146, 113, 92, 98, 99, 92, 110, 239, 96, 127, 120, 131, 102, 107, 97, 103, 94, 104, 107, 92, 126, 147, 130, 99, 129, 240, 106, 114, 112, 93, 100, 109, 93, 128, 121, 132, 99, 97, 241, 111, 148, 127, 98, 105, 103, 131, 104, 130, 108, 95, 93, 115107, , 108, 242, 101, 94, 129, 110, 133, 100, 149, 122, 113, 112, 243, 98, 128, 94, 132, 131, 106, 10999, , 100, 104, 11696, , 94, 108, 244, 109, 150, 134, 102, 123, 105, 95, 114, 113, 129133, , 132, 245, 111, 99, 107, 101, 110, 117, 151, 105, 97, 135, 246, 95, 101, 124, 110, 103, 134, 133, 109, 130, 115, 96, 247, 95, 112, 100108, , 152, 114, 102, 111, 118, 100, 136, 106, 248, 106, 125, 98, 102, 96, 134, 135, 111, 104131, , 153, 110, 249, 116, 109, 96, 97, 101, 137, 103, 119, 112, 113, 115, 126, 250, 135, 136, 103, 107, 107, 132, 154, 99, 105, 97, 112, 251, 138, 110, 120, 104, 111, 102, 98, 114, 113, 127, 117, 116, 155, 136137, , 101, 252, 133, 104, 108, 100, 106, 108, 139, 98, 253, 97, 111, 156, 113, 105, 137, 138, 103, 114, 128, 117, 99, 115, 112, 134, 254, 105, 109, 118, 107, 140, 101, 109, 157, 255, 112, 99, 138, 139, 106, 129, 118, 115, 135, 100, 256, 104, 114, 102, 141, 158, 106, 116, 110, 108, 113, 119, 110, 102, 257, 113, 139, 140, 98, 130, 100, 119136, , 107, 159, 258, 101, 142, 116, 107, 111, 109141, , 114, 140, 111, 105, 114, 117, 103, 115, 131, 137, 160, 120, 120, 101, 143, 102, 142, 103, 112, 141, 108, 115, 108, 110, 115, 112, 161, 132, 99, 116, 117, 138, 104, 121, 121, 144, 106, 118, 102, 103, 143, 142, 162, 109, 113, 116, 116, 111, 133, 113, 139, 117, 122, 105, 109, 122, 144, 118, 143, 103, 163, 104, 119, 104, 107, 110, 117114, , 134, 140, 112, 100, 114, 123, 118, 117, 123, 164, 145, 144, 106, 110, 119, 120, 104, 105105, , 108, 141, 135, 118, 111, 115, 113, 115, 165, 124, 145, 146, 119, 107, 124, 118, 142, 111, 106, 136, 105, 106119, , 109, 166, 120, 121, 101, 116, 114, 112, 146, 147, 116, 125, 120, 143, 108, 167, 137, 120, 107, 106, 148, 110, 107, 125, 147, 121, 115, 119, 117, 112, 117, 102, 122, 113, 168, 144, 126, 121, 138, 109, 149, 148, 121, 108, 107, 111, 116, 120, 118, 118, 126, 169, 122, 123, 113, 145, 114, 127, 139, 150, 149, 122, 122, 110, 170, 103, 109, 108, 108, 112, 117, 121, 119, 119, 146, 114123, , 124, 140, 127, 151, 150, 171, 128, 123, 115, 104, 110, 118, 109123, , 113, 120, 147, 122, 124, 152, 141, 151, 125, 172, 120, 115, 124, 109, 128, 148, 129, 119, 116, 111, 121, 110, 105, 123114, , 153, 173, 152, 142, 124, 125, 126, 121, 116, 125, 129, 149, 174, 130, 154, 153, 120, 122, 143, 111115, , 117, 126, 112, 124, 125, 122, 117, 127, 126, 110, 175, 150130, , 155, 131, 154, 144, 121, 106, 123, 116, 112, 176, 126, 118, 118, 127, 123, 127, 151, 156, 125, 155, 132, 131, 128, 113, 145, 122, 177, 124, 107, 117, 113, 152, 157, 128, 119, 156, 127, 124, 111, 128, 119, 133, 146, 178, 129, 126, 123, 125, 114, 132, 158, 118, 157, 114129, , 120, 108, 179, 134, 128, 147, 112, 129, 120, 125, 127, 124, 126, 159, 133, 130, 158, 115, 130, 119, 180, 115, 121, 135, 109, 148, 129, 113, 121, 160, 126, 159, 125, 181, 130, 127, 134, 131116, , 128, 131, 136, 122, 149, 116, 120, 161, 182, 122, 130, 160, 114, 127, 131135, , 126, 128, 132, 150, 132, 123, 110, 183, 129, 117, 117, 162, 161, 123, 137, 115, 128, 131, 132, 127, 129, 121, 133, 151, 184, 136, 163, 133, 124, 111, 130, 162, 118, 124, 138, 118, 129, 185, 128, 133, 116, 132, 130, 134, 152, 164, 125, 163, 137, 112, 122, 134, 119, 186, 125, 139, 131, 153, 129, 135, 165, 131, 117, 133, 164, 119, 130, 126, 138, 134, 187, 113, 140, 126, 120, 135, 166, 123, 154, 136, 130, 165, 132, 132, 188, 118, 134, 131, 139, 127, 120, 141, 167, 135, 155, 121, 166, 189, 136, 127, 137, 114, 124, 131, 133, 140, 132, 119, 128, 135, 168, 121, 142, 190, 156, 167, 138, 136, 122, 132, 128, 141, 137, 115, 133, 129, 169, 191, 125, 134, 120, 143, 136, 157, 168, 122, 139, 123, 133, 137, 192, 142, 138, 170, 129, 134, 130, 169, 158135, , 144, 121, 116, 137, 126, 140, 193, 123, 134, 171, 138, 124, 143, 170, 135, 131, 159, 139, 130, 145, 194, 122, 136, 138, 141, 172, 127, 135, 117, 139, 171, 125, 124, 160, 140, 132146, , 195, 144131, , 136, 173, 123, 142, 139, 137, 172, 140, 136, 118, 196, 128, 161, 126, 141, 133, 147, 132, 174, 125, 145, 143, 173, 137, 124, 197, 140, 138, 162, 141, 137, 129, 142, 119, 134, 148, 127, 175, 133, 198, 174, 126, 146, 144, 141, 163, 139, 125, 142, 138, 138, 143, 149, 176, 130, 135, 199, 128, 175, 134, 147, 145, 164, 127, 140, 142, 120, 126, 177, 200, 144139, , 150, 143, 136, 131, 176, 129, 139, 148, 146, 135, 165, 201, 141, 178, 143, 128, 145, 151, 127, 140, 177, 137, 132, 144, 149, 130, 140, 147, 202, 166, 136, 179, 121, 146178, , 152, 144, 142, 141, 128, 138, 129, 203, 133, 150, 148, 167, 180, 131, 145, 137, 179, 141, 147, 153, 122, 204, 145, 142, 139, 143, 129, 181, 168, 130, 151, 149, 134, 132, 138, 180, 146, 205, 148, 154, 142, 123, 143, 146, 182, 140, 144169, , 152, 150, 130, 131, 135, 181, 206, 133, 139, 147, 149, 155, 143, 183, 124, 144, 170, 147, 141, 145, 207, 153, 182, 151, 131, 136, 132, 140, 156, 150, 134, 148, 184, 208, 171, 144, 145, 125, 183, 148, 142, 152, 154, 146, 137, 132, 157, 151, 141, 185, 209, 133, 149, 135, 172, 184, 146, 143, 145, 149, 153, 155, 210, 186, 158, 152, 142, 133, 150, 185, 126, 173, 134, 136, 138, 147, 147, 144, 211, 154, 150, 146, 187, 153, 156, 143, 186, 159, 151, 174, 134, 135, 212, 139, 137, 148, 188, 145, 155, 151, 127, 147, 148, 154, 187, 157, 144, 175, 152, 213, 135, 189, 136, 160, 140, 138, 149, 156, 146, 152, 188, 155, 128, 214, 148, 158, 176, 145, 153, 149, 190, 161, 136, 137, 150, 157, 215, 139, 147, 189, 156, 153, 177, 141, 149, 159, 129, 146, 191, 216, 154, 190, 150, 158, 151, 137, 138162, , 148, 140, 157, 178, 160, 154, 150, 192, 217, 147, 191, 155, 159, 142, 130, 158, 151, 149, 152, 179, 138, 141, 139, 218, 193, 161, 155, 163, 151, 148, 192, 156, 160, 219, 159, 180, 143, 150, 194, 153, 152, 164, 139142, , 156, 152, 140, 193, 162, 149, 131, 220, 157, 161, 181, 160, 195, 151, 144, 154, 153, 194, 165, 157, 143, 140, 141, 221, 150, 153, 163, 196, 182, 162, 158, 161, 132, 152, 195, 155, 145, 166, 222, 158, 154, 144, 141, 151, 197, 142, 183, 154, 163, 159, 162, 164, 196, 223, 153, 167, 156, 146, 159, 133, 198, 155, 145, 142, 152, 184, 224, 163, 164, 155, 197, 160, 143, 154, 168, 157, 165, 147, 199, 160, 134, 225, 153, 185, 146, 143, 156, 198, 164, 165, 156, 161, 144, 169, 155, 200, 158, 166, 226, 161, 148, 186, 199, 154, 147, 165, 157, 166, 144, 170, 156, 201, 227, 157, 162, 167, 145, 159, 135, 162, 187, 200, 149, 155, 166, 158, 167, 148, 145, 228, 202, 171, 157, 163, 168, 158, 160, 201, 188, 136, 146, 163, 150, 167, 156, 229, 168, 159, 203, 149, 172, 146, 158, 202, 164, 189, 161, 159, 169, 168, 151, 147, 204, 157, 169, 160, 164, 173, 150, 190, 159, 147, 165, 137, 162, 160, 205, 169, 170, 158, 152, 174, 161, 148, 170, 191, 151, 165, 160, 166, 148, 206, 170, 161, 163, 171, 175, 159, 171, 153, 192, 162, 149, 207, 138, 161, 152, 166, 171, 167, 149, 162, 164, 176, 172, 193, 172, 160, 154, 208, 163, 150, 162, 172, 167, 153, 168, 177, 165, 163, 173, 150, 194, 173, 209, 161, 155, 164, 139, 173, 163, 151, 168, 169, 178, 195, 154, 174, 164, 166, 210, 174, 162, 165, 151, 174, 156, 164, 196, 169, 179, 152, 170, 211, 175, 165, 155, 167, 175, 140, 163, 175, 166, 157, 152, 197, 165, 212, 180, 170, 171, 176, 176, 166, 153, 168, 156, 176, 164, 167, 198, 181, 158, 166, 153, 171, 177, 172, 177, 169, 167, 141, 154, 177, 157, 199, 165, 168, 182, 167, 159, 178, 173, 154, 172, 170, 178, 178, 168, 200, 155, 158, 183, 166, 169, 168, 179, 174, 142, 160, 179, 201, 155, 171, 169, 179, 184, 173, 170, 167, 156159, , 180, 169, 175, 202, 180, 172, 156, 185, 180, 170, 171, 174, 168, 161, 160, 157, 181, 203, 143, 170, 176, 181, 186, 173, 181, 171, 157, 175, 172, 169, 204, 182, 158, 161, 182, 171, 177, 162, 187, 182, 174, 172, 173, 205, 176, 158, 170, 144, 183, 183, 159, 172, 178, 162, 188, 163, 183, 175, 206, 173, 174, 177, 184, 171, 184, 145, 159, 179, 173, 160, 163, 189, 184, 207, 164, 176, 174, 185, 175, 178, 185, 172, 146, 180, 174, 190, 161, 208, 164, 185, 165, 177, 186, 186, 176, 175, 173, 160, 191, 181, 147, 179, 175, 209, 162, 186, 165, 178, 187, 166, 177, 176, 174, 192, 210, 182, 148, 176, 187, 163, 188, 166, 180, 179, 167, 178, 161, 177, 211, 193, 175, 183, 177, 188, 149, 164, 189, 180, 167, 179, 212, 181, 194, 178, 162, 176, 184, 168, 178189, , 190, 165, 150, 213, 181, 195, 180, 168, 182, 179, 177, 185, 163, 190, 179, 191, 214, 166, 196, 169, 182, 183, 180, 169, 178, 186, 191, 192, 180, 164, 151, 215, 197, 167, 183, 184, 181, 170, 192, 170, 179, 193, 187, 216, 181, 165, 198, 168, 184, 185, 182, 194, 193, 217, 180, 188, 152, 171, 171, 182, 199, 166, 185, 169, 186, 218, 195, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183, 194, 181, 189, 200, 183, 172, 172, 167, 219, 186, 196, 195, 170, 187, 153184, , 182, 190, 201, 184, 173, 220, 197, 187, 196, 168, 171, 173, 185, 188, 154, 202, 183, 191, 185, 221, 174, 198, 188, 197, 203, 186, 172, 169, 155, 192, 184, 186, 222, 189, 174, 199, 175, 198, 189, 204, 187, 173, 193, 185, 156, 170, 223, 187, 190, 200, 175, 199, 176, 205, 190, 188, 194, 174, 224, 186, 157, 188, 191, 201, 176, 200, 206, 191, 177, 225, 195, 189, 171, 175, 187, 189, 202, 192, 177, 201, 192, 158, 226, 178, 196, 190, 176, 188, 203, 190, 202, 178, 227, 193, 193, 159, 197, 172, 191, 179, 177, 189, 204, 191, 203, 228, 194, 194, 179, 160, 198, 205, 192, 190, 180, 178, 192, 204, 229, 195, 195, 173, 199, 206, 161, 193, 180, 191, 230, 181, 193, 179, 205, 196, 196, 207, 200, 194, 231, 192162, , 181, 206, 194, 180, 182, 208, 174, 197, 197, 201, 232, 195, 207, 193, 163, 195, 182, 181, 183, 209, 233, 198, 202, 198, 196, 208, 194, 196, 182, 210, 234, 183, 175, 184, 203, 164, 199, 199, 209, 197, 195, 197, 235, 211, 183, 184, 176, 185, 204, 200, 200, 210, 198, 196, 236, 198, 212, 165, 184, 205, 186, 177, 211, 201, 201, 185, 199, 237, 197, 213, 199, 185, 206, 212, 166, 202, 238, 187, 202, 178, 186, 200, 214, 198, 200, 207, 186, 213, 239, 167, 203, 203, 215, 201, 188, 187, 179, 199, 201, 240, 208, 214, 187, 216, 204, 204, 168, 202, 189, 188, 202, 200, 180, 241, 215, 209, 217, 188, 205, 205, 203169, , 203, 242, 189, 190, 201, 181, 216, 210, 218, 189, 206, 206, 204, 243, 170, 204, 202, 190, 217, 191, 219, 211, 182, 190, 244, 207, 207, 205, 205, 218, 203, 191, 171, 220, 192, 212, 245, 208, 183, 208, 191, 206, 219, 206, 204, 221, 213, 192, 193, 246, 209, 209, 192, 207, 220, 207, 222, 172, 205, 214, 247, 193, 194, 184, 210, 210, 221, 208, 193, 223, 208, 206, 248, 215, 194, 195, 211, 222, 211, 185, 224, 173, 209, 194, 249, 209, 207, 216, 195, 223, 225, 196, 212, 250, 212, 210, 186, 210, 195, 208, 217, 174, 226, 251, 224, 196, 213, 213, 211, 197, 211, 196, 209, 187, 218, 252, 227, 225, 175, 214, 214, 197, 212, 212, 198, 210, 197, 219, 188, 253, 228, 226, 176, 215, 215, 213, 198, 213, 211, 199, 220, 254, 198, 229, 227, 189, 216, 177, 216, 214, 214255, , 221, 212, 199, 230, 200, 199, 228, 217, 256, 217, 178, 190, 215, 215, 231, 222, 213, 229, 200, 201, 200, 257, 218, 218, 232, 216, 216, 179, 223, 214, 230, 201, 201, 202, 258, 233, 219, 219, 191, 217, 217, 224, 231, 180, 215, 202, 203, 202, 234, 220, 220, 218, 232, 218, 225, 192, 181, 216, 203, 235, 204, 221, 221, 203, 233, 219, 226, 219, 217, 193, 182, 236, 204, 234, 205, 222, 222, 220, 227, 204, 220, 218, 183, 237, 205, 235, 223, 223, 221, 228, 206, 221, 194, 205, 219, 238, 236, 206, 224, 224, 222, 229, 184, 222, 207, 206, 239, 220, 237, 207, 195, 225, 225, 230223, , 223, 240, 207, 208, 221, 238, 185, 226, 208, 231, 226, 224, 241, 196, 224, 208, 222, 209, 239, 186, 227, 232, 225, 227, 209, 225, 197, 240, 223, 209, 210, 228, 233, 187, 226, 228, 210, 226, 241, 198, 224, 210, 211, 229, 234, 227, 229, 188, 227, 211, 242, 225, 199, 211, 212, 235, 230, 228, 230, 243, 189, 228, 212, 226, 200, 212, 236, 231, 229, 213, 244, 231, 229, 190, 213, 227, 213, 237, 232, 245, 230, 232, 214, 230, 228, 191, 214201, , 238, 246, 214, 231, 233, 233, 231, 215, 229, 215, 202, 192, 247, 234, 232, 234, 215, 232, 230, 216, 216, 248, 203, 193, 235, 233, 235, 233, 216, 231, 249, 217, 217, 204, 234, 236, 194, 236, 234, 232, 250, 217, 218, 218, 235, 237, 205, 237, 235, 195, 251, 233, 218, 219, 219, 236, 238, 238, 236, 206, 252, 196, 234, 219, 220, 237, 239, 220, 239, 237, 253, 235, 207, 197, 220, 221, 238, 240, 254, 221, 240, 238, 236, 198, 208, 221, 239, 255, 222, 241, 241, 239, 222, 237, 209, 222, 240, 242, 199, 223, 242, 240, 223, 238, 241, 210, 243, 223, 224, 243, 241, 239, 200, 224, 242, 244, 224, 244, 211, 242, 225, 240, 225, 243, 245, 201, 245, 243, 225, 226, 212, 241, 226, 246, 246, 202, 244, 226, 227, 242, 213, 227, 247, 247, 245, 203, 227, 228, 243, 214, 248, 228, 248, 246, 204, 244, 229228, , 215, 249, 249, 247, 229, 245, 205, 230, 229, 250, 250, 216, 248, 230, 246, 206, 231, 230, 251, 251, 249, 217, 247, 207231, , 232, 231, 252, 252, 250, 248, 218, 208, 233, 232, 232, 253, 253, 251, 249, 219, 209, 234, 233, 233, 254, 254, 252, 250, 235, 210, 220, 234, 234, 255, 255, 253, 251, 236, 211, 235, 256, 256, 221, 254, 235, 252, 237, 212, 257, 257, 236, 255, 222, 253, 236, 238, 258, 258, 213, 256, 237, 254, 223, 237, 259, 239, 259, 257, 214, 238, 255, 238, 224, 260, 260, 240, 258, 215, 239, 256, 239, 225, 261, 259, 261, 241, 216, 257, 240, 240, 262, 226, 262, 260, 242, 258, 217, 241, 263, 241, 261, 263, 227, 243, 259, 218, 242, 264, 262, 242, 260, 244, 228, 219, 243, 265, 263, 261, 243, 245, 229, 220, 244, 266, 264, 262, 246, 245, 221, 230, 267, 265, 263, 247, 246, 222, 268, 231, 264, 248, 247, 223, 269, 265, 232, 249, 248, 270, 224, 266, 233, 250, 271, 249, 225, 267, 234, 251, 272, 250, 268, 226, 252, 273, 235, 269, 251, 227, 274, 253, 270, 236, 252, 228, 275, 271, 254, 253, 237, 229, 276, 272, 255, 254, 230, 238, 277, 273, 256, 255, 231, 278, 239, 274, 257, 256, 232, 279, 275, 240, 258, 257, 233, 280, 276, 259, 241, 258, 281, 277, 234, 260, 242, 259, 282, 278, 235, 261, 283, 279, 260, 243, 236, 262, 280, 284, 261, 244, 237, 263, 281, 285, 262, 238, 245, 264, 282, 286, 263, 239, 246, 283, 287, 265, 264, 240, 284, 288, 266, 247, 265, 241, 285, 289, 267, 248, 266, 286, 242, 290, 268, 267, 249, 287, 291, 243, 269, 268, 250, 292, 244, 270, 269, 293, 245, 251, 271, 270, 294, 246, 272, 252, 295, 271, 247, 273, 253, 272, 248, 274, 273, 254, 249, 275, 274, 255, 250, 276, 275, 277, 251, 256, 276, 278, 252, 257, 277, 279, 253, 278, 258, 280, 254, 279, 281, 259, 255, 282, 256, 283, 257, 284, 258, 285, 259, 286, 260, 287, 261, 288, 262, 289, 263, 290, 264, 291, 265, 292, 266, 293, 294, done!\n"
     ]
    }
   ],
   "source": [
    "from hrvanalysis import get_time_domain_features\n",
    "from scipy.stats import kurtosis, skew\n",
    "from os import listdir as ld\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import os\n",
    "import threading\n",
    "os.system(f'find {Paths.get(Dirs.ROOT)} -name \".DS_Store\" -delete')\n",
    "\n",
    "\n",
    "WINDOW_SIZE = 60000\n",
    "OVERLAP = 0.75\n",
    "\n",
    "# compute lab features\n",
    "scenarios = ld(Paths.get(Dirs.NORMALIZED_LAB_STRESS_DATASET))\n",
    "if '.DS_Store':\n",
    "  scenarios.remove('.DS_Store')\n",
    "for scenario in scenarios:\n",
    "  print(scenario)\n",
    "  stress_label = 1 if 'stress' in scenario else (0 if 'baseline rest' in scenario else None)\n",
    "  if stress_label is not None:\n",
    "    for participant in ld(Paths.get(Dirs.NORMALIZED_LAB_STRESS_DATASET, scenario)):\n",
    "      rr_path = Paths.get(Dirs.NORMALIZED_LAB_STRESS_DATASET, scenario, participant, 'ppg_rr.csv')\n",
    "      hr_path = Paths.get(Dirs.NORMALIZED_LAB_STRESS_DATASET, scenario, participant, 'ppg_hr.csv')\n",
    "\n",
    "      if not os.path.exists(rr_path) or not os.path.exists(hr_path):\n",
    "        continue\n",
    "      \n",
    "      rr_df = pd.read_csv(rr_path, names=['ts', 'rr'], header=None)\n",
    "      rr_df.sort_values(by=['ts'])\n",
    "      hr_df = pd.read_csv(hr_path, names=['ts', 'hr'], header=None)\n",
    "      hr_df.sort_values(by=['ts'])\n",
    "      \n",
    "      from_ts = max(rr_df.iloc[0]['ts'], hr_df.iloc[0]['ts'])\n",
    "      till_ts = min(rr_df.iloc[-1]['ts'], hr_df.iloc[-1]['ts'])\n",
    "      while from_ts <= till_ts - WINDOW_SIZE:\n",
    "        sub_rr_df = rr_df.query(f'ts >= {from_ts} and ts < {from_ts + WINDOW_SIZE}')\n",
    "        sub_hr_df = hr_df.query(f'ts >= {from_ts} and ts < {from_ts + WINDOW_SIZE}')\n",
    "        from_ts += WINDOW_SIZE * (1 - OVERLAP)\n",
    "        \n",
    "        if sub_rr_df.shape[0] < 2 or sub_hr_df.shape[0] < 2:\n",
    "          continue\n",
    "        \n",
    "        # compute features\n",
    "        rr_tdf = get_time_domain_features(nn_intervals=sub_rr_df['rr'].values)\n",
    "        hr_tdf = get_time_domain_features(nn_intervals=sub_hr_df['hr'].values)\n",
    "        rr_values = sub_rr_df['rr'].values\n",
    "        hr_values = sub_hr_df['hr'].values\n",
    "        features = [\n",
    "          # rr\n",
    "          rr_tdf['mean_nni'],\n",
    "          rr_tdf['median_nni'],\n",
    "          rr_tdf['max_hr'],\n",
    "          rr_tdf['min_hr'],\n",
    "          rr_tdf['sdsd'],\n",
    "          kurtosis(rr_values, fisher=True),\n",
    "          skew(rr_values),\n",
    "          # TBD: rr_slope\n",
    "          np.percentile(rr_values, 20),\n",
    "          np.percentile(rr_values, 80),\n",
    "          # hr\n",
    "          hr_tdf['mean_nni'],\n",
    "          hr_tdf['median_nni'],\n",
    "          hr_tdf['max_hr'],\n",
    "          hr_tdf['min_hr'],\n",
    "          hr_tdf['sdsd'],\n",
    "          kurtosis(hr_values, fisher=True),\n",
    "          skew(hr_values),\n",
    "          # TBD: hr_slope\n",
    "          np.percentile(hr_values, 20),\n",
    "          np.percentile(hr_values, 80)\n",
    "        ]\n",
    "\n",
    "        path = Paths.get(Dirs.FEATURES_LAB_STRESS, f'{participant}.csv')\n",
    "        write_header = not os.path.exists(path)\n",
    "        with open(path, 'a+') as w:\n",
    "          if write_header:\n",
    "            w.write('rr_mean_nni,rr_median_nni,rr_max_hr,rr_min_hr,rr_sdsd,rr_kurtosis,rr_skew,rr_p20,rr_p80,hr_mean_nni,hr_median_nni,hr_max_hr,hr_min_hr,hr_sdsd,hr_kurtosis,hr_skew,hr_p20,hr_p80,label\\n')\n",
    "          w.write(f'{\",\".join([str(x) for x in features])},{stress_label}\\n')\n",
    "\n",
    "\n",
    "EMA_FEATURE_PERIOD = 30*60000 # ms\n",
    "USE_Z_SCORE = False\n",
    "# compute field features\n",
    "def compute_features(idx, participant):\n",
    "  print()\n",
    "  print(f'{idx+1}/{len(participants)} {participant}', end=' ')\n",
    "  rr_path = Paths.get(Dirs.NORMALIZED_FIELD_DATASET, participant, 'ppg_rr.csv')\n",
    "  hr_path = Paths.get(Dirs.NORMALIZED_FIELD_DATASET, participant, 'ppg_hr.csv')\n",
    "  ema_df = pd.read_csv(Paths.get(Dirs.INIT_PARTICIPANTS, participant, 'ema.csv'), names=['ts', 'cheerful', 'happy', 'angry', 'nervous', 'sad', 'activity', 'lat', 'lon'], header=None)\n",
    "\n",
    "  if not os.path.exists(rr_path) or not os.path.exists(hr_path):\n",
    "    return\n",
    "\n",
    "  rr_df = pd.read_csv(rr_path, names=['ts', 'rr'], header=None)\n",
    "  rr_df.sort_values(by=['ts'])\n",
    "  hr_df = pd.read_csv(hr_path, names=['ts', 'hr'], header=None)\n",
    "  hr_df.sort_values(by=['ts'])\n",
    "  \n",
    "  # compute ema label threshols for GT labeling\n",
    "  ema_scores = []\n",
    "  for idx, row in ema_df.iterrows():\n",
    "    score = ((4 - row['cheerful']) + (4 - row['happy']) + row['angry'] + row['nervous'] + row['sad']) / 5\n",
    "    ema_scores += [score]\n",
    "  ema_mean = np.mean(ema_scores)\n",
    "  ema_z_scores = stats.zscore(ema_scores)\n",
    "\n",
    "  # compute features for each EMA period\n",
    "  with open(Paths.get(Dirs.FEATURES_FIELD, f'{participant}.csv'), 'w+') as w:\n",
    "    w.write('rr_mean_nni,rr_median_nni,rr_max_hr,rr_min_hr,rr_sdsd,rr_kurtosis,rr_skew,rr_p20,rr_p80,hr_mean_nni,hr_median_nni,hr_max_hr,hr_min_hr,hr_sdsd,hr_kurtosis,hr_skew,hr_p20,hr_p80,label\\n')\n",
    "    max_count = ema_df.shape[0]\n",
    "    #print(f'out of {max_count}: ', end=' ')\n",
    "    for idx, row in ema_df.iterrows():\n",
    "      print(idx+1, end=', ', flush=True)\n",
    "      ema_score = ((4 - row['cheerful']) + (4 - row['happy']) + row['angry'] + row['nervous'] + row['sad']) / 5\n",
    "      if USE_Z_SCORE:\n",
    "        stress_label = 1 if ema_z_scores[idx] > .6 else 0\n",
    "      else:\n",
    "        stress_label = 1 if ema_score > ema_mean else 0\n",
    "      \n",
    "      from_ts = row['ts'] - EMA_FEATURE_PERIOD\n",
    "      till_ts = row['ts']\n",
    "      while from_ts <= till_ts - WINDOW_SIZE:\n",
    "        sub_rr_df = rr_df.query(f'ts >= {from_ts} and ts < {from_ts + WINDOW_SIZE}')\n",
    "        sub_hr_df = hr_df.query(f'ts >= {from_ts} and ts < {from_ts + WINDOW_SIZE}')\n",
    "        from_ts += WINDOW_SIZE * (1 - OVERLAP)\n",
    "        \n",
    "        if sub_rr_df.shape[0] < 2 or sub_hr_df.shape[0] < 2:\n",
    "          continue\n",
    "        \n",
    "        # compute features\n",
    "        rr_tdf = get_time_domain_features(nn_intervals=sub_rr_df['rr'].values)\n",
    "        hr_tdf = get_time_domain_features(nn_intervals=sub_hr_df['hr'].values)\n",
    "        rr_values = sub_rr_df['rr'].values\n",
    "        hr_values = sub_hr_df['hr'].values\n",
    "        features = [\n",
    "          # rr\n",
    "          rr_tdf['mean_nni'],\n",
    "          rr_tdf['median_nni'],\n",
    "          rr_tdf['max_hr'],\n",
    "          rr_tdf['min_hr'],\n",
    "          rr_tdf['sdsd'],\n",
    "          kurtosis(rr_values, fisher=True),\n",
    "          skew(rr_values),\n",
    "          # TBD: rr_slope\n",
    "          np.percentile(rr_values, 20),\n",
    "          np.percentile(rr_values, 80),\n",
    "          # hr\n",
    "          hr_tdf['mean_nni'],\n",
    "          hr_tdf['median_nni'],\n",
    "          hr_tdf['max_hr'],\n",
    "          hr_tdf['min_hr'],\n",
    "          hr_tdf['sdsd'],\n",
    "          kurtosis(hr_values, fisher=True),\n",
    "          skew(hr_values),\n",
    "          # TBD: hr_slope\n",
    "          np.percentile(hr_values, 20),\n",
    "          np.percentile(hr_values, 80)\n",
    "        ]\n",
    "        w.write(f'{\",\".join([str(x) for x in features])},{stress_label}\\n')\n",
    "\n",
    "participants = ld(Paths.get(Dirs.NORMALIZED_FIELD_DATASET))\n",
    "if '.DS_Store' in participants:\n",
    "  participants.remove('.DS_Store')\n",
    "threads = []\n",
    "for idx, participant in enumerate(participants):\n",
    "  t = threading.Thread(target=compute_features, args=(idx, participant))\n",
    "  t.start()\n",
    "  threads += [t]\n",
    "for t in threads:\n",
    "  t.join()\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osE8ZzLKZfje"
   },
   "source": [
    "## 2.4 Generalized Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "3Dq6NFag_66l",
    "outputId": "3c41928b-10b3-429a-ddad-64e1a9d0c3e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/35 12170603@inha.edu.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/35 12181157@inha.edu.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/35 12201686@inha.edu.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/35 12212982so@inha.edu.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/35 as7177as3912@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/35 bagle1029@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/35 bw04029@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/35 chaewoni65@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/35 csissis1997@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/35 david000914@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/35 duecjf011521212106@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/35 eowjdtjdwls@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/35 fpdlsqhdn962@inha.edu.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/35 gh011127@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/35 gurwns7772@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/35 hhcc05@inha.edu.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/35 jinsoonshim@naver.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/35 jwshoho4@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/35 jyypaul@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/35 km55181499@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 kpm1323@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/35 memm1439@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/35 minjae20000207@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/35 minrudcho01@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/35 nigaram8@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/35 nnozilaxonim@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/35 powerampshere@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/35 rlarkgus11170@inha.edu.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/35 unicpn21@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/35 vusgowlwk17@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/35 wjdwogus0604@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/35 wonjs0725@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/35 ysl@inha.edu.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 zkapdh123@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12170603@inha.edu.csv, 0.563876104620552, 0.51\n",
      "12181157@inha.edu.csv, 0.5060645569120146, 0.5\n",
      "12201686@inha.edu.csv, 0.5444729383182616, 0.49\n",
      "12212982so@inha.edu.csv, 0.5044070997617243, 0.51\n",
      "as7177as3912@gmail.com.csv, 0.5850852341871355, 0.45\n",
      "bagle1029@gmail.com.csv, 0.45988970970762666, 0.51\n",
      "bw04029@gmail.com.csv, 0.5432625321034461, 0.51\n",
      "chaewoni65@gmail.com.csv, 0.5283466697109739, 0.49\n",
      "csissis1997@gmail.com.csv, 0.5763899122296765, 0.52\n",
      "david000914@gmail.com.csv, 0.5195588235294117, 0.52\n",
      "duecjf011521212106@gmail.com.csv, 0.43636519623135955, 0.49\n",
      "eowjdtjdwls@gmail.com.csv, 0.5916921132804043, 0.51\n",
      "fpdlsqhdn962@inha.edu.csv, 0.5141390679603461, 0.49\n",
      "gh011127@gmail.com.csv, 0.5321888532036749, 0.51\n",
      "gurwns7772@gmail.com.csv, 0.48082454458293383, 0.59\n",
      "hhcc05@inha.edu.csv, 0.5880138078745898, 0.59\n",
      "jinsoonshim@naver.com.csv, 0.4816409391125629, 0.51\n",
      "jwshoho4@gmail.com.csv, 0.5013128463190195, 0.49\n",
      "jyypaul@gmail.com.csv, 0.41940733994468055, 0.48\n",
      "km55181499@gmail.com.csv, 0.7054514198295101, 0.56\n",
      "kpm1323@gmail.com.csv, 0.6221572681505763, 0.5\n",
      "memm1439@gmail.com.csv, 0.5087243049978678, 0.56\n",
      "minjae20000207@gmail.com.csv, 0.5247879968500021, 0.54\n",
      "minrudcho01@gmail.com.csv, 0.5176622618148405, 0.5\n",
      "nigaram8@gmail.com.csv, 0.5222093097221222, 0.53\n",
      "nnozilaxonim@gmail.com.csv, 0.5372027144395918, 0.53\n",
      "powerampshere@gmail.com.csv, 0.46892318556629486, 0.5\n",
      "rlarkgus11170@inha.edu.csv, 0.5735660271214095, 0.6\n",
      "unicpn21@gmail.com.csv, 0.6082020473355598, 0.51\n",
      "vusgowlwk17@gmail.com.csv, 0.537503747780161, 0.51\n",
      "wjdwogus0604@gmail.com.csv, 0.6555063282005398, 0.51\n",
      "wonjs0725@gmail.com.csv, 0.6000216540207185, 0.47\n",
      "ysl@inha.edu.csv, 0.5047651463580668, 0.49\n",
      "zkapdh123@gmail.com.csv, 0.5839975917788799, 0.5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x165.6 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join as j\n",
    "from os import listdir as ld\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import re\n",
    "import os\n",
    "os.system(f'find {Paths.get(Dirs.ROOT)} -name \".DS_Store\" -delete')\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "\n",
    "def extract_emails(_list):\n",
    "  _reg = re.compile(r'([0-9a-z.]+@[0-9a-z.]+)')\n",
    "  _elems = []\n",
    "  for _p in _list:\n",
    "    if _reg.search(_p):\n",
    "      _email = _reg.search(_p).group(1)\n",
    "      _elems += [(_email, _p)]\n",
    "  return _elems\n",
    "\n",
    "\n",
    "def train_test_generalized_xgboost(inp_dir, out_file, with_context=False):\n",
    "    inputs = extract_emails(ld(inp_dir))\n",
    "    inputs.sort(key=lambda x: x[0])\n",
    "    m_rounds = len(inputs)\n",
    "\n",
    "    feature_names = ['hr_min_hr', 'hr_max_hr', 'rr_mean_nni','rr_median_nni','rr_max_hr','rr_min_hr','rr_sdsd','rr_kurtosis','rr_skew','rr_p20','rr_p80','hr_mean_nni','hr_median_nni','hr_max_hr','hr_min_hr','hr_sdsd','hr_kurtosis','hr_skew','hr_p20','hr_p80']\n",
    "\n",
    "    models = []\n",
    "    x_tests = []\n",
    "\n",
    "    best_thresholds = {}\n",
    "    best_f1s = {}\n",
    "\n",
    "    with open(out_file, \"w+\") as w, open(f'{out_file[:out_file.rindex(\".csv\")]}_granular.csv', 'w+') as w_granular:\n",
    "        w.write('Balanced Accuracy,F1 score,ROC_AUC,TPR,TNR\\n')\n",
    "        w_granular.write('LOSO Subject,Balanced Accuracy,F1 score,ROC_AUC,TPR,TNR\\n')\n",
    "        all_scores = {'acc': [], 'f1': [], 'roc_auc': [], 'TPR': [], 'TNR': []}\n",
    "        \n",
    "        models = []\n",
    "        x_tests = []\n",
    "\n",
    "        for loso_idx in range(m_rounds):\n",
    "            # region load test subject's dataset\n",
    "            test_email, test_filename = inputs[loso_idx]\n",
    "            # if test_email != 'david000914@gmail.com.csv':\n",
    "            #   continue\n",
    "            test_dataset = pd.read_csv(j(inp_dir, test_filename)).replace([np.inf, -np.inf], np.nan,).dropna(axis=0)\n",
    "\n",
    "            if test_dataset.shape[0] < 2:\n",
    "              continue\n",
    "\n",
    "            x_test = test_dataset.iloc[:, :-1].copy()\n",
    "            x_test = x_test[feature_names]\n",
    "            y_test = test_dataset.iloc[:, -1].copy()\n",
    "            y_test = y_test.astype(int)\n",
    "\n",
    "            scores_1round = {'acc': [], 'f1': [], 'roc_auc': [], 'TPR': [], 'TNR': []}\n",
    "            x_train, y_train, initialized = None, None, False\n",
    "\n",
    "            print(f'{loso_idx+1}/{m_rounds} {test_email} participant is left out as test')\n",
    "\n",
    "            for idx, tp in enumerate(inputs):\n",
    "                if tp[0] == test_email:\n",
    "                    continue\n",
    "                train_dataset = pd.read_csv(j(inp_dir, tp[1])).replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n",
    "                _x_train = train_dataset.iloc[:, :-1].copy()\n",
    "                _x_train = _x_train[feature_names]\n",
    "                _y_train = train_dataset.iloc[:, -1].copy()\n",
    "                _y_train = _y_train.astype(int)\n",
    "                if _x_train.empty or _y_train.empty:\n",
    "                    continue\n",
    "                if initialized:\n",
    "                    x_train = pd.concat([x_train, _x_train])\n",
    "                    y_train = pd.concat([y_train, _y_train])\n",
    "                else:\n",
    "                    x_train = _x_train\n",
    "                    y_train = _y_train\n",
    "                    initialized = True\n",
    "\n",
    "            # SMOTE\n",
    "            try:\n",
    "                # 'fit_resample' conducts over-sampling data in the minority class. Again, resampling should be only conducted in train set.\n",
    "                x_train, y_train = SMOTE().fit_resample(x_train, y_train)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            \n",
    "            # np arrays --> data frames\n",
    "            x_train = pd.DataFrame(x_train, columns=x_train.columns)\n",
    "            y_train = pd.Series(y_train)\n",
    "\n",
    "            min_max_scaler = MinMaxScaler()\n",
    "            # StandardScaler.fit() finds characteristics of data distribution (i.e., min, max) in train set.\n",
    "            min_max_scaler.fit(x_train)\n",
    "            # Transform numeric data within train and test set.\n",
    "            x_train_scale = min_max_scaler.transform(x_train)\n",
    "            x_test_scale = min_max_scaler.transform(x_test)\n",
    "            # np arrays --> data frames\n",
    "            x_train = pd.DataFrame(x_train_scale, index=x_train.index, columns=x_train.columns)\n",
    "            x_test = pd.DataFrame(x_test_scale, index=x_test.index, columns=x_test.columns)\n",
    "\n",
    "            # data frame --> xgboost.DMatrix\n",
    "            d_train = xgb.DMatrix(data=x_train, label=y_train.to_numpy())\n",
    "            d_test = xgb.DMatrix(data=x_test, label=y_test.to_numpy())\n",
    "            # print(f'x_test size : {x_test.size}, y_test size : {y_test.size}')\n",
    "\n",
    "            # skip if only one class exists\n",
    "            if len(y_train.unique()) == 1 or len(y_test.unique()) == 1:\n",
    "              print(f'error - participant {test_email} train classes are {y_train.unique()}, test classes are {y_test.unique()}')\n",
    "              continue\n",
    "\n",
    "            # params : https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "            results = {}\n",
    "            booster = xgb.train(\n",
    "                params=dict(eval_metric='auc', booster='gbtree', verbosity=0, objective='binary:logistic'),\n",
    "                dtrain=d_train,\n",
    "                num_boost_round=1000,  # the number of boosted trees\n",
    "                early_stopping_rounds=25,  # early stop generating trees when eval_metric is not improved\n",
    "                evals=[(d_test, 'test')],  # evaluation set to check early stopping\n",
    "                verbose_eval=False,\n",
    "                evals_result=results\n",
    "            )\n",
    "            models += [booster]\n",
    "            x_tests += [x_test]\n",
    "\n",
    "            # [Plotting]\n",
    "            # epochs = len(results['test']['auc'])\n",
    "            # x_axis = range(0, epochs)\n",
    "            # fig, ax = pyplot.subplots()\n",
    "            # ax.plot(x_axis, results['train']['auc'], label='Train')\n",
    "            # ax.plot(x_axis, results['test']['auc'], label='Test')\n",
    "            # ax.legend()\n",
    "            # pyplot.ylabel('AUC')\n",
    "            # pyplot.title('XGBoost training plot')\n",
    "            # pyplot.show()\n",
    "\n",
    "            # predict() returns probability of a positive label (label == 1)\n",
    "            y_pred = booster.predict(data=d_test, ntree_limit=booster.best_ntree_limit)\n",
    "\n",
    "            # [predict() --> probability] cut-off as 0.5: positive label when a probability is higher than 0.5.\n",
    "            \n",
    "            best_threshold, max_f1 = None, 0\n",
    "            for threshold in range(5, 90, 1):\n",
    "              f1 = metrics.f1_score(y_test, np.where(y_pred > float(threshold)/100, 1, 0), average='macro')\n",
    "              if f1 > max_f1:\n",
    "                best_threshold = float(threshold)/100\n",
    "                max_f1 = f1\n",
    "            best_thresholds[test_email] = best_threshold\n",
    "            best_f1s[test_email] = max_f1\n",
    "            #print(y_test.to_numpy(), '\\npred\\n', y_pred)\n",
    "\n",
    "            # region append to total score\n",
    "            y_pred_class = np.where(y_pred > best_threshold, 1, 0)\n",
    "            all_scores['acc'].append(metrics.balanced_accuracy_score(y_test, y_pred_class))\n",
    "            all_scores['f1'].append(metrics.f1_score(y_test, y_pred_class, average='macro'))\n",
    "            all_scores['roc_auc'].append(metrics.roc_auc_score(y_test, y_pred))\n",
    "            all_scores['TPR'].append(metrics.recall_score(y_test, y_pred_class))\n",
    "            all_scores['TNR'].append(metrics.recall_score(y_test, y_pred_class, pos_label=0))\n",
    "            # endregion\n",
    "            # region append to local (round) score\n",
    "            scores_1round['acc'].append(metrics.balanced_accuracy_score(y_test, y_pred_class))\n",
    "            scores_1round['f1'].append(metrics.f1_score(y_test, y_pred_class, average='macro'))\n",
    "            scores_1round['roc_auc'].append(metrics.roc_auc_score(y_test, y_pred))\n",
    "            scores_1round['TPR'].append(metrics.recall_score(y_test, y_pred_class))\n",
    "            scores_1round['TNR'].append(metrics.recall_score(y_test, y_pred_class, pos_label=0))\n",
    "            # endregion\n",
    "            # conf_mtx += metrics.confusion_matrix(y_test, y_pred_class)\n",
    "            # xgb_models.append(booster)\n",
    "\n",
    "            w_granular.write(f'{test_email},{np.mean(scores_1round[\"acc\"])},{np.mean(scores_1round[\"f1\"])},{np.mean(scores_1round[\"roc_auc\"])},{np.mean(scores_1round[\"TPR\"])},{np.mean(scores_1round[\"TNR\"])}\\n')\n",
    "        w.write(f'{np.mean(all_scores[\"acc\"])},{np.mean(all_scores[\"f1\"])},{np.mean(all_scores[\"roc_auc\"])},{np.mean(all_scores[\"TPR\"])},{np.mean(all_scores[\"TNR\"])}\\n')\n",
    "        \n",
    "        keys = list(best_f1s.keys())\n",
    "        keys.sort()\n",
    "        for participant in keys:\n",
    "          print(f'{participant}, {best_f1s[participant]}, {best_thresholds[participant]}')\n",
    "\n",
    "        # plot shap\n",
    "        shap_values = []\n",
    "        test_features = []\n",
    "        for model, x_test in zip(models, x_tests):\n",
    "            explainer = shap.TreeExplainer(booster)\n",
    "            shap_value = explainer.shap_values(x_test)\n",
    "            shap_values.append(shap_value)\n",
    "            test_features.append(x_test)\n",
    "        shap_values = np.vstack(shap_values)\n",
    "        test_features = pd.concat(test_features, axis=0)\n",
    "        fig = plt.figure()\n",
    "        fig.suptitle(f'Labshap for generalized', fontsize=16)\n",
    "        plt.rcParams['axes.facecolor'] = '#f2f8ff'\n",
    "        # plt.xlim(-6, 6)\n",
    "        shap.summary_plot(shap_values, test_features, show=False, plot_type='bar')\n",
    "        fig.savefig(Paths.get(Dirs.RESULTS_ROOT,'fieldshap-generalized.svg'), format='svg')\n",
    "        # plt.show()\n",
    "\n",
    "# # lab stress\n",
    "# train_test_generalized_xgboost(Paths.get(Dirs.FEATURES_LAB_STRESS), Paths.get(Files.RESULTS_G_LAB_STRESS))\n",
    "\n",
    "# field stress\n",
    "train_test_generalized_xgboost(Paths.get(Dirs.FEATURES_FIELD), Paths.get(Files.RESULTS_G_FIELD_STRESS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDkgncv5abRP"
   },
   "source": [
    "## 2.5. Personalized model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbko8DFrafwx"
   },
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5E86ZOoaiU1"
   },
   "source": [
    "## 3.1. Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDXEcG1Rak-6"
   },
   "source": [
    "## 3.2. Data yield"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6PD_QY2a2rK"
   },
   "source": [
    "## 3.3. Model explanation via SHAP"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "SgpFjplrBgsH",
    "HtE4fudnCFxh",
    "otIGghPu4-uw",
    "bnV2IJnsDTXJ",
    "xXjzFgNsDWLH",
    "TbHZCikvDZrb",
    "ql5ugV85fb4s",
    "14aMd1EifgRb",
    "9vqLP9uG2oZ2",
    "G0eYLQzmDfdK",
    "FPomWz3FDj5w",
    "osE8ZzLKZfje"
   ],
   "name": "SOSW_code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
