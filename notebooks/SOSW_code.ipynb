{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wq7IWZ_7BIOM"
   },
   "source": [
    "# 1. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgpFjplrBgsH"
   },
   "source": [
    "## 1.1. Initialize paths\n",
    "    > Mount Google Drive\n",
    "    > Set file paths (e.g. sensing data, EMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4tuknI77_1eA",
    "outputId": "7861d49e-0962-407b-f4ca-59feaa6ea949"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths initialized -- filesystem ready --\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "from os.path import join as j\n",
    "from os.path import sep\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class Dirs:\n",
    "    ROOT = 'root dir'\n",
    "\n",
    "    # raw datasets\n",
    "    RAW_LAB = 'initial lab'\n",
    "    RAW_FIELD = 'initial field'\n",
    "    RAW_FIELD_LEFTOVERS = 'initial field leftovers'\n",
    "\n",
    "    # initial datasets (per participant)\n",
    "    INIT_PARTICIPANTS = 'imported participants data'\n",
    "\n",
    "    # splitting datasets\n",
    "    SPLIT_TREADMILL_DATASET = 'unprocessed treadmill data'\n",
    "    SPLIT_LAB_STRESS_DATASET = 'unprocessed lab stress data'\n",
    "    SPLIT_FIELD_DATASET = 'unprocessed field stress data'\n",
    "\n",
    "    # processed splitted datasets\n",
    "    PROCESSED_TREADMILL_DATASET = 'processed treadmill data'\n",
    "    PROCESSED_LAB_STRESS_DATASET = 'processed lab stress data'\n",
    "    PROCESSED_FIELD_DATASET = 'processed field stress data'\n",
    "\n",
    "    # filtered splitted datasets\n",
    "    FILTERED_LAB_STRESS_DATASET = 'filtered lab stress data'\n",
    "    FILTERED_FIELD_DATASET = 'filtered field stress data'\n",
    "\n",
    "    # normalized splitted dataset\n",
    "    NORMALIZED_LAB_STRESS_DATASET = 'normalized lab stress data'\n",
    "    NORMALIZED_FIELD_DATASET = 'normalized field stress data'\n",
    "\n",
    "    # dataset statistics\n",
    "    STATS_ROOT = 'stats root'\n",
    "    STATS_EMA_GRANULAR = 'granular ema stats'\n",
    "    STATS_LAB_EMA = 'lab stress ema scores'\n",
    "    STATS_LOCATION_ELBOW_CURVES = 'location k-means elbow curves'\n",
    "\n",
    "    # personalized model's features\n",
    "    FEATURES_LAB_STRESS = 'lab stress features'\n",
    "    FEATURES_FIELD = 'field stress features'\n",
    "    FEATURES_FIELD_WITH_CONTEXT = 'field (watch + context) features'\n",
    "    FEATURES_FIELD_WITH_CONTEXT_MOTION_FILTER = 'field (watch + context) features with motion filter'\n",
    "    FEATURES_FIELD_WITH_CONTEXT_LOOSENESS_FILTER = 'field (watch + stress) features with looseness filter'\n",
    "    FEATURES_FIELD_WITH_CONTEXT_COMBINED_FILTER = 'field (watch + stress) features with combined filter'\n",
    "\n",
    "    # results / accuracy\n",
    "    RESULTS_ROOT = 'results'\n",
    "\n",
    "    # thresholds\n",
    "    THRESHOLDS_TREADMILL_OVERVIEW = 'treadmill values overview for thresholds'\n",
    "    THRESHOLDS_MOTION = 'motion threshold results'\n",
    "    THRESHOLDS_LOOSENESS = 'looseness threshold results'\n",
    "\n",
    "    # ppg accuracy i.e. ppg vs. ecg\n",
    "    PPG_ACCURACY_SUMMARY = 'ppg accuracy (ppg vs. ecg)'\n",
    "    PPG_ACCURACY_DETAILED = 'ppg accuracy (ppg vs. ecg) detailed'\n",
    "\n",
    "    # all keys\n",
    "    keys = {\n",
    "        ROOT,\n",
    "        RAW_LAB,\n",
    "        RAW_FIELD,\n",
    "        RAW_FIELD_LEFTOVERS,\n",
    "        INIT_PARTICIPANTS,\n",
    "        SPLIT_TREADMILL_DATASET,\n",
    "        SPLIT_LAB_STRESS_DATASET,\n",
    "        SPLIT_FIELD_DATASET,\n",
    "        PROCESSED_TREADMILL_DATASET,\n",
    "        PROCESSED_LAB_STRESS_DATASET,\n",
    "        PROCESSED_FIELD_DATASET,\n",
    "        FILTERED_LAB_STRESS_DATASET,\n",
    "        FILTERED_FIELD_DATASET,\n",
    "        NORMALIZED_LAB_STRESS_DATASET,\n",
    "        NORMALIZED_FIELD_DATASET,\n",
    "        STATS_ROOT,\n",
    "        STATS_EMA_GRANULAR,\n",
    "        STATS_LAB_EMA,\n",
    "        STATS_LOCATION_ELBOW_CURVES,\n",
    "        FEATURES_LAB_STRESS,\n",
    "        FEATURES_FIELD,\n",
    "        FEATURES_FIELD_WITH_CONTEXT,\n",
    "        FEATURES_FIELD_WITH_CONTEXT_MOTION_FILTER,\n",
    "        FEATURES_FIELD_WITH_CONTEXT_LOOSENESS_FILTER,\n",
    "        FEATURES_FIELD_WITH_CONTEXT_COMBINED_FILTER,\n",
    "        RESULTS_ROOT,\n",
    "        THRESHOLDS_TREADMILL_OVERVIEW,\n",
    "        THRESHOLDS_MOTION,\n",
    "        THRESHOLDS_LOOSENESS,\n",
    "        PPG_ACCURACY_SUMMARY,\n",
    "        PPG_ACCURACY_DETAILED\n",
    "    }\n",
    "\n",
    "\n",
    "class Files:\n",
    "    # configuration files\n",
    "    CONF_PARTICIPANT_IDS = 'participant ids'\n",
    "    CONF_LAB_PROTOCOL = 'lab stress protocol'\n",
    "    CONF_TREADMILL_PROTOCOL = 'treadmill protocol'\n",
    "\n",
    "    # data files\n",
    "    DATA_EMA = 'ema responses'\n",
    "\n",
    "    # personalized model's results\n",
    "    RESULTS_LAB_STRESS = 'lab stress results'\n",
    "    RESULTS_FIELD_STRESS = 'field stress results'\n",
    "    RESULTS_FIELD_WITH_CONTEXT = 'field (watch+context) stress results'\n",
    "    RESULTS_FIELD_WITH_CONTEXT_MOTION_FILTER = 'field (watch+context) stress results with motion filter'\n",
    "    RESULTS_FIELD_WITH_CONTEXT_LOOSENESS_FILTER = 'field (watch+context) stress results with looseness filter'\n",
    "    RESULTS_FIELD_WITH_CONTEXT_COMBINED_FILTER = 'field (watch+context) stress results with combined filter'\n",
    "\n",
    "    # generic model's results\n",
    "    RESULTS_G_LAB_STRESS = 'generalized lab stress results'\n",
    "    RESULTS_G_FIELD_STRESS = 'generalized field stress results'\n",
    "    RESULTS_G_FIELD_WITH_CONTEXT = 'generalized field (watch+context) stress results'\n",
    "    RESULTS_G_FIELD_WITH_CONTEXT_MOTION_FILTER = 'generalized field (watch+context) stress results with motion filter'\n",
    "    RESULTS_G_FIELD_WITH_CONTEXT_LOOSENESS_FILTER = 'generalized field (watch+context) stress results with looseness filter'\n",
    "    RESULTS_G_FIELD_WITH_CONTEXT_COMBINED_FILTER = 'generalized field (watch+context) stress results with combined filter'\n",
    "\n",
    "    # all keys\n",
    "    keys = {\n",
    "        CONF_PARTICIPANT_IDS,\n",
    "        CONF_LAB_PROTOCOL,\n",
    "        CONF_TREADMILL_PROTOCOL,\n",
    "        DATA_EMA,\n",
    "        RESULTS_LAB_STRESS,\n",
    "        RESULTS_FIELD_STRESS,\n",
    "        RESULTS_FIELD_WITH_CONTEXT,\n",
    "        RESULTS_FIELD_WITH_CONTEXT_MOTION_FILTER,\n",
    "        RESULTS_FIELD_WITH_CONTEXT_LOOSENESS_FILTER,\n",
    "        RESULTS_FIELD_WITH_CONTEXT_COMBINED_FILTER,\n",
    "        RESULTS_G_LAB_STRESS,\n",
    "        RESULTS_G_FIELD_STRESS,\n",
    "        RESULTS_G_FIELD_WITH_CONTEXT,\n",
    "        RESULTS_G_FIELD_WITH_CONTEXT_MOTION_FILTER,\n",
    "        RESULTS_G_FIELD_WITH_CONTEXT_LOOSENESS_FILTER,\n",
    "        RESULTS_G_FIELD_WITH_CONTEXT_COMBINED_FILTER,\n",
    "    }\n",
    "\n",
    "\n",
    "class Paths:\n",
    "    _root = \"\"\n",
    "    _dirs = {\n",
    "        Dirs.ROOT: _root,\n",
    "\n",
    "        # initial dataset\n",
    "        Dirs.RAW_LAB: j(_root, '0. raw dataset', 'lab_study_data'),\n",
    "        Dirs.RAW_FIELD: j(_root, '0. raw dataset', 'field_sensing'),\n",
    "        Dirs.RAW_FIELD_LEFTOVERS: j(_root, '0. raw dataset', 'field_sensing_leftovers'),\n",
    "\n",
    "        # merging datasets\n",
    "        Dirs.INIT_PARTICIPANTS: j(_root, '1. participant datasets'),\n",
    "\n",
    "        # splitting datasets\n",
    "        Dirs.SPLIT_TREADMILL_DATASET: j(_root, '2. three studies datasets', '1. ppg accuracy'),\n",
    "        Dirs.SPLIT_LAB_STRESS_DATASET: j(_root, '2. three studies datasets', '2. lab stress'),\n",
    "        Dirs.SPLIT_FIELD_DATASET: j(_root, '2. three studies datasets', '3. field stress'),\n",
    "\n",
    "        # processed splitted datasets\n",
    "        Dirs.PROCESSED_TREADMILL_DATASET: j(_root, '2.1. three studies datasets (processed)', '1. ppg accuracy'),\n",
    "        Dirs.PROCESSED_LAB_STRESS_DATASET: j(_root, '2.1. three studies datasets (processed)', '2. lab stress'),\n",
    "        Dirs.PROCESSED_FIELD_DATASET: j(_root, '2.1. three studies datasets (processed)', '3. field stress'),\n",
    "\n",
    "        # filtered splitted datasets\n",
    "        Dirs.FILTERED_LAB_STRESS_DATASET: j(_root, '2.2. three studies datasets (filtered)', '2. lab stress'),\n",
    "        Dirs.FILTERED_FIELD_DATASET: j(_root, '2.2. three studies datasets (filtered)', '3. field stress'),\n",
    "\n",
    "        # normalized splitted datasets\n",
    "        Dirs.NORMALIZED_LAB_STRESS_DATASET: j(_root, '2.3. three studies datasets (normalized)', '2. lab stress'),\n",
    "        Dirs.NORMALIZED_FIELD_DATASET: j(_root, '2.3. three studies datasets (normalized)', '3. field stress'),\n",
    "\n",
    "        # dataset statistics\n",
    "        Dirs.STATS_ROOT: j(_root, '3. dataset stats'),\n",
    "        Dirs.STATS_EMA_GRANULAR: j(_root, '3. dataset stats', 'granular_ema_stats'),\n",
    "        Dirs.STATS_LAB_EMA: j(_root, '3. dataset stats', 'lab_stress_ema_scores'),\n",
    "        Dirs.STATS_LOCATION_ELBOW_CURVES: j(_root, '3. dataset stats', 'location_elbow_curves'),\n",
    "\n",
    "        # features\n",
    "        Dirs.FEATURES_LAB_STRESS: j(_root, '4. stress features', '1. lab stress'),\n",
    "        Dirs.FEATURES_FIELD: j(_root, '4. stress features', '2. field stress'),\n",
    "        Dirs.FEATURES_FIELD_WITH_CONTEXT: j(_root, '4. stress features', '3. field stress + context'),\n",
    "        Dirs.FEATURES_FIELD_WITH_CONTEXT_MOTION_FILTER: j(_root, '4. stress features', '4. field stress + context + motion filter'),\n",
    "        Dirs.FEATURES_FIELD_WITH_CONTEXT_LOOSENESS_FILTER: j(_root, '4. stress features', '5. field stress + context + looseness filter'),\n",
    "        Dirs.FEATURES_FIELD_WITH_CONTEXT_COMBINED_FILTER: j(_root, '4. stress features', '6. field stress + context + combined filter'),\n",
    "\n",
    "        # results / accuracy\n",
    "        Dirs.RESULTS_ROOT: j(_root, '5. result'),\n",
    "\n",
    "        # thresholds\n",
    "        Dirs.THRESHOLDS_TREADMILL_OVERVIEW: j(_root, '6. threshold gridsearch results', '1. treadmill values overview'),\n",
    "        Dirs.THRESHOLDS_MOTION: j(_root, '6. threshold gridsearch results', '2. motion threshold results'),\n",
    "        Dirs.THRESHOLDS_LOOSENESS: j(_root, '6. threshold gridsearch results', '3. looseness threshold results'),\n",
    "\n",
    "        # ppg accuracy i.e. ppg vs. ecg\n",
    "        Dirs.PPG_ACCURACY_SUMMARY: j(_root, '7. ppg accuracy', '1. summary of differences'),\n",
    "        Dirs.PPG_ACCURACY_DETAILED: j(_root, '7. ppg accuracy', '2. detailed differences'),\n",
    "    }\n",
    "    _files = {\n",
    "        # configuration files\n",
    "        Files.CONF_PARTICIPANT_IDS: j(_root, '0. raw dataset', 'participant_map.csv'),\n",
    "        Files.CONF_LAB_PROTOCOL: j(_root, '0. raw dataset', 'lab_stress_protocol.csv'),\n",
    "        Files.CONF_TREADMILL_PROTOCOL: j(_root, '0. raw dataset', 'ppg_accuracy_protocol.csv'),\n",
    "\n",
    "        # data files\n",
    "        Files.DATA_EMA: j(_root, '0. raw dataset', 'ema_responses.csv'),\n",
    "\n",
    "        # personalized model's results / accuracy\n",
    "        Files.RESULTS_LAB_STRESS: j(_root, '5. result', 'lab.csv'),\n",
    "        Files.RESULTS_FIELD_STRESS: j(_root, '5. result', 'field.csv'),\n",
    "        Files.RESULTS_FIELD_WITH_CONTEXT: j(_root, '5. result', 'field_context.csv'),\n",
    "        Files.RESULTS_FIELD_WITH_CONTEXT_MOTION_FILTER: j(_root, '5. result', 'field_context_motion_filter.csv'),\n",
    "        Files.RESULTS_FIELD_WITH_CONTEXT_LOOSENESS_FILTER: j(_root, '5. result', 'field_context_looseness_filter.csv'),\n",
    "        Files.RESULTS_FIELD_WITH_CONTEXT_COMBINED_FILTER: j(_root, '5. result', 'field_context_combined_filter.csv'),\n",
    "\n",
    "        Files.RESULTS_G_LAB_STRESS: j(_root, '5. result', 'lab_generalized.csv'),\n",
    "        Files.RESULTS_G_FIELD_STRESS: j(_root, '5. result', 'field_generalized.csv'),\n",
    "        Files.RESULTS_G_FIELD_WITH_CONTEXT: j(_root, '5. result', 'field_generalized_context.csv'),\n",
    "        Files.RESULTS_G_FIELD_WITH_CONTEXT_MOTION_FILTER: j(_root, '5. result', 'field_generalized_context_motion_filter.csv'),\n",
    "        Files.RESULTS_G_FIELD_WITH_CONTEXT_LOOSENESS_FILTER: j(_root, '5. result', 'field_generalized_context_looseness_filter.csv'),\n",
    "        Files.RESULTS_G_FIELD_WITH_CONTEXT_COMBINED_FILTER: j(_root, '5. result', 'field_generalized_context_combined_filter.csv'),\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def get(key, *attach: str):\n",
    "        if key in Dirs.keys:\n",
    "            if attach:\n",
    "                return j(Paths._dirs[key], f'{sep}'.join(attach))\n",
    "            else:\n",
    "                return Paths._dirs[key]\n",
    "        elif key in Files.keys:\n",
    "            return Paths._files[key]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "print('Paths initialized -- filesystem ready --')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtE4fudnCFxh"
   },
   "source": [
    "## 1.3. Initialize study configs\n",
    "    > Feature selection\n",
    "    > Sliding window size\n",
    "    > Sliding window overlap\n",
    "    > M_ROUNDS for building personalized models\n",
    "    > EMA span for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ypse15ywCK3Q",
    "outputId": "91eeaf0a-0b0d-41f2-96c7-c2e3a45cdab7"
   },
   "outputs": [],
   "source": [
    "#!pip install hrv-analysis\n",
    "#!pip install shap==0.36.0\n",
    "\n",
    "\n",
    "class Labels:\n",
    "    LABEL_STRESSED = 'stressed'\n",
    "    LABEL_NOT_STRESSED = 'not-stressed'\n",
    "    STR_LABELS = [LABEL_STRESSED, LABEL_NOT_STRESSED]\n",
    "\n",
    "    LABEL_STRESSED_INT = 1\n",
    "    LABEL_NOT_STRESSED_INT = 0\n",
    "    INT_LABELS = [LABEL_STRESSED_INT, LABEL_NOT_STRESSED_INT]\n",
    "\n",
    "\n",
    "class Features:\n",
    "    CONTEXT_ONLY = ['activity', 'location', 'is_weekday']\n",
    "    CATEGORICAL = ['activity', 'location']\n",
    "\n",
    "    WATCH_BASIC = ['mean_nni', 'sdnn', 'rmssd', 'nni_50', 'mean_hr', 'std_hr', 'lf', 'hf', 'lf_hf_ratio', 'sd1', 'sd2', 'sampen']\n",
    "    WATCH_ALL = ['mean_nni', 'sdnn', 'rmssd', 'nni_50', 'mean_hr', 'std_hr', 'sdsd', 'median_nni', 'pnni_50', 'nni_20', 'pnni_20', 'range_nni', 'cvsd', 'cvnni', 'max_hr', 'min_hr', 'lf', 'hf', 'total_power', 'vlf', 'csi', 'cvi', 'modified_csi', 'sd1', 'sd2', 'ratio_sd2_sd1', 'sampen']\n",
    "    # WATCH_ALL = ['hr_mean', 'hr_median', 'hr_max', 'hr_min', 'hr_std', 'hr_kurtosis', 'hr_skew', 'hr_80', 'hr_20', 'rr_mean', 'rr_median', 'rr_max', 'rr_min', 'rr_std', 'rr_kurtosis', 'rr_skew', 'rr_80', 'rr_20', 'rr_rmssd']\n",
    "    WATCH_AND_CONTEXT = ['mean_nni', 'sdnn', 'rmssd', 'nni_50', 'mean_hr', 'std_hr', 'lf', 'hf', 'lf_hf_ratio', 'sd1', 'sd2', 'sampen', 'activity', 'location', 'hour', 'is_weekday']\n",
    "    WATCH_AND_CONTEXT_NO_LOCATION = ['mean_nni', 'sdnn', 'rmssd', 'nni_50', 'mean_hr', 'std_hr', 'lf', 'hf', 'lf_hf_ratio', 'sd1', 'sd2', 'sampen', 'activity', 'hour', 'is_weekday']\n",
    "\n",
    "    ALL = ['mean_nni', 'sdnn', 'rmssd', 'nni_50', 'mean_hr', 'std_hr', 'sdsd', 'median_nni', 'pnni_50', 'nni_20', 'pnni_20', 'range_nni', 'cvsd', 'cvnni', 'max_hr', 'min_hr', 'lf', 'hf', 'lf_hf_ratio', 'total_power', 'vlf', 'lfnu', 'hfnu', 'csi', 'cvi', 'modified_csi', 'triangular_index', 'sd1', 'sd2', 'ratio_sd2_sd1', 'sampen', 'activity', 'location', 'hour', 'is_weekday']\n",
    "\n",
    "\n",
    "class Configs:\n",
    "    # region previous thresholds\n",
    "    # MOTION_THRESHOLDS_RANGE = np.array(range(20, 110, 5)) / 10\n",
    "    # LOOSENESS_THRESHOLDS_RANGE = np.array(range(187, 348, 10)) * 1000\n",
    "    # MOTION_THRESHOLDS = {\n",
    "    #     '12170603@inha.edu': 10.5,\n",
    "    #     '12181157@inha.edu': 7,\n",
    "    #     '12201686@inha.edu': 9.5,\n",
    "    #     '12212982so@inha.edu': 10,\n",
    "    #     'as7177as3912@gmail.com': 8.5,\n",
    "    #     'bagle1029@gmail.com': 9.5,\n",
    "    #     'bw04029@gmail.com': 6,\n",
    "    #     'chaewoni65@gmail.com': 2,\n",
    "    #     'csissis1997@gmail.com': 3,\n",
    "    #     'david000914@gmail.com': 5,\n",
    "    #     'duecjf011521212106@gmail.com': 2.5,\n",
    "    #     'eowjdtjdwls@gmail.com': 3.5,\n",
    "    #     'fpdlsqhdn962@inha.edu': 2,\n",
    "    #     'gh011127@gmail.com': 9,\n",
    "    #     'gurwns7772@gmail.com': 2,\n",
    "    #     'hhcc05@inha.edu': 3,\n",
    "    #     'jinsoonshim@naver.com': 7,\n",
    "    #     'jwshoho4@gmail.com': 8.5,\n",
    "    #     'jyypaul@gmail.com': 5.5,\n",
    "    #     'km55181499@gmail.com': 7,\n",
    "    #     'kpm1323@gmail.com': 2.5,\n",
    "    #     'memm1439@gmail.com': 4.5,\n",
    "    #     'minjae20000207@gmail.com': 6.5,\n",
    "    #     'minrudcho01@gmail.com': 4,\n",
    "    #     'nigaram8@gmail.com': 7,\n",
    "    #     'nnozilaxonim@gmail.com': 10.5,\n",
    "    #     'powerampshere@gmail.com': 3,\n",
    "    #     'rlarkgus11170@inha.edu': 7.5,\n",
    "    #     'unicpn21@gmail.com': 3.5,\n",
    "    #     'vusgowlwk17@gmail.com': 7,\n",
    "    #     'wjdwogus0604@gmail.com': 5.5,\n",
    "    #     'wonjs0725@gmail.com': 5,\n",
    "    #     'zkapdh123@gmail.com': 2.5\n",
    "    # }\n",
    "    # LOOSENESS_THRESHOLDS = {\n",
    "    #     '12170603@inha.edu': 237000,\n",
    "    #     '12181157@inha.edu': 297000,\n",
    "    #     '12212982so@inha.edu': 327000,\n",
    "    #     'as7177as3912@gmail.com': 277000,\n",
    "    #     'bagle1029@gmail.com': 267000,\n",
    "    #     'bw04029@gmail.com': 187000,\n",
    "    #     'chaewoni65@gmail.com': 327000,\n",
    "    #     'csissis1997@gmail.com': 247000,\n",
    "    #     'david000914@gmail.com': 317000,\n",
    "    #     'duecjf011521212106@gmail.com': 247000,\n",
    "    #     'eowjdtjdwls@gmail.com': 237000,\n",
    "    #     'fpdlsqhdn962@inha.edu': 187000,\n",
    "    #     'gh011127@gmail.com': 217000,\n",
    "    #     'gurwns7772@gmail.com': 277000,\n",
    "    #     'hhcc05@inha.edu': 237000,\n",
    "    #     'jinsoonshim@naver.com': 297000,\n",
    "    #     'jwshoho4@gmail.com': 347000,\n",
    "    #     'jyypaul@gmail.com': 197000,\n",
    "    #     'km55181499@gmail.com': 287000,\n",
    "    #     'kpm1323@gmail.com': 247000,\n",
    "    #     'memm1439@gmail.com': 317000,\n",
    "    #     'minjae20000207@gmail.com': 277000,\n",
    "    #     'minrudcho01@gmail.com': 247000,\n",
    "    #     'nigaram8@gmail.com': 347000,\n",
    "    #     'nnozilaxonim@gmail.com': 307000,\n",
    "    #     'powerampshere@gmail.com': 267000,\n",
    "    #     'rlarkgus11170@inha.edu': 237000,\n",
    "    #     'unicpn21@gmail.com': 267000,\n",
    "    #     'vusgowlwk17@gmail.com': 227000,\n",
    "    #     'wjdwogus0604@gmail.com': 227000,\n",
    "    #     'wonjs0725@gmail.com': 267000,\n",
    "    #     'zkapdh123@gmail.com': 287000\n",
    "    # }\n",
    "    # endregion\n",
    "\n",
    "    MOTION_THRESHOLDS_RANGE = np.array(range(20, 60, 2)) / 10\n",
    "    LOOSENESS_THRESHOLDS_RANGE = np.array(range(180, 300, 6)) * 1000\n",
    "    MOTION_THRESHOLDS = {\n",
    "        '12170603@inha.edu': 3.2,\n",
    "        '12181157@inha.edu': 4.6,\n",
    "        '12201686@inha.edu': 2.8,\n",
    "        '12212982so@inha.edu': 4.2,\n",
    "        'as7177as3912@gmail.com': 4.4,\n",
    "        'bagle1029@gmail.com': 4.8,\n",
    "        'bw04029@gmail.com': 3.6,\n",
    "        'chaewoni65@gmail.com': 2.2,\n",
    "        'csissis1997@gmail.com': 2.6,\n",
    "        'david000914@gmail.com': 3.2,\n",
    "        'duecjf011521212106@gmail.com': 5.2,\n",
    "        'eowjdtjdwls@gmail.com': 3.6,\n",
    "        'fpdlsqhdn962@inha.edu': 5.8,\n",
    "        'gh011127@gmail.com': 5.8,\n",
    "        'gurwns7772@gmail.com': 5.0,\n",
    "        'hhcc05@inha.edu': 2.0,\n",
    "        'jinsoonshim@naver.com': 3.6,\n",
    "        'jwshoho4@gmail.com': 2.0,\n",
    "        'jyypaul@gmail.com': 4.8,\n",
    "        'km55181499@gmail.com': 3.0,\n",
    "        'kpm1323@gmail.com': 2.2,\n",
    "        'memm1439@gmail.com': 2.8,\n",
    "        'minjae20000207@gmail.com': 2.8,\n",
    "        'minrudcho01@gmail.com': 4.8,\n",
    "        'nigaram8@gmail.com': 5.2,\n",
    "        'nnozilaxonim@gmail.com': 2.0,\n",
    "        'powerampshere@gmail.com': 4.0,\n",
    "        'rlarkgus11170@inha.edu': 5.8,\n",
    "        'unicpn21@gmail.com': 2.0,\n",
    "        'vusgowlwk17@gmail.com': 3.2,\n",
    "        'wjdwogus0604@gmail.com': 2.4,\n",
    "        'wonjs0725@gmail.com': 5.2,\n",
    "        'zkapdh123@gmail.com': 3.4\n",
    "    }\n",
    "    LOOSENESS_THRESHOLDS = {\n",
    "        '12170603@inha.edu': 204000,\n",
    "        '12181157@inha.edu': 258000,\n",
    "        '12212982so@inha.edu': 180000,\n",
    "        'as7177as3912@gmail.com': 288000,\n",
    "        'bagle1029@gmail.com': 198000,\n",
    "        'bw04029@gmail.com': 216000,\n",
    "        'chaewoni65@gmail.com': 204000,\n",
    "        'csissis1997@gmail.com': 276000,\n",
    "        'david000914@gmail.com': 180000,\n",
    "        'duecjf011521212106@gmail.com': 198000,\n",
    "        'eowjdtjdwls@gmail.com': 288000,\n",
    "        'fpdlsqhdn962@inha.edu': 234000,\n",
    "        'gh011127@gmail.com': 246000,\n",
    "        'gurwns7772@gmail.com': 210000,\n",
    "        'hhcc05@inha.edu': 222000,\n",
    "        'jinsoonshim@naver.com': 180000,\n",
    "        'jwshoho4@gmail.com': 222000,\n",
    "        'jyypaul@gmail.com': 294000,\n",
    "        'km55181499@gmail.com': 276000,\n",
    "        'kpm1323@gmail.com': 210000,\n",
    "        'memm1439@gmail.com': 264000,\n",
    "        'minjae20000207@gmail.com': 276000,\n",
    "        'minrudcho01@gmail.com': 282000,\n",
    "        'nigaram8@gmail.com': 210000,\n",
    "        'nnozilaxonim@gmail.com': 222000,\n",
    "        'powerampshere@gmail.com': 282000,\n",
    "        'rlarkgus11170@inha.edu': 288000,\n",
    "        'unicpn21@gmail.com': 210000,\n",
    "        'vusgowlwk17@gmail.com': 216000,\n",
    "        'wjdwogus0604@gmail.com': 252000,\n",
    "        'wonjs0725@gmail.com': 222000,\n",
    "        'zkapdh123@gmail.com': 228000\n",
    "    }\n",
    "\n",
    "    M_ROUNDS = 30\n",
    "\n",
    "    # time unit: milliseconds\n",
    "    WINDOW_SIZE = 60 * 1000\n",
    "    WINDOW_OVERLAP = 0.75\n",
    "    MIN_WINDOW_SAMPLES = 60\n",
    "    EMA_SPAN = 30 * 60 * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caT-7lUPC1av"
   },
   "source": [
    "# 2. Data processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GAinuuHDDh4"
   },
   "source": [
    "## 2.1. Clean HR signals\n",
    "    > Motion and looseness filters\n",
    "    > Obvious errors by PPG sensor\n",
    "    > Outlier HR readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otIGghPu4-uw"
   },
   "source": [
    "### 2.1.1 Copying untouched data (done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "pj1T00ak5C9m",
    "outputId": "69174869-53ae-4ec1-9ac8-aad7fa9cde81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab stress\n",
      "\t1. baseline rest\n",
      "\t2. speech instructions\n",
      "\t3. speech preparation\n",
      "\t4. speech stressor\n",
      "\t5. rest\n",
      "\t6. arithmetic instructions\n",
      "\t7. arithmetic stressor p1\n",
      "\t8. arithmetic rest\n",
      "\t9. arithmetic stressor p2\n",
      "\t10. rest\n",
      "\t11. cold instructions\n",
      "\t12. cold stressor\n",
      "\t13. recovery\n",
      "ppg accuracy\n",
      "\t1. static tight\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-cbdb7bcd4564>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0msrc_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_dataset_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscenario\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparticipant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{filename}.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mdst_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_dataset_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscenario\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparticipant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{filename.replace(\"ecg\", \"ppg\")}.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mcopy_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-cbdb7bcd4564>\u001b[0m in \u001b[0;36mcopy_file\u001b[0;34m(src_path, dst_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mcp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                 \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;34m\"\"\"copy data from file-like object fsrc to file-like object fdst\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from shutil import copyfile as cp\n",
    "from os import listdir as ld\n",
    "import os\n",
    "\n",
    "\n",
    "# normal copy with exists() check\n",
    "def copy_file(src_path, dst_path):\n",
    "  if os.path.exists(src_path):\n",
    "    if os.path.exists(dst_path):\n",
    "      os.remove(dst_path)\n",
    "    cp(src_path, dst_path)\n",
    "\n",
    "\n",
    "'''\n",
    "(STEP 1)\n",
    "  Copy Lab-Stress and PPG-Accuracy datasets\n",
    "  parent_dir / scenario / participant / data\n",
    "'''\n",
    "datasets = [\n",
    "  ('lab stress', Dirs.SPLIT_LAB_STRESS_DATASET, Dirs.PROCESSED_LAB_STRESS_DATASET),\n",
    "  ('ppg accuracy', Dirs.SPLIT_TREADMILL_DATASET, Dirs.PROCESSED_TREADMILL_DATASET)\n",
    "]\n",
    "for title, src_dataset_key, dst_dataset_key in datasets:\n",
    "  print(title)\n",
    "  scenarios = ld(Paths.get(src_dataset_key))\n",
    "  scenarios.sort(key=lambda x: int(x[:x.index('.')]))\n",
    "  for scenario in scenarios:\n",
    "    print(f'\\t{scenario}')\n",
    "    # path check\n",
    "    if not os.path.exists(Paths.get(dst_dataset_key, scenario)):\n",
    "      os.mkdir(Paths.get(dst_dataset_key, scenario))\n",
    "    \n",
    "    for participant in ld(Paths.get(src_dataset_key, scenario)):\n",
    "      # path check\n",
    "      if not os.path.exists(Paths.get(dst_dataset_key, scenario, participant)):\n",
    "          os.mkdir(Paths.get(dst_dataset_key, scenario, participant))\n",
    "\n",
    "      # # copy other files (i.e. ecg_hr, ecg_rr, ppg_li)\n",
    "      # for filename in ['acc', 'ppg_hr', 'ppg_rr', 'ecg_hr', 'ecg_rr', 'ppg_li']:\n",
    "      #   src_path = Paths.get(src_dataset_key, scenario, participant, f'{filename}.csv')\n",
    "      #   dst_path = Paths.get(dst_dataset_key, scenario, participant, f'{filename}.csv')\n",
    "      #   copy_file(src_path, dst_path)\n",
    "\n",
    "      # remove this after ecg check\n",
    "      for filename in ['ecg_hr', 'ecg_rr']:\n",
    "        src_path = Paths.get(src_dataset_key, scenario, participant, f'{filename}.csv')\n",
    "        dst_path = Paths.get(dst_dataset_key, scenario, participant, f'{filename.replace(\"ecg\", \"ppg\")}.csv')\n",
    "        copy_file(src_path, dst_path)\n",
    "\n",
    "\n",
    "# '''\n",
    "# (STEP 2)\n",
    "#   Copy field stress dataset\n",
    "#   parent_dir / participant / data\n",
    "# '''\n",
    "# print('field stress')\n",
    "# participants = ld(Paths.get(Dirs.SPLIT_FIELD_DATASET))\n",
    "# for idx, participant in enumerate(participants):\n",
    "#   print(f'\\t {idx+1}/{len(participants)} {participant}')\n",
    "\n",
    "#   # path check\n",
    "#   if not os.path.exists(Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant)):\n",
    "#       os.mkdir(Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant))\n",
    "\n",
    "#   # copy other files (i.e. ecg_hr, ecg_rr, ppg_li)\n",
    "#   for filename in ['ppg_hr', 'ppg_rr']: #, 'acc', 'ppg_li']:\n",
    "#     src_path = Paths.get(Dirs.SPLIT_FIELD_DATASET, participant, f'{filename}.csv')\n",
    "#     dst_path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, f'{filename}.csv')\n",
    "#     copy_file(src_path, dst_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnV2IJnsDTXJ"
   },
   "source": [
    "### 2.1.1. Human HR range (counter done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lnb4xKTCGPfK",
    "outputId": "c05e6131-178e-485c-aaba-1f7570a91388"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "field stress\n",
      "\t 1/35 jinsoonshim@naver.com (0 hrs removed, 0 rrs removed)\n",
      "\t 2/35 rlarkgus11170@inha.edu (0 hrs removed, 0 rrs removed)\n",
      "\t 3/35 kpm1323@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 4/35 eowjdtjdwls@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 5/35 chaewoni65@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 6/35 minrudcho01@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 7/35 csissis1997@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 8/35 km55181499@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 9/35 fpdlsqhdn962@inha.edu (0 hrs removed, 0 rrs removed)\n",
      "\t 10/35 bw04029@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 11/35 12201686@inha.edu (0 hrs removed, 0 rrs removed)\n",
      "\t 12/35 vusgowlwk17@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 13/35 duecjf011521212106@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 14/35 jwshoho4@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 15/35 nnozilaxonim@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 16/35 minjae20000207@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 17/35 12181157@inha.edu (0 hrs removed, 0 rrs removed)\n",
      "\t 18/35 rbfl675@inha.edu (0 hrs removed, 0 rrs removed)\n",
      "\t 19/35 nigaram8@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 20/35 as7177as3912@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 21/35 12212982so@inha.edu (0 hrs removed, 0 rrs removed)\n",
      "\t 22/35 wjdwogus0604@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 23/35 gurwns7772@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 24/35 jyypaul@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 25/35 unicpn21@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 26/35 powerampshere@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 27/35 ysl@inha.edu (0 hrs removed, 0 rrs removed)\n",
      "\t 28/35 wonjs0725@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 29/35 zkapdh123@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 30/35 gh011127@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 31/35 hhcc05@inha.edu (0 hrs removed, 0 rrs removed)\n",
      "\t 32/35 bagle1029@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 33/35 12170603@inha.edu (0 hrs removed, 0 rrs removed)\n",
      "\t 34/35 memm1439@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "\t 35/35 david000914@gmail.com (0 hrs removed, 0 rrs removed)\n",
      "# of dropped lines (ppg_hr, ppg_rr): 0 0\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile as cp\n",
    "from os import listdir as ld\n",
    "import os\n",
    "\n",
    "\n",
    "# checks HR via range\n",
    "def process_hr_file(path):\n",
    "  if os.path.exists(path):\n",
    "    with open(path, 'r') as r:\n",
    "      lines = r.readlines()\n",
    "      l_ = len(lines)\n",
    "    with open(path, 'w+') as w:\n",
    "      for line in lines:\n",
    "        ts, hr = line[:-1].split(',')\n",
    "        try:\n",
    "          if 30 <= int(hr) <= 220:\n",
    "            w.write(line)\n",
    "            l_-=1\n",
    "        except ValueError:\n",
    "          pass\n",
    "    return l_ #the number of dropped hr lines\n",
    "  else:\n",
    "    return 0                       \n",
    "\n",
    "\n",
    "# checks HR-interval via range\n",
    "def process_rr_file(path):\n",
    "  if os.path.exists(path):\n",
    "    with open(path, 'r') as r:\n",
    "      lines = r.readlines()\n",
    "      l_ = len(lines)\n",
    "    with open(path, 'w+') as w:\n",
    "      for line in lines:\n",
    "        ts, rri = line[:-1].split(',')\n",
    "        try:\n",
    "          if 272 <= int(rri) <= 2000:\n",
    "            w.write(line)\n",
    "            l_-=1\n",
    "        except ValueError:\n",
    "          pass\n",
    "    return l_ #the number of dropped rr lines\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "\n",
    "# '''\n",
    "# (STEP 1)\n",
    "#   Cleaning Lab-Stress and PPG-Accuracy datasets\n",
    "#   parent_dir / scenario / participant / data\n",
    "# '''\n",
    "# datasets = [\n",
    "#   ('lab stress', Dirs.PROCESSED_LAB_STRESS_DATASET),\n",
    "#   #('ppg accuracy', Dirs.PROCESSED_TREADMILL_DATASET)\n",
    "# ]\n",
    "# for title, dataset_key in datasets:\n",
    "#   t_n_ppg_hr = t_n_ecg_hr = t_n_ppg_rr = t_n_ecg_rr = 0\n",
    "#   print(\"\\n\",title)\n",
    "#   scenarios = ld(Paths.get(dataset_key))\n",
    "#   scenarios.sort(key=lambda x: int(x[:x.index('.')]))\n",
    "#   for scenario in scenarios:\n",
    "#     print(f'\\t{scenario}')\n",
    "#     for participant in ld(Paths.get(dataset_key, scenario)):\n",
    "#       # (a) fix heart rate (bpm)\n",
    "#       path = Paths.get(dataset_key, scenario, participant, 'ppg_hr.csv')\n",
    "#       n_ppg_hr = process_hr_file(path)\n",
    "#       t_n_ppg_hr += n_ppg_hr\n",
    "#       path = Paths.get(dataset_key, scenario, participant, 'ecg_hr.csv')\n",
    "#       n_ecg_hr = process_hr_file(path)\n",
    "#       t_n_ecg_hr += n_ecg_hr\n",
    "\n",
    "#       # (b) fix interbeat-intervals (ms)\n",
    "#       path = Paths.get(dataset_key, scenario, participant, 'ppg_rr.csv')\n",
    "#       n_ppg_rr = process_rr_file(path)\n",
    "#       t_n_ppg_rr += n_ppg_rr\n",
    "#       path = Paths.get(dataset_key, scenario, participant, 'ecg_rr.csv')\n",
    "#       n_ecg_rr = process_rr_file(path)\n",
    "#       t_n_ecg_rr += n_ecg_rr\n",
    "#   print(\"# of dropped lines (ppg_hr, ecg_hr, ppg_rr, ecg_rr)\", t_n_ppg_hr, t_n_ecg_hr, t_n_ppg_rr, t_n_ecg_rr)\n",
    "\n",
    "'''\n",
    "(STEP 2)\n",
    "  Cleaning field stress dataset\n",
    "  parent_dir / participant / data\n",
    "'''\n",
    "print('\\nfield stress')\n",
    "t_n_ppg_hr = t_n_ppg_rr = 0\n",
    "participants = ld(Paths.get(Dirs.PROCESSED_FIELD_DATASET))\n",
    "for idx, participant in enumerate(participants):\n",
    "  print(f'\\t {idx+1}/{len(participants)} {participant}', end = ' ')\n",
    "\n",
    "  # (a) fix heart rate (bpm)\n",
    "  path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, 'ppg_hr.csv')\n",
    "  n_ppg_hr = process_hr_file(path)\n",
    "  print(f'({n_ppg_hr} hrs removed, ', end='')\n",
    "  t_n_ppg_hr += n_ppg_hr\n",
    "\n",
    "  # (b) fix interbeat-intervals (ms)\n",
    "  path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, 'ppg_rr.csv')\n",
    "  n_ppg_rr = process_rr_file(path)\n",
    "  print(f'{n_ppg_rr} rrs removed)')\n",
    "  t_n_ppg_rr += n_ppg_rr\n",
    "print(\"# of dropped lines (ppg_hr, ppg_rr):\", t_n_ppg_hr, t_n_ppg_rr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXjzFgNsDWLH"
   },
   "source": [
    "### 2.1.2. Outlier trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C6ajK6c7lRzz",
    "outputId": "7960f3c2-b09b-4d8f-c058-96f4878fb96d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "field stress\n",
      "\t 1/35 jinsoonshim@naver.com\n",
      "\t 2/35 rlarkgus11170@inha.edu\n",
      "\t 3/35 kpm1323@gmail.com\n",
      "\t 4/35 eowjdtjdwls@gmail.com\n",
      "\t 5/35 chaewoni65@gmail.com\n",
      "\t 6/35 minrudcho01@gmail.com\n",
      "\t 7/35 csissis1997@gmail.com\n",
      "\t 8/35 km55181499@gmail.com\n",
      "\t 9/35 fpdlsqhdn962@inha.edu\n",
      "\t 10/35 bw04029@gmail.com\n",
      "\t 11/35 12201686@inha.edu\n",
      "\t 12/35 vusgowlwk17@gmail.com\n",
      "\t 13/35 duecjf011521212106@gmail.com\n",
      "\t 14/35 jwshoho4@gmail.com\n",
      "\t 15/35 nnozilaxonim@gmail.com\n",
      "\t 16/35 minjae20000207@gmail.com\n",
      "\t 17/35 12181157@inha.edu\n",
      "\t 18/35 rbfl675@inha.edu\n",
      "\t 19/35 nigaram8@gmail.com\n",
      "\t 20/35 as7177as3912@gmail.com\n",
      "\t 21/35 12212982so@inha.edu\n",
      "\t 22/35 wjdwogus0604@gmail.com\n",
      "\t 23/35 gurwns7772@gmail.com\n",
      "\t 24/35 jyypaul@gmail.com\n",
      "\t 25/35 unicpn21@gmail.com\n",
      "\t 26/35 powerampshere@gmail.com\n",
      "\t 27/35 ysl@inha.edu\n",
      "\t 28/35 wonjs0725@gmail.com\n",
      "\t 29/35 zkapdh123@gmail.com\n",
      "\t 30/35 gh011127@gmail.com\n",
      "\t 31/35 hhcc05@inha.edu\n",
      "\t 32/35 bagle1029@gmail.com\n",
      "\t 33/35 12170603@inha.edu\n",
      "\t 34/35 memm1439@gmail.com\n",
      "\t 35/35 david000914@gmail.com\n",
      "# of dropped lines (ppg_hr, ppg_rr): 0 0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import median_absolute_deviation\n",
    "from shutil import copyfile as cp\n",
    "from os import listdir as ld\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# checks via 3*MAD range\n",
    "def process_hr_file(path):\n",
    "  if os.path.exists(path):\n",
    "    with open(path, 'r') as r:\n",
    "      hrs = []\n",
    "      lines = []\n",
    "      for line in r:\n",
    "          cells = line[:-1].split(' ')\n",
    "          if len(cells) == 2:\n",
    "            hr = cells[1]\n",
    "            if hr.isdigit():\n",
    "                hrs += [int(hr)]\n",
    "                lines += [line]\n",
    "      l_ = len(lines)\n",
    "    \n",
    "    if len(hrs) == 0:\n",
    "      return 0\n",
    "\n",
    "    trim_min = np.median(hrs) - 3 * median_absolute_deviation(hrs)\n",
    "    trim_max = np.median(hrs) + 3 * median_absolute_deviation(hrs)\n",
    "    \n",
    "    with open(path, 'w+') as w:\n",
    "      for line in lines:\n",
    "        ts, hr = line[:-1].split(',')\n",
    "        try:\n",
    "          if trim_min <= int(hr) <= trim_max:\n",
    "            w.write(line)\n",
    "            l_-=1\n",
    "        except ValueError:\n",
    "          pass\n",
    "    return l_ #the number of dropped hr lines\n",
    "  else:\n",
    "    return 0                       \n",
    "\n",
    "\n",
    "# checks via 3*MAD range\n",
    "def process_rr_file(path):\n",
    "  if os.path.exists(path):\n",
    "    with open(path, 'r') as r:\n",
    "      rr_intervals = []\n",
    "      lines = []\n",
    "      for line in r:\n",
    "          cells = line[:-1].split(' ')\n",
    "          if len(cells) == 2:\n",
    "            rr = cells[1]\n",
    "            if rr.isdigit():\n",
    "                rr_intervals += [int(rr)]\n",
    "                lines += [line]\n",
    "      l_ = len(lines)\n",
    "    \n",
    "    if len(rr_intervals) == 0:\n",
    "      return 0\n",
    "\n",
    "    trim_min = np.median(rr_intervals) - 3 * median_absolute_deviation(rr_intervals)\n",
    "    trim_max = np.median(rr_intervals) + 3 * median_absolute_deviation(rr_intervals)\n",
    "\n",
    "    with open(path, 'w+') as w:\n",
    "      for line in lines:\n",
    "        ts, rri = line[:-1].split(',')\n",
    "        try:\n",
    "          if trim_min <= int(rri) <= trim_max:\n",
    "            w.write(line)\n",
    "            l_-=1\n",
    "        except ValueError:\n",
    "          pass\n",
    "    return l_ #the number of dropped rr lines\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "\n",
    "# '''\n",
    "# (STEP 1)\n",
    "#   Cleaning Lab-Stress and PPG-Accuracy datasets\n",
    "#   parent_dir / scenario / participant / data\n",
    "# '''\n",
    "# datasets = [\n",
    "#   ('lab stress', Dirs.PROCESSED_LAB_STRESS_DATASET),\n",
    "#   #('ppg accuracy', Dirs.PROCESSED_TREADMILL_DATASET)\n",
    "# ]\n",
    "# t_n_ppg_hr = t_n_ppg_rr = 0\n",
    "# for title, dataset_key in datasets:\n",
    "#   t_n_ppg_hr = t_n_ecg_hr = t_n_ppg_rr = t_n_ecg_rr = 0\n",
    "#   print(\"\\n\",title)\n",
    "#   scenarios = ld(Paths.get(dataset_key))\n",
    "#   scenarios.sort(key=lambda x: int(x[:x.index('.')]))\n",
    "#   for scenario in scenarios:\n",
    "#     print(f'\\t{scenario}')\n",
    "#     for participant in ld(Paths.get(dataset_key, scenario)):\n",
    "#       # (a) fix heart rate (bpm)\n",
    "#       path = Paths.get(dataset_key, scenario, participant, 'ppg_hr.csv')\n",
    "#       n_ppg_hr = process_hr_file(path)\n",
    "#       t_n_ppg_hr += n_ppg_hr\n",
    "#       path = Paths.get(dataset_key, scenario, participant, 'ecg_hr.csv')\n",
    "#       n_ecg_hr = process_hr_file(path)\n",
    "#       t_n_ecg_hr += n_ecg_hr\n",
    "\n",
    "#       # (b) fix interbeat-intervals (ms)\n",
    "#       path = Paths.get(dataset_key, scenario, participant, 'ppg_rr.csv')\n",
    "#       n_ppg_rr = process_rr_file(path)\n",
    "#       t_n_ppg_rr += n_ppg_rr\n",
    "#       path = Paths.get(dataset_key, scenario, participant, 'ecg_rr.csv')\n",
    "#       n_ecg_rr = process_rr_file(path)\n",
    "#       t_n_ecg_rr += n_ecg_rr\n",
    "#   print(\"# of dropped lines (ppg_hr, ecg_hr, ppg_rr, ecg_rr)\", t_n_ppg_hr, t_n_ecg_hr, t_n_ppg_rr, t_n_ecg_rr)\n",
    "\n",
    "'''\n",
    "(STEP 2)\n",
    "  Cleaning field stress dataset\n",
    "  parent_dir / participant / data\n",
    "'''\n",
    "print('\\nfield stress')\n",
    "t_n_ppg_hr = t_n_ppg_rr = 0\n",
    "participants = ld(Paths.get(Dirs.PROCESSED_FIELD_DATASET))\n",
    "for idx, participant in enumerate(participants):\n",
    "  print(f'\\t {idx+1}/{len(participants)} {participant}')\n",
    "\n",
    "  # (a) fix heart rate (bpm)\n",
    "  path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, 'ppg_hr.csv')\n",
    "  n_ppg_hr = process_hr_file(path)\n",
    "  t_n_ppg_hr += n_ppg_hr\n",
    "\n",
    "  # (b) fix interbeat-intervals (ms)\n",
    "  path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, 'ppg_rr.csv')\n",
    "  n_ppg_rr = process_rr_file(path)\n",
    "  t_n_ppg_rr += n_ppg_rr\n",
    "print(\"# of dropped lines (ppg_hr, ppg_rr):\", t_n_ppg_hr, t_n_ppg_rr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbHZCikvDZrb"
   },
   "source": [
    "### 2.1.3. Motion filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ql5ugV85fb4s"
   },
   "source": [
    "#### (a) Initial cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGutEEFvuLxP"
   },
   "outputs": [],
   "source": [
    "# initial cleanup\n",
    "from os import listdir as ld\n",
    "import os\n",
    "\n",
    "# treadmill and lab stress datasets\n",
    "for key in [Dirs.SPLIT_TREADMILL_DATASET, Dirs.SPLIT_LAB_STRESS_DATASET, Dirs.PROCESSED_TREADMILL_DATASET, Dirs.PROCESSED_LAB_STRESS_DATASET]:\n",
    "  print(key)\n",
    "  for scenario in ld(Paths.get(key)):\n",
    "    for participant in ld(Paths.get(key, scenario)):\n",
    "      path = Paths.get(key, scenario, participant, 'acc.csv')\n",
    "      if os.path.exists(path):\n",
    "        with open(path) as r:\n",
    "          lines = r.readlines()\n",
    "        with open(path, 'w') as w:\n",
    "          for line in lines:\n",
    "            w.write(line.replace('\"\\n', '\\n'))\n",
    "\n",
    "# field stress dataset\n",
    "for key in [Dirs.SPLIT_FIELD_DATASET, Dirs.PROCESSED_FIELD_DATASET]:\n",
    "  for participant in ld(Paths.get(key)):\n",
    "    path = Paths.get(key, participant, 'acc.csv')\n",
    "    if os.path.exists(path):\n",
    "      with open(path) as r:\n",
    "        lines = r.readlines()\n",
    "      with open(path, 'w') as w:\n",
    "        for line in lines:\n",
    "          w.write(line.replace('\"\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14aMd1EifgRb"
   },
   "source": [
    "#### (b) filter out motion artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AweAHkj8yXDo",
    "outputId": "e72f2f9b-93c6-492b-93ce-949a355040ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "field stress\n",
      "1/35 participant: jinsoonshim@naver.com\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import median_absolute_deviation\n",
    "from shutil import copyfile as cp\n",
    "from os import listdir as ld\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import os\n",
    "import math\n",
    "\n",
    "\n",
    "WINDOW_SIZE = 30000 # ms\n",
    "SUB_WINDOWS_COUNT = 5\n",
    "OVERLAP = 0.5 # half of window size\n",
    "MILD_THRESHOLD = 0.35\n",
    "\n",
    "def clean_motion_artifacts(acc_df, hrv_df, threshold=0.21384):\n",
    "  if acc_df.empty:\n",
    "    return hrv_df.iloc[0:0], 0, hrv_df.shape[0] # drop all\n",
    "  if hrv_df.empty:\n",
    "    return hrv_df, 0, 0 # empty set\n",
    "  drop_count = 0\n",
    "  drop_missing_data_count = 0\n",
    "  hrv_df.sort_values(by=['ts'])\n",
    "  \n",
    "  from_ts = hrv_df.iloc[0]['ts']\n",
    "  till_ts = hrv_df.iloc[-1]['ts']\n",
    "\n",
    "  while from_ts < till_ts:\n",
    "    sub_acc_df = acc_df.query(f'ts >= {from_ts} and ts < {from_ts + WINDOW_SIZE}')\n",
    "    \n",
    "    if sub_acc_df.empty:\n",
    "      drop_indices = hrv_df.query(f'ts >= {from_ts} and ts < {from_ts + WINDOW_SIZE}').index\n",
    "      drop_missing_data_count += len(drop_indices)\n",
    "      hrv_df.drop(drop_indices, inplace=True)\n",
    "      from_ts += WINDOW_SIZE\n",
    "      continue\n",
    "    \n",
    "    activeness_scores = []\n",
    "    _ts = from_ts\n",
    "    while _ts < from_ts + WINDOW_SIZE:\n",
    "      _df = sub_acc_df.query(f'ts >= {_ts} and ts < {_ts + WINDOW_SIZE / SUB_WINDOWS_COUNT}')\n",
    "      magnitudes = []\n",
    "      for _, _r in _df.iterrows():\n",
    "        magnitudes += [math.sqrt(_r['x'] ** 2 + _r['y'] ** 2 + _r['z'] ** 2)]\n",
    "      if len(magnitudes) > 0:\n",
    "        activeness_scores += [np.mean(magnitudes)]\n",
    "      _ts += WINDOW_SIZE  / SUB_WINDOWS_COUNT\n",
    "    \n",
    "    active_count = sum((1 for score in activeness_scores if score > threshold))\n",
    "    if active_count > SUB_WINDOWS_COUNT / 2:\n",
    "      drop_indices = hrv_df.query(f'ts >= {from_ts} and ts < {from_ts + WINDOW_SIZE}').index\n",
    "      drop_count += len(drop_indices)\n",
    "      hrv_df.drop(drop_indices, inplace=True)\n",
    "      from_ts += WINDOW_SIZE\n",
    "      continue\n",
    "    from_ts += WINDOW_SIZE * OVERLAP\n",
    "  return hrv_df, drop_count, drop_missing_data_count\n",
    "\n",
    "\n",
    "# '''\n",
    "# (STEP 1)\n",
    "#   Cleaning Lab-Stress dataset\n",
    "#   parent_dir / scenario / participant / data\n",
    "# '''\n",
    "# print('lab stress')\n",
    "# participants = os.listdir(Paths.get(Dirs.PROCESSED_FIELD_DATASET))\n",
    "# scenarios = ld(Paths.get(Dirs.PROCESSED_LAB_STRESS_DATASET))\n",
    "# scenarios.sort(key=lambda x: int(x[:x.index('.')]))\n",
    "# for index, participant in enumerate(participants):\n",
    "#   rr_total_count, rr_drop_count = 0, 0\n",
    "#   hr_total_count, hr_drop_count = 0, 0\n",
    "#   print(f'{index+1}/{len(participants)} participant: {participant}')\n",
    "  \n",
    "#   for scenario in scenarios:  \n",
    "#     # fix paths\n",
    "#     if not os.path.exists(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario)):\n",
    "#       os.mkdir(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario))\n",
    "#       os.mkdir(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario, participant))\n",
    "#     elif not os.path.exists(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario, participant)):\n",
    "#       os.mkdir(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario, participant))\n",
    "    \n",
    "#     # print(f'\\t{scenario}')\n",
    "#     acc_path = Paths.get(Dirs.PROCESSED_LAB_STRESS_DATASET, scenario, participant, 'acc.csv')\n",
    "#     if os.path.exists(acc_path):\n",
    "#       acc_df = pd.read_csv(acc_path, names=['ts', 'x', 'y', 'z'], header=None)\n",
    "#       rr_path = Paths.get(Dirs.PROCESSED_LAB_STRESS_DATASET, scenario, participant, 'ppg_rr.csv')\n",
    "#       if os.path.exists(rr_path):\n",
    "#         rr_df = pd.read_csv(rr_path, names=['ts', 'rr'], header=None)\n",
    "#         rr_count = rr_df.shape[0]\n",
    "#         rr_total_count += rr_count\n",
    "#         rr_df, drop_count, drop_missing_data_count = clean_motion_artifacts(acc_df=acc_df, hrv_df=rr_df, threshold=MILD_THRESHOLD)\n",
    "#         rr_drop_count += drop_count\n",
    "#         #print(f'(rr) drop/max: {rr_drop_count}/{rr_count}, missing_data_drops={drop_missing_data_count}')\n",
    "#         rr_df.to_csv(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario, participant, 'ppg_rr.csv'), header=None)\n",
    "#       hr_path = Paths.get(Dirs.PROCESSED_LAB_STRESS_DATASET, scenario, participant, 'ppg_hr.csv')\n",
    "#       if os.path.exists(hr_path):\n",
    "#         hr_df = pd.read_csv(hr_path, names=['ts', 'hr'], header=None)\n",
    "#         hr_count = hr_df.shape[0]\n",
    "#         hr_total_count += hr_count\n",
    "#         hr_df, drop_count, drop_missing_data_count = clean_motion_artifacts(acc_df=acc_df, hrv_df=hr_df, threshold=MILD_THRESHOLD)\n",
    "#         hr_drop_count += drop_count\n",
    "#         #print(f'(hr) drop/max: {hr_drop_count}/{hr_count}, missing_data_drops={drop_missing_data_count}')\n",
    "#         hr_df.to_csv(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario, participant, 'ppg_hr.csv'), header=None)\n",
    "#   print(f'(rr_drop_count/rr_total, hr_drop_count/hr_total) = ({rr_drop_count}/{rr_total_count}, {hr_drop_count}/{hr_total_count})')\n",
    "\n",
    "'''\n",
    "(STEP 2)\n",
    "  Cleaning field stress dataset\n",
    "  parent_dir / participant / data\n",
    "'''\n",
    "print('field stress')\n",
    "participants = os.listdir(Paths.get(Dirs.PROCESSED_FIELD_DATASET))\n",
    "for index, participant in enumerate(participants):\n",
    "  rr_total_count, rr_drop_count = 0, 0\n",
    "  hr_total_count, hr_drop_count = 0, 0\n",
    "  print(f'{index+1}/{len(participants)} participant: {participant}')\n",
    "  \n",
    "  # fix paths\n",
    "  if not os.path.exists(Paths.get(Dirs.FILTERED_FIELD_DATASET, participant)):\n",
    "    os.mkdir(Paths.get(Dirs.FILTERED_FIELD_DATASET, participant))\n",
    "  \n",
    "  acc_path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, 'acc.csv')\n",
    "  if os.path.exists(acc_path):\n",
    "    acc_df = pd.read_csv(acc_path, names=['ts', 'x', 'y', 'z'], header=None)\n",
    "    rr_path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, 'ppg_rr.csv')\n",
    "    if os.path.exists(rr_path):\n",
    "      rr_df = pd.read_csv(rr_path, names=['ts', 'rr'], header=None)\n",
    "      rr_count = rr_df.shape[0]\n",
    "      rr_total_count += rr_count\n",
    "      rr_df, drop_count, drop_missing_data_count = clean_motion_artifacts(acc_df=acc_df, hrv_df=rr_df)\n",
    "      rr_drop_count += drop_count\n",
    "      #print(f'(rr) drop/max: {drop_count}/{rr_count}, missing_data_drops={drop_missing_data_count}')\n",
    "      rr_df.to_csv(Paths.get(Dirs.FILTERED_FIELD_DATASET, participant, 'ppg_rr.csv'), header=None)\n",
    "    hr_path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, 'ppg_hr.csv')\n",
    "    if os.path.exists(hr_path):\n",
    "      hr_df = pd.read_csv(hr_path, names=['ts', 'hr'], header=None)\n",
    "      hr_count = hr_df.shape[0]\n",
    "      hr_total_count += hr_count\n",
    "      hr_df, drop_count, drop_missing_data_count = clean_motion_artifacts(acc_df=acc_df, hrv_df=hr_df)\n",
    "      hr_drop_count += drop_count\n",
    "      #print(f'(hr) drop/max: {drop_count}/{hr_count}, missing_data_drops={drop_missing_data_count}')\n",
    "      hr_df.to_csv(Paths.get(Dirs.FILTERED_FIELD_DATASET, participant, 'ppg_hr.csv'), header=None)\n",
    "  print(f'(rr_drop_count/rr_total, hr_drop_count/hr_total) = ({rr_drop_count}/{rr_total_count}, {hr_drop_count}/{hr_total_count})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vqLP9uG2oZ2"
   },
   "source": [
    "### Backup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "B9juJqFyhpvk",
    "outputId": "342b5910-784e-4915-d819-177c24370311"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ysl@inha.edu\n",
      "-1.551918029785156 3.815654993057251 -1.580695867538452\n",
      "0.0225224494934082 0.1271672248840332 0.08612251281738281\n",
      "wonjs0725@gmail.com\n",
      "zkapdh123@gmail.com\n",
      "gh011127@gmail.com\n",
      "hhcc05@inha.edu\n",
      "bagle1029@gmail.com\n",
      "12170603@inha.edu\n",
      "memm1439@gmail.com\n",
      "david000914@gmail.com\n",
      "jinsoonshim@naver.com\n",
      "\t static\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0cff9cbc241b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticipant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t static'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{participant}: ${acc_values[participant]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'jinsoonshim@naver.com'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy.stats import median_absolute_deviation\n",
    "from collections import OrderedDict\n",
    "from shutil import copyfile as cp\n",
    "from os import listdir as ld\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "# get list of participants\n",
    "participants = os.listdir(Paths.get(Dirs.PROCESSED_FIELD_DATASET))\n",
    "\n",
    "# identify thresholds using preliminary dataset\n",
    "scenarios = ld(Paths.get(Dirs.PROCESSED_TREADMILL_DATASET))\n",
    "scenarios.sort(key=lambda x: int(x[:x.index('.')]))\n",
    "\n",
    "acc_values = OrderedDict()\n",
    "li_values = OrderedDict()\n",
    "for participant in participants:\n",
    "  if participant in ['jinsoonshim@naver.com', 'rlarkgus11170@inha.edu', 'kpm1323@gmail.com', 'eowjdtjdwls@gmail.com', 'chaewoni65@gmail.com', 'minrudcho01@gmail.com', 'csissis1997@gmail.com', 'km55181499@gmail.com', 'fpdlsqhdn962@inha.edu', 'bw04029@gmail.com', '12201686@inha.edu', 'vusgowlwk17@gmail.com', 'duecjf011521212106@gmail.com', 'jwshoho4@gmail.com', 'nnozilaxonim@gmail.com', 'minjae20000207@gmail.com', '12181157@inha.edu', 'rbfl675@inha.edu', 'nigaram8@gmail.com', 'as7177as3912@gmail.com', '12212982so@inha.edu', 'wjdwogus0604@gmail.com', 'gurwns7772@gmail.com', 'jyypaul@gmail.com', 'unicpn21@gmail.com', 'powerampshere@gmail.com']:\n",
    "    continue\n",
    "  print(participant)\n",
    "\n",
    "  static_magnitudes = []\n",
    "  walking_magnitudes = []\n",
    "  running_magnitudes = []\n",
    "\n",
    "  tight_intensities = []\n",
    "  medium_intensities = []\n",
    "  loose_intensities = []\n",
    "  \n",
    "  for scenario in scenarios:\n",
    "    path = Paths.get(Dirs.PROCESSED_TREADMILL_DATASET, scenario, participant, 'acc.csv')\n",
    "    if 'tight' in scenario:\n",
    "      intensities = tight_intensities\n",
    "    elif 'medium' in scenario:\n",
    "      intensities = medium_intensities\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "      df = pd.read_csv(path, names=['ts', 'li'], header=None)\n",
    "      for _, r in df.iterrows():\n",
    "        intensities += [r['li']]\n",
    "    acc_values[participant] = {\n",
    "      'static': (np.mean(static_magnitudes), np.var(static_magnitudes)),\n",
    "      'walking': (np.mean(walking_magnitudes), np.var(walking_magnitudes)),\n",
    "      'running': (np.mean(running_magnitudes), np.var(running_magnitudes)),\n",
    "    }\n",
    "\n",
    "\n",
    "for participant in participants:\n",
    "  print(participant)\n",
    "  print('\\t static', )\n",
    "  print(f'{participant}: {acc_values[participant]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0eYLQzmDfdK"
   },
   "source": [
    "### 2.1.4. Looseness filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "CSuLVSv-RYvG",
    "outputId": "e02f1bab-2d62-49c7-d618-347bc258971a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/35 participant: 12170603@inha.edu\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  85.18518518518519\n",
      "2/35 participant: 12181157@inha.edu\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "participant 12181157@inha.edu has a missing label for building a looseness model\n",
      "3/35 participant: 12201686@inha.edu\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "participant 12201686@inha.edu has a missing label for building a looseness model\n",
      "4/35 participant: 12212982so@inha.edu\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  60.0\n",
      "5/35 participant: as7177as3912@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "participant as7177as3912@gmail.com has a missing label for building a looseness model\n",
      "6/35 participant: bagle1029@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  88.23529411764706\n",
      "7/35 participant: bw04029@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  63.1578947368421\n",
      "8/35 participant: chaewoni65@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  57.89473684210527\n",
      "9/35 participant: csissis1997@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "participant csissis1997@gmail.com has a missing label for building a looseness model\n",
      "10/35 participant: david000914@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  78.57142857142857\n",
      "11/35 participant: duecjf011521212106@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  100.0\n",
      "12/35 participant: eowjdtjdwls@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  91.66666666666666\n",
      "13/35 participant: fpdlsqhdn962@inha.edu\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "participant fpdlsqhdn962@inha.edu has a missing label for building a looseness model\n",
      "14/35 participant: gh011127@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  100.0\n",
      "15/35 participant: gurwns7772@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  80.95238095238095\n",
      "16/35 participant: hhcc05@inha.edu\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  95.65217391304348\n",
      "17/35 participant: jinsoonshim@naver.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "participant jinsoonshim@naver.com has a missing label for building a looseness model\n",
      "18/35 participant: jwshoho4@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  56.25\n",
      "19/35 participant: jyypaul@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  100.0\n",
      "20/35 participant: km55181499@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  78.26086956521739\n",
      "21/35 participant: kpm1323@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  81.25\n",
      "22/35 participant: memm1439@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  86.66666666666667\n",
      "23/35 participant: minjae20000207@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  82.35294117647058\n",
      "24/35 participant: minrudcho01@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  66.66666666666666\n",
      "25/35 participant: nigaram8@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "participant nigaram8@gmail.com has a missing label for building a looseness model\n",
      "26/35 participant: nnozilaxonim@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "participant nnozilaxonim@gmail.com has a missing label for building a looseness model\n",
      "27/35 participant: powerampshere@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "participant powerampshere@gmail.com has a missing label for building a looseness model\n",
      "28/35 participant: rbfl675@inha.edu\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "participant rbfl675@inha.edu has a missing label for building a looseness model\n",
      "29/35 participant: rlarkgus11170@inha.edu\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "participant rlarkgus11170@inha.edu has a missing label for building a looseness model\n",
      "30/35 participant: unicpn21@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  73.68421052631578\n",
      "31/35 participant: vusgowlwk17@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "participant vusgowlwk17@gmail.com has a missing label for building a looseness model\n",
      "32/35 participant: wjdwogus0604@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  73.91304347826086\n",
      "33/35 participant: wonjs0725@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  100.0\n",
      "34/35 participant: ysl@inha.edu\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "acc  76.19047619047619\n",
      "35/35 participant: zkapdh123@gmail.com\n",
      "\t1. static tight\n",
      "\t2. static medium\n",
      "\t3. static loose\n",
      "\t4. walking tight\n",
      "\t5. walking medium\n",
      "\t6. walking loose\n",
      "\t7. running tight\n",
      "\t8. running medium\n",
      "\t9. running loose\n",
      "\n",
      "\n",
      "acc  95.83333333333334\n",
      "field stress\n",
      "1/35 participant: 12170603@inha.edu\n",
      "(rr) drop/max: 110328/948711, missing_data_drops=32230\n",
      "(hr) drop/max: 90472/858748, missing_data_drops=35795\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (110328/948711, 90472/858748)\n",
      "2/35 participant: 12181157@inha.edu\n",
      "(rr) drop/max: 422876/1702759, missing_data_drops=117276\n",
      "(hr) drop/max: 425245/1706621, missing_data_drops=118491\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (422876/1702759, 425245/1706621)\n",
      "3/35 participant: 12201686@inha.edu\n",
      "(rr) drop/max: 34320/136035, missing_data_drops=0\n",
      "(hr) drop/max: 34289/135895, missing_data_drops=0\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (34320/136035, 34289/135895)\n",
      "4/35 participant: 12212982so@inha.edu\n",
      "(rr) drop/max: 58858/1205894, missing_data_drops=74123\n",
      "(hr) drop/max: 25807/1079820, missing_data_drops=69191\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (58858/1205894, 25807/1079820)\n",
      "5/35 participant: as7177as3912@gmail.com\n",
      "(rr) drop/max: 43756/164380, missing_data_drops=3020\n",
      "(hr) drop/max: 38936/136733, missing_data_drops=1157\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (43756/164380, 38936/136733)\n",
      "6/35 participant: bagle1029@gmail.com\n",
      "(rr) drop/max: 300657/1348058, missing_data_drops=52459\n",
      "(hr) drop/max: 63884/637397, missing_data_drops=29660\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (300657/1348058, 63884/637397)\n",
      "7/35 participant: bw04029@gmail.com\n",
      "(rr) drop/max: 72387/912143, missing_data_drops=23038\n",
      "(hr) drop/max: 60630/775173, missing_data_drops=17753\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (72387/912143, 60630/775173)\n",
      "8/35 participant: chaewoni65@gmail.com\n",
      "(rr) drop/max: 43245/696490, missing_data_drops=3792\n",
      "(hr) drop/max: 33560/373703, missing_data_drops=2615\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (43245/696490, 33560/373703)\n",
      "9/35 participant: csissis1997@gmail.com\n",
      "(rr) drop/max: 29878/376634, missing_data_drops=16619\n",
      "(hr) drop/max: 20173/330219, missing_data_drops=17749\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (29878/376634, 20173/330219)\n",
      "10/35 participant: david000914@gmail.com\n",
      "(rr) drop/max: 50280/225824, missing_data_drops=44\n",
      "(hr) drop/max: 50453/232684, missing_data_drops=6747\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (50280/225824, 50453/232684)\n",
      "11/35 participant: duecjf011521212106@gmail.com\n",
      "(rr) drop/max: 482023/615228, missing_data_drops=32963\n",
      "(hr) drop/max: 326831/404956, missing_data_drops=13966\n",
      "(rr_drop_count/rr_total, hr_drop_count/hr_total) = (482023/615228, 326831/404956)\n",
      "12/35 participant: eowjdtjdwls@gmail.com\n",
      "(rr) drop/max: 77126/1266241, missing_data_drops=20445\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy.stats import median_absolute_deviation\n",
    "from collections import OrderedDict\n",
    "from shutil import copyfile as cp\n",
    "from os import listdir as ld\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pandas.errors import EmptyDataError\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "\n",
    "WINDOW_SIZE = 30000 # ms\n",
    "OVERLAP = 0.5       #1-OVERLAP\n",
    "\n",
    "def li_to_var(li_df):\n",
    "  var_=[]\n",
    "  from_ts = li_df.iloc[0]['ts']\n",
    "  till_ts = li_df.iloc[-1]['ts']\n",
    "  while from_ts < till_ts-WINDOW_SIZE:\n",
    "    sub_df = li_df.query(f'ts >= {from_ts} and ts < {from_ts + WINDOW_SIZE}')\n",
    "    var_.append(sub_df['li'].var())\n",
    "    from_ts += WINDOW_SIZE * OVERLAP\n",
    "  return var_\n",
    "\n",
    "def create_dataset(df_loose, df1, df2):\n",
    "  var1 = li_to_var(df_loose)\n",
    "  if not df1.empty:\n",
    "    var2_1 = li_to_var(df1)\n",
    "  else:\n",
    "    var2_1 = None\n",
    "  if not df2.empty:\n",
    "    var2_2 = li_to_var(df2)\n",
    "  else:\n",
    "    var2_2 = None\n",
    "  df_var1 = pd.DataFrame(var1)\n",
    "  df_var1['class']=1\n",
    "  df_var2_1 = pd.DataFrame(var2_1)\n",
    "  df_var2_2 = pd.DataFrame(var2_2)\n",
    "  if None in [var2_1, var2_2]:\n",
    "    if var2_1 is None:\n",
    "      df_var2 = df_var2_2\n",
    "    else:\n",
    "      df_var2 = df_var2_1\n",
    "  else:\n",
    "    df_var2 = pd.concat([df_var2_1, df_var2_2], ignore_index=True)\n",
    "  df_var2['class']=0\n",
    "  df_var1.columns=df_var2.columns=['var','class']\n",
    "  X = pd.concat([df_var1, df_var2], ignore_index=True)\n",
    "  x = X.drop(['class'], axis = 1)\n",
    "  y = X.drop(['var'], axis = 1)\n",
    "  return x,y\n",
    "\n",
    "def xgboost_looseness(x_train, x_test, y_train, y_test):\n",
    "  d_train = xgb.DMatrix(data=x_train, label=y_train.to_numpy())\n",
    "  d_test = xgb.DMatrix(data=x_test, label=y_test.to_numpy())\n",
    "  \n",
    "  booster = xgb.train(\n",
    "      params=dict(eval_metric='auc', booster='gbtree', verbosity=0, objective='binary:logistic'),\n",
    "      dtrain=d_train,\n",
    "      num_boost_round=10000,  # the number of boosted trees\n",
    "      early_stopping_rounds=25,  # early stop generating trees when eval_metric is not improved\n",
    "      evals=[(d_test, 'test')],  # evaluation set to check early stopping\n",
    "      \n",
    "      verbose_eval=False\n",
    "  )\n",
    "  \n",
    "  y_pred = booster.predict(data=d_test, ntree_limit=booster.best_ntree_limit)\n",
    "  y_pred_class = np.where(y_pred > 0.5, 1, 0) \n",
    "  acc = accuracy_score(y_test, y_pred_class)\n",
    "  print(\"\\n\\nacc \", acc*100)\n",
    "  return booster, y_pred_class\n",
    "\n",
    "\n",
    "def clean_looseness_artifacts(booster, li_df, hrv_df):\n",
    "  if li_df.empty:\n",
    "    return hrv_df.iloc[0:0], 0, hrv_df.shape[0] # drop all\n",
    "  if hrv_df.empty:\n",
    "    return hrv_df, 0, 0 # empty set\n",
    "  drop_count = 0\n",
    "  drop_missing_data_count = 0\n",
    "\n",
    "  from_ts = li_df.iloc[0]['ts']\n",
    "  till_ts = li_df.iloc[-1]['ts']\n",
    "  while from_ts < till_ts-WINDOW_SIZE:\n",
    "    sub_li_df = li_df.query(f'ts >= {from_ts} and ts < {from_ts + WINDOW_SIZE}')\n",
    "    if sub_li_df.empty:\n",
    "      drop_indices = hrv_df.query(f'ts >= {from_ts} and ts < {from_ts + WINDOW_SIZE}').index\n",
    "      drop_missing_data_count += len(drop_indices)\n",
    "      hrv_df.drop(drop_indices, inplace=True)\n",
    "      from_ts += WINDOW_SIZE\n",
    "      continue\n",
    "    var_ = sub_li_df['li'].var()\n",
    "    x_ = pd.DataFrame([var_],columns =['var'])\n",
    "    x_ = xgb.DMatrix(x_) \n",
    "    y_pred = booster.predict(data=x_, ntree_limit=booster.best_ntree_limit) #!  \n",
    "    y_pred = np.where(y_pred > 0.5, 1, 0) \n",
    "    \n",
    "    if y_pred == 1:\n",
    "      drop_indices = hrv_df.query(f'ts >= {from_ts} and ts < {from_ts + WINDOW_SIZE}').index\n",
    "      drop_count += len(drop_indices)\n",
    "      hrv_df.drop(drop_indices, inplace=True)\n",
    "      from_ts += WINDOW_SIZE\n",
    "      continue\n",
    "    from_ts += WINDOW_SIZE * OVERLAP\n",
    "  return hrv_df, drop_count, drop_missing_data_count\n",
    "\n",
    "'''\n",
    "(STEP 1)\n",
    "  Train Looseness models\n",
    "  parent_dir / scenario / participant / data\n",
    "'''\n",
    "# todo Bunyodbek pls fill with model building code here\n",
    "booster = {}\n",
    "participants = os.listdir(Paths.get(Dirs.PROCESSED_FIELD_DATASET))\n",
    "scenarios = ld(Paths.get(Dirs.PROCESSED_TREADMILL_DATASET))\n",
    "scenarios.sort(key=lambda x: int(x[:x.index('.')]))\n",
    "for index, participant in enumerate(participants):\n",
    "  rr_total_count, rr_drop_count = 0, 0\n",
    "  hr_total_count, hr_drop_count = 0, 0\n",
    "  print(f'{index+1}/{len(participants)} participant: {participant}')\n",
    "  \n",
    "  for scenario in scenarios:\n",
    "    print(f'\\t{scenario}')\n",
    "    try:\n",
    "      if 'loose' in scenario:\n",
    "        df_loose = pd.read_csv(Paths.get(Dirs.PROCESSED_TREADMILL_DATASET, scenario, participant, 'ppg_li.csv'), names=['ts', 'li'], header = None)\n",
    "      elif 'medium' in scenario:\n",
    "        df_medium = pd.read_csv(Paths.get(Dirs.PROCESSED_TREADMILL_DATASET, scenario, participant, 'ppg_li.csv'), names=['ts', 'li'], header = None)\n",
    "      else:\n",
    "        df_tight = pd.read_csv(Paths.get(Dirs.PROCESSED_TREADMILL_DATASET, scenario, participant, 'ppg_li.csv'), names=['ts', 'li'], header = None)\n",
    "    except EmptyDataError:\n",
    "      print(f'participant {participant} skipped on looseness model building')\n",
    "      continue\n",
    "  if df_loose.empty or (df_medium.empty and df_tight.empty):\n",
    "    print(f'participant {participant} has a missing label for building a looseness model')\n",
    "  else:\n",
    "    x,y = create_dataset(df_loose, df_medium, df_tight)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    booster_, y_pred = xgboost_looseness(x_train, x_test, y_train, y_test)\n",
    "    booster[participant] = booster_\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "(STEP 2)\n",
    "  Cleaning Lab-Stress dataset\n",
    "  parent_dir / scenario / participant / data\n",
    "'''\n",
    "# print('\\nlab stress')\n",
    "# participants = os.listdir(Paths.get(Dirs.PROCESSED_FIELD_DATASET))\n",
    "# scenarios = ld(Paths.get(Dirs.PROCESSED_LAB_STRESS_DATASET))\n",
    "# scenarios.sort(key=lambda x: int(x[:x.index('.')]))\n",
    "# for index, participant in enumerate(participants):\n",
    "#   rr_total_count, rr_drop_count = 0, 0\n",
    "#   hr_total_count, hr_drop_count = 0, 0\n",
    "#   print(f'{index+1}/{len(participants)} participant: {participant}')\n",
    "  \n",
    "#   for scenario in scenarios:  \n",
    "#     # fix paths\n",
    "#     if not os.path.exists(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario)):\n",
    "#       os.mkdir(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario))\n",
    "#       os.mkdir(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario, participant))\n",
    "#     elif not os.path.exists(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario, participant)):\n",
    "#       os.mkdir(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario, participant))\n",
    "\n",
    "#     li_path = Paths.get(Dirs.PROCESSED_LAB_STRESS_DATASET, scenario, participant, 'ppg_li.csv')\n",
    "#     if os.path.exists(li_path):\n",
    "#       li_df = pd.read_csv(li_path, names=['ts', 'li'], header=None)\n",
    "#       rr_path = Paths.get(Dirs.PROCESSED_LAB_STRESS_DATASET, scenario, participant, 'ppg_rr.csv')\n",
    "#       booster_ = booster[participant] if participant in booster else booster[list(booster.keys())[0]]\n",
    "#       if os.path.exists(rr_path):\n",
    "#         rr_df = pd.read_csv(rr_path, names=['ts', 'rr'], header=None)\n",
    "#         rr_count = rr_df.shape[0]\n",
    "#         rr_total_count += rr_count\n",
    "#         rr_df, drop_count, drop_missing_data_count = clean_looseness_artifacts(booster_, li_df=li_df, hrv_df=rr_df)\n",
    "#         rr_drop_count += drop_count\n",
    "#         #print(f'(rr) drop/max: {rr_drop_count}/{rr_count}, missing_data_drops={drop_missing_data_count}')\n",
    "#         #rr_df.to_csv(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario, participant, 'ppg_rr.csv'), header=None)\n",
    "#       hr_path = Paths.get(Dirs.PROCESSED_LAB_STRESS_DATASET, scenario, participant, 'ppg_hr.csv')\n",
    "#       if os.path.exists(hr_path):\n",
    "#         hr_df = pd.read_csv(hr_path, names=['ts', 'hr'], header=None)\n",
    "#         hr_count = hr_df.shape[0]\n",
    "#         hr_total_count += hr_count\n",
    "#         hr_df, drop_count, drop_missing_data_count = clean_looseness_artifacts(booster_, li_df=li_df, hrv_df=hr_df)\n",
    "#         hr_drop_count += drop_count\n",
    "#         #print(f'(hr) drop/max: {hr_drop_count}/{hr_count}, missing_data_drops={drop_missing_data_count}')\n",
    "#         #hr_df.to_csv(Paths.get(Dirs.FILTERED_LAB_STRESS_DATASET, scenario, participant, 'ppg_hr.csv'), header=None)\n",
    "#   print(f'(rr_drop_count/rr_total, hr_drop_count/hr_total) = ({rr_drop_count}/{rr_total_count}, {hr_drop_count}/{hr_total_count})')\n",
    "\n",
    "'''\n",
    "(STEP 3)\n",
    "  Cleaning field stress dataset\n",
    "  parent_dir / participant / data\n",
    "'''\n",
    "print('field stress')\n",
    "participants = os.listdir(Paths.get(Dirs.PROCESSED_FIELD_DATASET))\n",
    "for index, participant in enumerate(participants):\n",
    "  rr_total_count, rr_drop_count = 0, 0\n",
    "  hr_total_count, hr_drop_count = 0, 0\n",
    "  print(f'{index+1}/{len(participants)} participant: {participant}')\n",
    "  \n",
    "  # fix paths\n",
    "  if not os.path.exists(Paths.get(Dirs.FILTERED_FIELD_DATASET, participant)):\n",
    "    os.mkdir(Paths.get(Dirs.FILTERED_FIELD_DATASET, participant))\n",
    "  \n",
    "  li_path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, 'ppg_li.csv')\n",
    "  if os.path.exists(li_path):\n",
    "    li_df = pd.read_csv(li_path, names=['ts', 'li'], header=None)\n",
    "    rr_path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, 'ppg_rr.csv')\n",
    "    booster_ = booster[participant] if participant in booster else booster[list(booster.keys())[0]]\n",
    "    if os.path.exists(rr_path):\n",
    "      rr_df = pd.read_csv(rr_path, names=['ts', 'rr'], header=None)\n",
    "      rr_count = rr_df.shape[0]\n",
    "      rr_total_count += rr_count\n",
    "      rr_df, drop_count, drop_missing_data_count = clean_looseness_artifacts(booster_, li_df=li_df, hrv_df=rr_df)\n",
    "      rr_drop_count += drop_count\n",
    "      print(f'(rr) drop/max: {drop_count}/{rr_count}, missing_data_drops={drop_missing_data_count}')\n",
    "      rr_df.to_csv(Paths.get(Dirs.FILTERED_FIELD_DATASET, participant, 'ppg_rr.csv'), header=None)\n",
    "    hr_path = Paths.get(Dirs.PROCESSED_FIELD_DATASET, participant, 'ppg_hr.csv')\n",
    "    if os.path.exists(hr_path):\n",
    "      hr_df = pd.read_csv(hr_path, names=['ts', 'hr'], header=None)\n",
    "      hr_count = hr_df.shape[0]\n",
    "      hr_total_count += hr_count\n",
    "      hr_df, drop_count, drop_missing_data_count = clean_looseness_artifacts(booster_, li_df=li_df, hrv_df=hr_df)\n",
    "      hr_drop_count += drop_count\n",
    "      print(f'(hr) drop/max: {drop_count}/{hr_count}, missing_data_drops={drop_missing_data_count}')\n",
    "      hr_df.to_csv(Paths.get(Dirs.FILTERED_FIELD_DATASET, participant, 'ppg_hr.csv'), header=None)\n",
    "  print(f'(rr_drop_count/rr_total, hr_drop_count/hr_total) = ({rr_drop_count}/{rr_total_count}, {hr_drop_count}/{hr_total_count})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPomWz3FDj5w"
   },
   "source": [
    "## 2.2 Normalize the HR data\n",
    "    Z-score normalization\n",
    "\n",
    "    Input: Preprocessed HR data\n",
    "    Output: HRV with person-specific attributes removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KfwL-e5BcSFo",
    "outputId": "d37ccdfb-71b5-4e06-dd65-723998a2c64a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab\n",
      "\t 1. baseline rest\n",
      "\t 10. rest\n",
      "\t 11. cold instructions\n",
      "\t 12. cold stressor\n",
      "\t 13. recovery\n",
      "\t 2. speech instructions\n",
      "\t 3. speech preparation\n",
      "\t 4. speech stressor\n",
      "\t 5. rest\n",
      "\t 6. arithmetic instructions\n",
      "\t 7. arithmetic stressor p1\n",
      "\t 8. arithmetic rest\n",
      "\t 9. arithmetic stressor p2\n"
     ]
    }
   ],
   "source": [
    "from os import listdir as ld\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "lab_src_key = Dirs.PROCESSED_LAB_STRESS_DATASET # Dirs.FILTERED_LAB_STRESS_DATASET\n",
    "field_src_key = Dirs.PROCESSED_FIELD_DATASET # Dirs.FILTERED_FIELD_DATASET\n",
    "\n",
    "# # normalize the lab data\n",
    "print('lab')\n",
    "for scenario in ld(Paths.get(lab_src_key)):\n",
    "  print(f'\\t {scenario}')\n",
    "  # fix paths\n",
    "  if not os.path.exists(Paths.get(Dirs.NORMALIZED_LAB_STRESS_DATASET, scenario)):\n",
    "    os.mkdir(Paths.get(Dirs.NORMALIZED_LAB_STRESS_DATASET, scenario))\n",
    "\n",
    "  for participant in ld(Paths.get(lab_src_key, scenario)):\n",
    "    # fix paths\n",
    "    if not os.path.exists(Paths.get(Dirs.NORMALIZED_LAB_STRESS_DATASET, scenario, participant)):\n",
    "      os.mkdir(Paths.get(Dirs.NORMALIZED_LAB_STRESS_DATASET, scenario, participant))\n",
    "\n",
    "    # normalize rr data\n",
    "    rr_path = Paths.get(lab_src_key, scenario, participant, 'ppg_rr.csv')\n",
    "    if os.path.exists(rr_path):\n",
    "      rr_df = pd.read_csv(rr_path, names=['ts', 'rr'], header=None)\n",
    "      if rr_df.empty:\n",
    "        continue\n",
    "      norm_rr = stats.zscore(rr_df['rr'])\n",
    "      norm_rr_df = pd.DataFrame({'ts': rr_df['ts'], 'rr': norm_rr})\n",
    "      norm_rr_df.to_csv(Paths.get(Dirs.NORMALIZED_LAB_STRESS_DATASET, scenario, participant, 'ppg_rr.csv'), index=False, header=False)\n",
    "    \n",
    "    # normalize hr data\n",
    "    hr_path = Paths.get(lab_src_key, scenario, participant, 'ppg_hr.csv')\n",
    "    if os.path.exists(hr_path):\n",
    "      hr_df = pd.read_csv(hr_path, names=['ts', 'hr'], header=None)\n",
    "      if hr_df.empty:\n",
    "        continue\n",
    "      norm_hr = stats.zscore(hr_df['hr'])\n",
    "      norm_hr_df = pd.DataFrame({'ts': hr_df['ts'], 'hr': norm_hr})\n",
    "      norm_hr_df.to_csv(Paths.get(Dirs.NORMALIZED_LAB_STRESS_DATASET, scenario, participant, 'ppg_hr.csv'), index=False, header=False)\n",
    "\n",
    "# normalize the field data\n",
    "# print('field')\n",
    "# participants = ld(Paths.get(field_src_key))\n",
    "# for idx, participant in enumerate(participants):\n",
    "#   print(f'\\t {idx+1}/{len(participants)} {participant}')\n",
    "#   # fix paths\n",
    "#   if not os.path.exists(Paths.get(Dirs.NORMALIZED_FIELD_DATASET, participant)):\n",
    "#     os.mkdir(Paths.get(Dirs.NORMALIZED_FIELD_DATASET, participant))\n",
    "\n",
    "#   # normalize rr data\n",
    "#   rr_path = Paths.get(field_src_key, participant, 'ppg_rr.csv')\n",
    "#   if os.path.exists(rr_path):\n",
    "#     rr_df = pd.read_csv(rr_path, names=['ts', 'rr'], header=None)\n",
    "#     if rr_df.empty:\n",
    "#       continue\n",
    "#     norm_rr = stats.zscore(rr_df['rr'])\n",
    "#     norm_rr_df = pd.DataFrame({'ts': rr_df['ts'], 'rr': norm_rr})\n",
    "#     norm_rr_df.to_csv(Paths.get(Dirs.NORMALIZED_FIELD_DATASET, participant, 'ppg_rr.csv'), index=False, header=False)\n",
    "  \n",
    "#   # normalize hr data\n",
    "#   hr_path = Paths.get(field_src_key, participant, 'ppg_hr.csv')\n",
    "#   if os.path.exists(hr_path):\n",
    "#     hr_df = pd.read_csv(hr_path, names=['ts', 'hr'], header=None)\n",
    "#     if hr_df.empty:\n",
    "#       continue\n",
    "#     norm_hr = stats.zscore(hr_df['hr'])\n",
    "#     norm_hr_df = pd.DataFrame({'ts': hr_df['ts'], 'hr': norm_hr})\n",
    "#     norm_hr_df.to_csv(Paths.get(Dirs.NORMALIZED_FIELD_DATASET, participant, 'ppg_hr.csv'), index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEpSQCerZaJZ"
   },
   "source": [
    "## 2.3. Feature computation\n",
    "    > Breaking down the data into windows\n",
    "    > Time-domain features\n",
    "\n",
    "    Input: Preprocessed, normalized HR & rrInterval data\n",
    "    Output: Time-domain features from HR and rInterval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xxVF1hmstytc",
    "outputId": "820ec7ce-6bcc-497a-e4fc-05d6bf3d0f02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2/35 rlarkgus11170@inha.edu out of 263:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, \n",
      "3/35 kpm1323@gmail.com out of 152:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, \n",
      "4/35 eowjdtjdwls@gmail.com out of 132:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, \n",
      "5/35 chaewoni65@gmail.com out of 206:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, \n",
      "6/35 minrudcho01@gmail.com out of 255:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, \n",
      "7/35 csissis1997@gmail.com out of 258:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, \n",
      "8/35 km55181499@gmail.com out of 229:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, \n",
      "9/35 fpdlsqhdn962@inha.edu out of 241:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, \n",
      "10/35 bw04029@gmail.com out of 180:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, \n",
      "11/35 12201686@inha.edu out of 51:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, \n",
      "12/35 vusgowlwk17@gmail.com out of 243:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, \n",
      "13/35 duecjf011521212106@gmail.com out of 108:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, \n",
      "14/35 jwshoho4@gmail.com out of 238:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, \n",
      "15/35 nnozilaxonim@gmail.com out of 144:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, \n",
      "16/35 minjae20000207@gmail.com out of 129:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, \n",
      "17/35 12181157@inha.edu out of 110:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, \n",
      "18/35 rbfl675@inha.edu out of 82:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, \n",
      "19/35 nigaram8@gmail.com out of 127:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, \n",
      "20/35 as7177as3912@gmail.com out of 125:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, \n",
      "21/35 12212982so@inha.edu out of 279:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, \n",
      "22/35 wjdwogus0604@gmail.com out of 258:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, \n",
      "23/35 gurwns7772@gmail.com out of 295:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, \n",
      "24/35 jyypaul@gmail.com out of 202:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, \n",
      "25/35 unicpn21@gmail.com out of 108:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, \n",
      "26/35 powerampshere@gmail.com out of 287:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, \n",
      "27/35 ysl@inha.edu out of 161:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, \n",
      "28/35 wonjs0725@gmail.com out of 212:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, \n",
      "29/35 zkapdh123@gmail.com out of 266:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, \n",
      "30/35 gh011127@gmail.com out of 120:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, \n",
      "31/35 hhcc05@inha.edu out of 186:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, \n",
      "32/35 bagle1029@gmail.com out of 265:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, \n",
      "33/35 12170603@inha.edu out of 243:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, \n",
      "34/35 memm1439@gmail.com out of 294:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, \n",
      "35/35 david000914@gmail.com out of 101:  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, "
     ]
    }
   ],
   "source": [
    "from hrvanalysis import get_time_domain_features\n",
    "from scipy.stats import kurtosis, skew\n",
    "from os import listdir as ld\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import os\n",
    "import threading\n",
    "\n",
    "\n",
    "WINDOW_SIZE = 60000\n",
    "OVERLAP = 0.75\n",
    "\n",
    "# # compute lab features\n",
    "# scenarios = ld(Paths.get(Dirs.NORMALIZED_LAB_STRESS_DATASET))\n",
    "# if '.DS_Store':\n",
    "#   scenarios.remove('.DS_Store')\n",
    "# for scenario in scenarios:\n",
    "#   print(scenario)\n",
    "#   stress_label = 1 if 'stress' in scenario else (0 if 'baseline rest' in scenario else None)\n",
    "#   if stress_label is not None:\n",
    "#     for participant in ld(Paths.get(Dirs.NORMALIZED_LAB_STRESS_DATASET, scenario)):\n",
    "#       rr_path = Paths.get(Dirs.NORMALIZED_LAB_STRESS_DATASET, scenario, participant, 'ppg_rr.csv')\n",
    "#       hr_path = Paths.get(Dirs.NORMALIZED_LAB_STRESS_DATASET, scenario, participant, 'ppg_hr.csv')\n",
    "\n",
    "#       if not os.path.exists(rr_path) or not os.path.exists(hr_path):\n",
    "#         continue\n",
    "      \n",
    "#       rr_df = pd.read_csv(rr_path, names=['ts', 'rr'], header=None)\n",
    "#       rr_df.sort_values(by=['ts'])\n",
    "#       hr_df = pd.read_csv(hr_path, names=['ts', 'hr'], header=None)\n",
    "#       hr_df.sort_values(by=['ts'])\n",
    "      \n",
    "#       from_ts = max(rr_df.iloc[0]['ts'], hr_df.iloc[0]['ts'])\n",
    "#       till_ts = min(rr_df.iloc[-1]['ts'], hr_df.iloc[-1]['ts'])\n",
    "#       while from_ts <= till_ts - WINDOW_SIZE:\n",
    "#         sub_rr_df = rr_df.query(f'ts >= {from_ts} and ts < {from_ts + WINDOW_SIZE}')\n",
    "#         sub_hr_df = hr_df.query(f'ts >= {from_ts} and ts < {from_ts + WINDOW_SIZE}')\n",
    "#         from_ts += WINDOW_SIZE * (1 - OVERLAP)\n",
    "        \n",
    "#         if sub_rr_df.shape[0] < 2 or sub_hr_df.shape[0] < 2:\n",
    "#           continue\n",
    "        \n",
    "#         # compute features\n",
    "#         rr_tdf = get_time_domain_features(nn_intervals=sub_rr_df['rr'].values)\n",
    "#         hr_tdf = get_time_domain_features(nn_intervals=sub_hr_df['hr'].values)\n",
    "#         rr_values = sub_rr_df['rr'].values\n",
    "#         hr_values = sub_hr_df['hr'].values\n",
    "#         features = [\n",
    "#           # rr\n",
    "#           rr_tdf['mean_nni'],\n",
    "#           rr_tdf['median_nni'],\n",
    "#           rr_tdf['max_hr'],\n",
    "#           rr_tdf['min_hr'],\n",
    "#           rr_tdf['sdsd'],\n",
    "#           kurtosis(rr_values, fisher=True),\n",
    "#           skew(rr_values),\n",
    "#           # TBD: rr_slope\n",
    "#           np.percentile(rr_values, 20),\n",
    "#           np.percentile(rr_values, 80),\n",
    "#           # hr\n",
    "#           hr_tdf['mean_nni'],\n",
    "#           hr_tdf['median_nni'],\n",
    "#           hr_tdf['max_hr'],\n",
    "#           hr_tdf['min_hr'],\n",
    "#           hr_tdf['sdsd'],\n",
    "#           kurtosis(hr_values, fisher=True),\n",
    "#           skew(hr_values),\n",
    "#           # TBD: hr_slope\n",
    "#           np.percentile(hr_values, 20),\n",
    "#           np.percentile(hr_values, 80)\n",
    "#         ]\n",
    "\n",
    "#         path = Paths.get(Dirs.FEATURES_LAB_STRESS, f'{participant}.csv')\n",
    "#         write_header = not os.path.exists(path)\n",
    "#         with open(path, 'a+') as w:\n",
    "#           if write_header:\n",
    "#             w.write('rr_mean_nni,rr_median_nni,rr_max_hr,rr_min_hr,rr_sdsd,rr_kurtosis,rr_skew,rr_p20,rr_p80,hr_mean_nni,hr_median_nni,hr_max_hr,hr_min_hr,hr_sdsd,hr_kurtosis,hr_skew,hr_p20,hr_p80,label\\n')\n",
    "#           w.write(f'{\",\".join([str(x) for x in features])},{stress_label}\\n')\n",
    "\n",
    "\n",
    "EMA_FEATURE_PERIOD = 30*60000 # ms\n",
    "USE_Z_SCORE = False\n",
    "# compute field features\n",
    "def compute_features(idx, participant):\n",
    "  print()\n",
    "  print(f'{idx+1}/{len(participants)} {participant}', end=' ')\n",
    "  rr_path = Paths.get(Dirs.NORMALIZED_FIELD_DATASET, participant, 'ppg_rr.csv')\n",
    "  hr_path = Paths.get(Dirs.NORMALIZED_FIELD_DATASET, participant, 'ppg_hr.csv')\n",
    "  ema_df = pd.read_csv(Paths.get(Dirs.INIT_PARTICIPANTS, participant, 'ema.csv'), names=['ts', 'cheerful', 'happy', 'angry', 'nervous', 'sad', 'activity', 'lat', 'lon'], header=None)\n",
    "\n",
    "  if not os.path.exists(rr_path) or not os.path.exists(hr_path):\n",
    "    return\n",
    "rr_df = pd.read_csv(rr_path, names=['ts', 'rr'], header=None)\n",
    "  rr_df.sort_values(by=['ts'])\n",
    "  hr_df = pd.read_csv(hr_path, names=['ts', 'hr'], header=None)\n",
    "  hr_df.sort_values(by=['ts'])\n",
    "  \n",
    "  # compute ema label threshols for GT labeling\n",
    "  ema_scores = []\n",
    "  for idx, row in ema_df.iterrows():\n",
    "    score = ((4 - row['cheerful']) + (4 - row['happy']) + row['angry'] + row['nervous'] + row['sad']) / 5\n",
    "    ema_scores += [score]\n",
    "  ema_mean = np.mean(ema_scores)\n",
    "  ema_z_scores = stats.zscore(ema_scores)\n",
    "\n",
    "  # compute features for each EMA period\n",
    "  with open(Paths.get(Dirs.FEATURES_FIELD, f'{participant}.csv'), 'w+') as w:\n",
    "    w.write('rr_mean_nni,rr_median_nni,rr_max_hr,rr_min_hr,rr_sdsd,rr_kurtosis,rr_skew,rr_p20,rr_p80,hr_mean_nni,hr_median_nni,hr_max_hr,hr_min_hr,hr_sdsd,hr_kurtosis,hr_skew,hr_p20,hr_p80,label\\n')\n",
    "    max_count = ema_df.shape[0]\n",
    "    #print(f'out of {max_count}: ', end=' ')\n",
    "    for idx, row in ema_df.iterrows():\n",
    "      print(idx+1, end=', ', flush=True)\n",
    "      ema_score = ((4 - row['cheerful']) + (4 - row['happy']) + row['angry'] + row['nervous'] + row['sad']) / 5\n",
    "      if USE_Z_SCORE:\n",
    "        stress_label = 1 if ema_z_scores[idx] > .6 else 0\n",
    "      else:\n",
    "        stress_label = 1 if ema_score > ema_mean else 0\n",
    "      \n",
    "      from_ts = row['ts'] - EMA_FEATURE_PERIOD\n",
    "      till_ts = row['ts']\n",
    "      while from_ts <= till_ts - WINDOW_SIZE:\n",
    "        sub_rr_df = rr_df.query(f'ts >= {from_ts} and ts < {from_ts + WINDOW_SIZE}')\n",
    "        sub_hr_df = hr_df.query(f'ts >= {from_ts} and ts < {from_ts + WINDOW_SIZE}')\n",
    "        from_ts += WINDOW_SIZE * (1 - OVERLAP)\n",
    "        \n",
    "        if sub_rr_df.shape[0] < 2 or sub_hr_df.shape[0] < 2:\n",
    "          continue\n",
    "        \n",
    "        # compute features\n",
    "        rr_tdf = get_time_domain_features(nn_intervals=sub_rr_df['rr'].values)\n",
    "        hr_tdf = get_time_domain_features(nn_intervals=sub_hr_df['hr'].values)\n",
    "        rr_values = sub_rr_df['rr'].values\n",
    "        hr_values = sub_hr_df['hr'].values\n",
    "        features = [\n",
    "          # rr\n",
    "          rr_tdf['mean_nni'],\n",
    "          rr_tdf['median_nni'],\n",
    "          rr_tdf['max_hr'],\n",
    "          rr_tdf['min_hr'],\n",
    "          rr_tdf['sdsd'],\n",
    "          kurtosis(rr_values, fisher=True),\n",
    "          skew(rr_values),\n",
    "          # TBD: rr_slope\n",
    "          np.percentile(rr_values, 20),\n",
    "          np.percentile(rr_values, 80),\n",
    "          # hr\n",
    "          hr_tdf['mean_nni'],\n",
    "          hr_tdf['median_nni'],\n",
    "          hr_tdf['max_hr'],\n",
    "          hr_tdf['min_hr'],\n",
    "          hr_tdf['sdsd'],\n",
    "          kurtosis(hr_values, fisher=True),\n",
    "          skew(hr_values),\n",
    "          # TBD: hr_slope\n",
    "          np.percentile(hr_values, 20),\n",
    "          np.percentile(hr_values, 80)\n",
    "        ]\n",
    "        w.write(f'{\",\".join([str(x) for x in features])},{stress_label}\\n')\n",
    "\n",
    "participants = ld(Paths.get(Dirs.NORMALIZED_FIELD_DATASET))\n",
    "if '.DS_Store' in participants:\n",
    "  participants.remove('.DS_Store')\n",
    "threads = []\n",
    "for idx, participant in enumerate(participants):\n",
    "  t = threading.Thread(target=compute_features, args=(idx, participant))\n",
    "  t.start()\n",
    "  threads += [t]\n",
    "for t in threads:\n",
    "  t.join()\n",
    "print('done!')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osE8ZzLKZfje"
   },
   "source": [
    "## 2.4 Generalized Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "3Dq6NFag_66l",
    "outputId": "3c41928b-10b3-429a-ddad-64e1a9d0c3e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/34 12170603@inha.edu.csv participant is left out as test\n",
      "2/34 12181157@inha.edu.csv participant is left out as test\n",
      "3/34 12201686@inha.edu.csv participant is left out as test\n",
      "4/34 12212982so@inha.edu.csv participant is left out as test\n",
      "5/34 as7177as3912@gmail.com.csv participant is left out as test\n",
      "6/34 bagle1029@gmail.com.csv participant is left out as test\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-c554a9f2590f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;31m# field stress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m \u001b[0mtrain_test_generalized_xgboost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDirs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFEATURES_FIELD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRESULTS_G_FIELD_STRESS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-76-c554a9f2590f>\u001b[0m in \u001b[0;36mtrain_test_generalized_xgboost\u001b[0;34m(inp_dir, out_file, with_context)\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# evaluation set to check early stopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             )\n\u001b[1;32m    127\u001b[0m             \u001b[0mmodels\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join as j\n",
    "from os import listdir as ld\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import re\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "\n",
    "def extract_emails(_list):\n",
    "  _reg = re.compile(r'([0-9a-z.]+@[0-9a-z.]+)')\n",
    "  _elems = []\n",
    "  for _p in _list:\n",
    "    if _reg.search(_p):\n",
    "      _email = _reg.search(_p).group(1)\n",
    "      _elems += [(_email, _p)]\n",
    "  return _elems\n",
    "\n",
    "\n",
    "def train_test_generalized_xgboost(inp_dir, out_file, with_context=False):\n",
    "    inputs = extract_emails(ld(inp_dir))\n",
    "    inputs.sort(key=lambda x: x[0])\n",
    "    m_rounds = len(inputs)\n",
    "\n",
    "    feature_names = ['rr_mean_nni','rr_median_nni','rr_max_hr','rr_min_hr','rr_sdsd','rr_kurtosis','rr_skew','rr_p20','rr_p80','hr_mean_nni','hr_median_nni','hr_max_hr','hr_min_hr','hr_sdsd','hr_kurtosis','hr_skew','hr_p20','hr_p80']\n",
    "\n",
    "    models = []\n",
    "    x_tests = []\n",
    "\n",
    "    best_thresholds = {}\n",
    "    best_f1s = {}\n",
    "\n",
    "    with open(out_file, \"w+\") as w, open(f'{out_file[:out_file.rindex(\".csv\")]}_granular.csv', 'w+') as w_granular:\n",
    "        w.write('Balanced Accuracy,F1 score,ROC_AUC,TPR,TNR\\n')\n",
    "        w_granular.write('LOSO Subject,Balanced Accuracy,F1 score,ROC_AUC,TPR,TNR\\n')\n",
    "        all_scores = {'acc': [], 'f1': [], 'roc_auc': [], 'TPR': [], 'TNR': []}\n",
    "        \n",
    "        models = []\n",
    "        x_tests = []\n",
    "\n",
    "        for loso_idx in range(m_rounds):\n",
    "            # region load test subject's dataset\n",
    "            test_email, test_filename = inputs[loso_idx]\n",
    "            # if test_email != 'david000914@gmail.com.csv':\n",
    "            #   continue\n",
    "            test_dataset = pd.read_csv(j(inp_dir, test_filename)).replace([np.inf, -np.inf], np.nan,).dropna(axis=0)\n",
    "\n",
    "            if test_dataset.shape[0] < 2:\n",
    "              continue\n",
    "\n",
    "            x_test = test_dataset.iloc[:, :-1].copy()\n",
    "            x_test = x_test[feature_names]\n",
    "            y_test = test_dataset.iloc[:, -1].copy()\n",
    "            y_test = y_test.astype(int)\n",
    "\n",
    "            scores_1round = {'acc': [], 'f1': [], 'roc_auc': [], 'TPR': [], 'TNR': []}\n",
    "            x_train, y_train, initialized = None, None, False\n",
    "\n",
    "            print(f'{loso_idx+1}/{m_rounds} {test_email} participant is left out as test')\n",
    "\n",
    "            for idx, tp in enumerate(inputs):\n",
    "                if tp[0] == test_email:\n",
    "                    continue\n",
    "                train_dataset = pd.read_csv(j(inp_dir, tp[1])).replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n",
    "                _x_train = train_dataset.iloc[:, :-1].copy()\n",
    "                _x_train = _x_train[feature_names]\n",
    "                _y_train = train_dataset.iloc[:, -1].copy()\n",
    "                _y_train = _y_train.astype(int)\n",
    "                if _x_train.empty or _y_train.empty:\n",
    "                    continue\n",
    "                if initialized:\n",
    "                    x_train = pd.concat([x_train, _x_train])\n",
    "                    y_train = pd.concat([y_train, _y_train])\n",
    "                else:\n",
    "                    x_train = _x_train\n",
    "                    y_train = _y_train\n",
    "                    initialized = True\n",
    "\n",
    "            # try:\n",
    "            #     # 'fit_resample' conducts over-sampling data in the minority class. Again, resampling should be only conducted in train set.\n",
    "            #     x_train, y_train = SMOTE().fit_resample(x_train, y_train)\n",
    "            # except ValueError:\n",
    "            #     continue\n",
    "            \n",
    "            # np arrays --> data frames\n",
    "            x_train = pd.DataFrame(x_train, columns=x_train.columns)\n",
    "            y_train = pd.Series(y_train)\n",
    "\n",
    "            min_max_scaler = MinMaxScaler()\n",
    "            # StandardScaler.fit() finds characteristics of data distribution (i.e., min, max) in train set.\n",
    "            min_max_scaler.fit(x_train)\n",
    "            # Transform numeric data within train and test set.\n",
    "            x_train_scale = min_max_scaler.transform(x_train)\n",
    "            x_test_scale = min_max_scaler.transform(x_test)\n",
    "            # np arrays --> data frames\n",
    "            x_train = pd.DataFrame(x_train_scale, index=x_train.index, columns=x_train.columns)\n",
    "            x_test = pd.DataFrame(x_test_scale, index=x_test.index, columns=x_test.columns)\n",
    "\n",
    "            # data frame --> xgboost.DMatrix\n",
    "            d_train = xgb.DMatrix(data=x_train, label=y_train.to_numpy())\n",
    "            d_test = xgb.DMatrix(data=x_test, label=y_test.to_numpy())\n",
    "            # print(f'x_test size : {x_test.size}, y_test size : {y_test.size}')\n",
    "\n",
    "            # skip if only one class exists\n",
    "            if len(y_train.unique()) == 1 or len(y_test.unique()) == 1:\n",
    "              print(f'error - participant {test_email} train classes are {y_train.unique()}, test classes are {y_test.unique()}')\n",
    "              continue\n",
    "\n",
    "            # params : https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "            results = {}\n",
    "            booster = xgb.train(\n",
    "                params=dict(eval_metric='auc', booster='gbtree', verbosity=0, objective='binary:logistic'),\n",
    "                dtrain=d_train,\n",
    "                num_boost_round=1000,  # the number of boosted trees\n",
    "                early_stopping_rounds=25,  # early stop generating trees when eval_metric is not improved\n",
    "                evals=[(d_test, 'test')],  # evaluation set to check early stopping\n",
    "                verbose_eval=False,\n",
    "                evals_result=results\n",
    "            )\n",
    "            models += [booster]\n",
    "            x_tests += [x_test]\n",
    "\n",
    "            # [Plotting]\n",
    "            # epochs = len(results['test']['auc'])\n",
    "            # x_axis = range(0, epochs)\n",
    "            # fig, ax = pyplot.subplots()\n",
    "            # ax.plot(x_axis, results['train']['auc'], label='Train')\n",
    "            # ax.plot(x_axis, results['test']['auc'], label='Test')\n",
    "            # ax.legend()\n",
    "            # pyplot.ylabel('AUC')\n",
    "            # pyplot.title('XGBoost training plot')\n",
    "            # pyplot.show()\n",
    "\n",
    "            # predict() returns probability of a positive label (label == 1)\n",
    "            y_pred = booster.predict(data=d_test, ntree_limit=booster.best_ntree_limit)\n",
    "\n",
    "            # [predict() --> probability] cut-off as 0.5: positive label when a probability is higher than 0.5.\n",
    "            \n",
    "            best_threshold, max_f1 = None, 0\n",
    "            for threshold in range(5, 90, 1):\n",
    "              f1 = metrics.f1_score(y_test, np.where(y_pred > float(threshold)/100, 1, 0), average='macro')\n",
    "              if f1 > max_f1:\n",
    "                best_threshold = float(threshold)/100\n",
    "                max_f1 = f1\n",
    "            best_thresholds[test_email] = best_threshold\n",
    "            best_f1s[test_email] = max_f1\n",
    "            #print(y_test.to_numpy(), '\\npred\\n', y_pred)\n",
    "\n",
    "            # region append to total score\n",
    "            y_pred_class = np.where(y_pred > best_threshold, 1, 0)\n",
    "            all_scores['acc'].append(metrics.balanced_accuracy_score(y_test, y_pred_class))\n",
    "            all_scores['f1'].append(metrics.f1_score(y_test, y_pred_class, average='macro'))\n",
    "            all_scores['roc_auc'].append(metrics.roc_auc_score(y_test, y_pred))\n",
    "            all_scores['TPR'].append(metrics.recall_score(y_test, y_pred_class))\n",
    "            all_scores['TNR'].append(metrics.recall_score(y_test, y_pred_class, pos_label=0))\n",
    "            # endregion\n",
    "            # region append to local (round) score\n",
    "            scores_1round['acc'].append(metrics.balanced_accuracy_score(y_test, y_pred_class))\n",
    "            scores_1round['f1'].append(metrics.f1_score(y_test, y_pred_class, average='macro'))\n",
    "            scores_1round['roc_auc'].append(metrics.roc_auc_score(y_test, y_pred))\n",
    "            scores_1round['TPR'].append(metrics.recall_score(y_test, y_pred_class))\n",
    "            scores_1round['TNR'].append(metrics.recall_score(y_test, y_pred_class, pos_label=0))\n",
    "            # endregion\n",
    "            # conf_mtx += metrics.confusion_matrix(y_test, y_pred_class)\n",
    "            # xgb_models.append(booster)\n",
    "\n",
    "            w_granular.write(f'{test_email},{np.mean(scores_1round[\"acc\"])},{np.mean(scores_1round[\"f1\"])},{np.mean(scores_1round[\"roc_auc\"])},{np.mean(scores_1round[\"TPR\"])},{np.mean(scores_1round[\"TNR\"])}\\n')\n",
    "        w.write(f'{np.mean(all_scores[\"acc\"])},{np.mean(all_scores[\"f1\"])},{np.mean(all_scores[\"roc_auc\"])},{np.mean(all_scores[\"TPR\"])},{np.mean(all_scores[\"TNR\"])}\\n')\n",
    "        \n",
    "        keys = list(best_f1s.keys())\n",
    "        keys.sort()\n",
    "        for participant in keys:\n",
    "          print(f'{participant}, {best_f1s[participant]}, {best_thresholds[participant]}')\n",
    "\n",
    "        # plot shap\n",
    "        shap_values = []\n",
    "        test_features = []\n",
    "        for model, x_test in zip(models, x_tests):\n",
    "            explainer = shap.TreeExplainer(booster)\n",
    "            shap_value = explainer.shap_values(x_test)\n",
    "            shap_values.append(shap_value)\n",
    "            test_features.append(x_test)\n",
    "        shap_values = np.vstack(shap_values)\n",
    "        test_features = pd.concat(test_features, axis=0)\n",
    "        fig = plt.figure()\n",
    "        fig.suptitle(f'Labshap for generalized', fontsize=16)\n",
    "        plt.rcParams['axes.facecolor'] = '#f2f8ff'\n",
    "        # plt.xlim(-6, 6)\n",
    "        shap.summary_plot(shap_values, test_features, show=False)\n",
    "        fig.savefig(Paths.get(Dirs.RESULTS_ROOT,'fieldshap-generalized.svg'), format='svg')\n",
    "        # plt.show()\n",
    "\n",
    "# lab stress\n",
    "# train_test_generalized_xgboost(Paths.get(Dirs.FEATURES_LAB_STRESS), Paths.get(Files.RESULTS_G_LAB_STRESS))\n",
    "\n",
    "# field stress\n",
    "train_test_generalized_xgboost(Paths.get(Dirs.FEATURES_FIELD), Paths.get(Files.RESULTS_G_FIELD_STRESS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDkgncv5abRP"
   },
   "source": [
    "## 2.5. Personalized model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbko8DFrafwx"
   },
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5E86ZOoaiU1"
   },
   "source": [
    "## 3.1. Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDXEcG1Rak-6"
   },
   "source": [
    "## 3.2. Data yield"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6PD_QY2a2rK"
   },
   "source": [
    "## 3.3. Model explanation via SHAP"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "SgpFjplrBgsH",
    "HtE4fudnCFxh",
    "otIGghPu4-uw",
    "bnV2IJnsDTXJ",
    "xXjzFgNsDWLH",
    "TbHZCikvDZrb",
    "9vqLP9uG2oZ2",
    "G0eYLQzmDfdK",
    "FPomWz3FDj5w",
    "PEpSQCerZaJZ"
   ],
   "name": "SOSW_code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
